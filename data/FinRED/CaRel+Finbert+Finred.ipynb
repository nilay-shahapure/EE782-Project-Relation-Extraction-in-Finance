{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65e542c82a9b4e289cbae90473ec4f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbc6a38f60ce406a8342bfd6b8339e3e",
              "IPY_MODEL_11d7c68d030043e88ec48580df2692d5",
              "IPY_MODEL_5cdd7687be3748edac59c129f8e81200"
            ],
            "layout": "IPY_MODEL_3af2b246e28b4aeb8e2e49cd1ac8a773"
          }
        },
        "cbc6a38f60ce406a8342bfd6b8339e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e91807874431491fb1ca854cc92a7292",
            "placeholder": "​",
            "style": "IPY_MODEL_2e380df2f2fc4badade5c7242a8dd3ba",
            "value": "vocab.txt: "
          }
        },
        "11d7c68d030043e88ec48580df2692d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f1e6cde39cb438da44b02b069cd4721",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8619caf5983c4795a3c39bf42b2b1717",
            "value": 1
          }
        },
        "5cdd7687be3748edac59c129f8e81200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af9bede75773496e9f20132eb7095430",
            "placeholder": "​",
            "style": "IPY_MODEL_376c46a57eff4f5bb033a8eba2f3686f",
            "value": " 226k/? [00:00&lt;00:00, 6.96MB/s]"
          }
        },
        "3af2b246e28b4aeb8e2e49cd1ac8a773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e91807874431491fb1ca854cc92a7292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e380df2f2fc4badade5c7242a8dd3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f1e6cde39cb438da44b02b069cd4721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8619caf5983c4795a3c39bf42b2b1717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af9bede75773496e9f20132eb7095430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376c46a57eff4f5bb033a8eba2f3686f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9965a03e534b4fcc87adee7e54031d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0ea80917d7e47fdbdb2ac3e9e08decd",
              "IPY_MODEL_e051205bc39f4b8c9b4035cc4c13a2ce",
              "IPY_MODEL_50a857277d9a4aeba3c38060b844b718"
            ],
            "layout": "IPY_MODEL_13b1eb58ae7642dc9ffa1138bb6a9e9f"
          }
        },
        "a0ea80917d7e47fdbdb2ac3e9e08decd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585f3257f0754b659dd1af895a2ed067",
            "placeholder": "​",
            "style": "IPY_MODEL_cfbbb8dc444b433b86a299020d2f4166",
            "value": "config.json: 100%"
          }
        },
        "e051205bc39f4b8c9b4035cc4c13a2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e931d1df0c6416cb4d7156dcdb175fe",
            "max": 359,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0089815f86de4090946e2711e2ab8780",
            "value": 359
          }
        },
        "50a857277d9a4aeba3c38060b844b718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78a0ac5d267846de851337ebd05f4baa",
            "placeholder": "​",
            "style": "IPY_MODEL_8e7c2539de0d4ef7bf43495b21772c90",
            "value": " 359/359 [00:00&lt;00:00, 24.6kB/s]"
          }
        },
        "13b1eb58ae7642dc9ffa1138bb6a9e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585f3257f0754b659dd1af895a2ed067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfbbb8dc444b433b86a299020d2f4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e931d1df0c6416cb4d7156dcdb175fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0089815f86de4090946e2711e2ab8780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78a0ac5d267846de851337ebd05f4baa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7c2539de0d4ef7bf43495b21772c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dbde674d67943aeb5590d0d34e145dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f85efca0f61461792a670ddf49d8b5e",
              "IPY_MODEL_ae1867a455e045e5b5ab68eb5872056a",
              "IPY_MODEL_777774bb66384765b8ad58504e1f6383"
            ],
            "layout": "IPY_MODEL_83aa3f53d4da4786882fb64ea4f3e943"
          }
        },
        "9f85efca0f61461792a670ddf49d8b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdc880e443604aaf851e49d4acaee99b",
            "placeholder": "​",
            "style": "IPY_MODEL_3276594f6ce24d14ae8e6dcb97590599",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ae1867a455e045e5b5ab68eb5872056a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2966a891fa0e48b8a7d7b9a504bf13d9",
            "max": 441551705,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84ce1312b94d467c8ec3e01027c85d70",
            "value": 441551705
          }
        },
        "777774bb66384765b8ad58504e1f6383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d8252ed6a2d4b6cb0d6c104465df64a",
            "placeholder": "​",
            "style": "IPY_MODEL_4e299a4abc1742fda6b034c2dfc501e3",
            "value": " 442M/442M [00:03&lt;00:00, 255MB/s]"
          }
        },
        "83aa3f53d4da4786882fb64ea4f3e943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdc880e443604aaf851e49d4acaee99b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3276594f6ce24d14ae8e6dcb97590599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2966a891fa0e48b8a7d7b9a504bf13d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ce1312b94d467c8ec3e01027c85d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d8252ed6a2d4b6cb0d6c104465df64a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e299a4abc1742fda6b034c2dfc501e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7268f0e8cd3646eb9c04fc3a17bf5a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc940f78a9f04bc19cb560deb94345d3",
              "IPY_MODEL_de1f16097e1145e19a71ac3cf7606a2a",
              "IPY_MODEL_8cc8ad7303994f3da43afe9a190bf0d9"
            ],
            "layout": "IPY_MODEL_7fcb92c5ea774ba2a7d5e73897a50119"
          }
        },
        "bc940f78a9f04bc19cb560deb94345d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_619104e6358e43d2945d115d0707db5d",
            "placeholder": "​",
            "style": "IPY_MODEL_3e37e881794342a9a721c90043bcf248",
            "value": "model.safetensors: 100%"
          }
        },
        "de1f16097e1145e19a71ac3cf7606a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20f9c49811941df8eed0d84762d9c5d",
            "max": 441529472,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16ec92acb3bf4e2fa266e1bac8ab1fe5",
            "value": 441529472
          }
        },
        "8cc8ad7303994f3da43afe9a190bf0d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbfd969e8ddd4ead871a8b1f6ca15daf",
            "placeholder": "​",
            "style": "IPY_MODEL_4cbe4018793443229e87e8fa6d51b4d5",
            "value": " 442M/442M [00:03&lt;00:00, 163MB/s]"
          }
        },
        "7fcb92c5ea774ba2a7d5e73897a50119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619104e6358e43d2945d115d0707db5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e37e881794342a9a721c90043bcf248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f20f9c49811941df8eed0d84762d9c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ec92acb3bf4e2fa266e1bac8ab1fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbfd969e8ddd4ead871a8b1f6ca15daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cbe4018793443229e87e8fa6d51b4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "65e542c82a9b4e289cbae90473ec4f15",
            "cbc6a38f60ce406a8342bfd6b8339e3e",
            "11d7c68d030043e88ec48580df2692d5",
            "5cdd7687be3748edac59c129f8e81200",
            "3af2b246e28b4aeb8e2e49cd1ac8a773",
            "e91807874431491fb1ca854cc92a7292",
            "2e380df2f2fc4badade5c7242a8dd3ba",
            "5f1e6cde39cb438da44b02b069cd4721",
            "8619caf5983c4795a3c39bf42b2b1717",
            "af9bede75773496e9f20132eb7095430",
            "376c46a57eff4f5bb033a8eba2f3686f",
            "9965a03e534b4fcc87adee7e54031d12",
            "a0ea80917d7e47fdbdb2ac3e9e08decd",
            "e051205bc39f4b8c9b4035cc4c13a2ce",
            "50a857277d9a4aeba3c38060b844b718",
            "13b1eb58ae7642dc9ffa1138bb6a9e9f",
            "585f3257f0754b659dd1af895a2ed067",
            "cfbbb8dc444b433b86a299020d2f4166",
            "9e931d1df0c6416cb4d7156dcdb175fe",
            "0089815f86de4090946e2711e2ab8780",
            "78a0ac5d267846de851337ebd05f4baa",
            "8e7c2539de0d4ef7bf43495b21772c90",
            "1dbde674d67943aeb5590d0d34e145dd",
            "9f85efca0f61461792a670ddf49d8b5e",
            "ae1867a455e045e5b5ab68eb5872056a",
            "777774bb66384765b8ad58504e1f6383",
            "83aa3f53d4da4786882fb64ea4f3e943",
            "bdc880e443604aaf851e49d4acaee99b",
            "3276594f6ce24d14ae8e6dcb97590599",
            "2966a891fa0e48b8a7d7b9a504bf13d9",
            "84ce1312b94d467c8ec3e01027c85d70",
            "3d8252ed6a2d4b6cb0d6c104465df64a",
            "4e299a4abc1742fda6b034c2dfc501e3",
            "7268f0e8cd3646eb9c04fc3a17bf5a4b",
            "bc940f78a9f04bc19cb560deb94345d3",
            "de1f16097e1145e19a71ac3cf7606a2a",
            "8cc8ad7303994f3da43afe9a190bf0d9",
            "7fcb92c5ea774ba2a7d5e73897a50119",
            "619104e6358e43d2945d115d0707db5d",
            "3e37e881794342a9a721c90043bcf248",
            "f20f9c49811941df8eed0d84762d9c5d",
            "16ec92acb3bf4e2fa266e1bac8ab1fe5",
            "cbfd969e8ddd4ead871a8b1f6ca15daf",
            "4cbe4018793443229e87e8fa6d51b4d5"
          ]
        },
        "id": "LDIHVEtrYHQL",
        "outputId": "62b7c7d0-ce2f-4f0f-d0b0-e1b651fbe5fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65e542c82a9b4e289cbae90473ec4f15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/359 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9965a03e534b4fcc87adee7e54031d12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 5700 records to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_train.jsonl\n",
            "Wrote 1007 records to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_dev.jsonl\n",
            "Wrote 1068 records to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_test.jsonl\n",
            "Number of predicates: 29\n",
            "Records: train 5582 dev 971 test 1034\n",
            "Computed pos-weights (approx): {'s_head': 37.49537774346583, 's_tail': 37.052623280754275, 'obj_head': [3603.7760656152827, 9288.230411991137, 1577.5620811924048, 1547.2051182871467, 2980.728358262613, 3658.393883963729, 992.909460934529, 4092.5592526684873, 4092.5592526684873, 5137.723294942058, 510.6949141722565, 1453.9397502774714, 475.370807740886, 224.50887000512708, 3658.393883963729, 1127.5981255719714, 1436.6190390677439, 5488.090784361573, 1653.246564018859, 3499.289804358119, 384.81469587090305, 552.944952860218, 508.5358639060425, 6036.999849075004, 1311.6086885238658, 185.6460585891452, 7789.96749064621, 5137.723294942058, 715.6765557398322], 'obj_tail': [3603.7760656152827, 9288.230411991137, 1577.5620811924048, 1547.2051182871467, 2980.728358262613, 3658.393883963729, 992.909460934529, 4092.5592526684873, 4092.5592526684873, 5137.723294942058, 508.5358639060425, 1453.9397502774714, 475.370807740886, 224.71962595820594, 3658.393883963729, 1127.5981255719714, 1436.6190390677439, 5488.090784361573, 1653.246564018859, 3499.289804358119, 384.19936142870915, 550.4155238574988, 507.46315682639334, 6036.999849075004, 1311.6086885238658, 184.92763650121043, 7789.96749064621, 5137.723294942058, 711.4483754824532]}\n",
            "DataLoaders ready. Example batch sizes: 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dbde674d67943aeb5590d0d34e145dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7268f0e8cd3646eb9c04fc3a17bf5a4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train Epoch 1/3:   0%|          | 0/698 [00:00<?, ?it/s]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 0/698 [00:03<?, ?it/s, loss=4.1140]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 1/698 [00:03<41:00,  3.53s/it, loss=4.1140]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 1/698 [00:04<41:00,  3.53s/it, loss=4.4702]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 2/698 [00:04<22:10,  1.91s/it, loss=4.4702]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 2/698 [00:04<22:10,  1.91s/it, loss=5.0150]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 3/698 [00:04<15:30,  1.34s/it, loss=5.0150]\u001b[A\n",
            "Train Epoch 1/3:   0%|          | 3/698 [00:05<15:30,  1.34s/it, loss=5.3895]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 4/698 [00:05<11:40,  1.01s/it, loss=5.3895]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 4/698 [00:06<11:40,  1.01s/it, loss=4.1947]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 5/698 [00:06<09:42,  1.19it/s, loss=4.1947]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 5/698 [00:06<09:42,  1.19it/s, loss=5.4339]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 6/698 [00:06<08:45,  1.32it/s, loss=5.4339]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 6/698 [00:07<08:45,  1.32it/s, loss=3.7987]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 7/698 [00:07<07:54,  1.46it/s, loss=3.7987]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 7/698 [00:07<07:54,  1.46it/s, loss=4.6403]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 8/698 [00:07<07:15,  1.58it/s, loss=4.6403]\u001b[A\n",
            "Train Epoch 1/3:   1%|          | 8/698 [00:08<07:15,  1.58it/s, loss=5.1164]\u001b[A\n",
            "Train Epoch 1/3:   1%|▏         | 9/698 [00:08<07:07,  1.61it/s, loss=5.1164]\u001b[A\n",
            "Train Epoch 1/3:   1%|▏         | 9/698 [00:08<07:07,  1.61it/s, loss=4.2270]\u001b[A\n",
            "Train Epoch 1/3:   1%|▏         | 10/698 [00:08<06:45,  1.70it/s, loss=4.2270]\u001b[A\n",
            "Train Epoch 1/3:   1%|▏         | 10/698 [00:09<06:45,  1.70it/s, loss=3.7172]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 11/698 [00:09<06:41,  1.71it/s, loss=3.7172]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 11/698 [00:09<06:41,  1.71it/s, loss=4.2409]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 12/698 [00:09<06:30,  1.76it/s, loss=4.2409]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 12/698 [00:10<06:30,  1.76it/s, loss=4.8222]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 13/698 [00:10<06:37,  1.72it/s, loss=4.8222]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 13/698 [00:11<06:37,  1.72it/s, loss=4.7492]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 14/698 [00:11<06:42,  1.70it/s, loss=4.7492]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 14/698 [00:11<06:42,  1.70it/s, loss=4.0439]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 15/698 [00:11<06:20,  1.80it/s, loss=4.0439]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 15/698 [00:12<06:20,  1.80it/s, loss=5.1955]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 16/698 [00:12<06:34,  1.73it/s, loss=5.1955]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 16/698 [00:12<06:34,  1.73it/s, loss=5.9755]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 17/698 [00:12<06:22,  1.78it/s, loss=5.9755]\u001b[A\n",
            "Train Epoch 1/3:   2%|▏         | 17/698 [00:13<06:22,  1.78it/s, loss=4.6161]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 18/698 [00:13<06:40,  1.70it/s, loss=4.6161]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 18/698 [00:14<06:40,  1.70it/s, loss=5.9894]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 19/698 [00:14<06:54,  1.64it/s, loss=5.9894]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 19/698 [00:14<06:54,  1.64it/s, loss=4.1403]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 20/698 [00:14<06:30,  1.73it/s, loss=4.1403]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 20/698 [00:15<06:30,  1.73it/s, loss=4.5804]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 21/698 [00:15<06:46,  1.66it/s, loss=4.5804]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 21/698 [00:15<06:46,  1.66it/s, loss=4.7350]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 22/698 [00:15<06:58,  1.62it/s, loss=4.7350]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 22/698 [00:16<06:58,  1.62it/s, loss=4.3760]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 23/698 [00:16<06:55,  1.63it/s, loss=4.3760]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 23/698 [00:17<06:55,  1.63it/s, loss=6.3071]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 24/698 [00:17<07:30,  1.50it/s, loss=6.3071]\u001b[A\n",
            "Train Epoch 1/3:   3%|▎         | 24/698 [00:17<07:30,  1.50it/s, loss=5.1956]\u001b[A\n",
            "Train Epoch 1/3:   4%|▎         | 25/698 [00:17<07:30,  1.50it/s, loss=5.1956]\u001b[A\n",
            "Train Epoch 1/3:   4%|▎         | 25/698 [00:18<07:30,  1.50it/s, loss=3.9656]\u001b[A\n",
            "Train Epoch 1/3:   4%|▎         | 26/698 [00:18<07:27,  1.50it/s, loss=3.9656]\u001b[A\n",
            "Train Epoch 1/3:   4%|▎         | 26/698 [00:19<07:27,  1.50it/s, loss=4.8151]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 27/698 [00:19<07:07,  1.57it/s, loss=4.8151]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 27/698 [00:19<07:07,  1.57it/s, loss=3.9783]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 28/698 [00:19<06:44,  1.66it/s, loss=3.9783]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 28/698 [00:20<06:44,  1.66it/s, loss=4.9098]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 29/698 [00:20<06:37,  1.68it/s, loss=4.9098]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 29/698 [00:20<06:37,  1.68it/s, loss=5.2238]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 30/698 [00:20<06:12,  1.80it/s, loss=5.2238]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 30/698 [00:21<06:12,  1.80it/s, loss=4.1277]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 31/698 [00:21<05:57,  1.87it/s, loss=4.1277]\u001b[A\n",
            "Train Epoch 1/3:   4%|▍         | 31/698 [00:21<05:57,  1.87it/s, loss=3.7773]\u001b[A\n",
            "Train Epoch 1/3:   5%|▍         | 32/698 [00:21<06:08,  1.81it/s, loss=3.7773]\u001b[A\n",
            "Train Epoch 1/3:   5%|▍         | 32/698 [00:22<06:08,  1.81it/s, loss=5.6217]\u001b[A\n",
            "Train Epoch 1/3:   5%|▍         | 33/698 [00:22<05:51,  1.89it/s, loss=5.6217]\u001b[A\n",
            "Train Epoch 1/3:   5%|▍         | 33/698 [00:22<05:51,  1.89it/s, loss=3.9265]\u001b[A\n",
            "Train Epoch 1/3:   5%|▍         | 34/698 [00:22<05:40,  1.95it/s, loss=3.9265]\u001b[A\n",
            "Train Epoch 1/3:   5%|▍         | 34/698 [00:23<05:40,  1.95it/s, loss=5.7716]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 35/698 [00:23<05:41,  1.94it/s, loss=5.7716]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 35/698 [00:23<05:41,  1.94it/s, loss=5.5331]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 36/698 [00:23<05:54,  1.87it/s, loss=5.5331]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 36/698 [00:24<05:54,  1.87it/s, loss=4.4229]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 37/698 [00:24<05:41,  1.93it/s, loss=4.4229]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 37/698 [00:24<05:41,  1.93it/s, loss=4.7406]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 38/698 [00:25<06:10,  1.78it/s, loss=4.7406]\u001b[A\n",
            "Train Epoch 1/3:   5%|▌         | 38/698 [00:25<06:10,  1.78it/s, loss=4.2178]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 39/698 [00:25<05:51,  1.88it/s, loss=4.2178]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 39/698 [00:26<05:51,  1.88it/s, loss=4.2008]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 40/698 [00:26<06:00,  1.82it/s, loss=4.2008]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 40/698 [00:26<06:00,  1.82it/s, loss=5.4263]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 41/698 [00:26<05:52,  1.86it/s, loss=5.4263]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 41/698 [00:27<05:52,  1.86it/s, loss=3.7674]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 42/698 [00:27<05:38,  1.94it/s, loss=3.7674]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 42/698 [00:27<05:38,  1.94it/s, loss=4.2177]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 43/698 [00:27<05:30,  1.98it/s, loss=4.2177]\u001b[A\n",
            "Train Epoch 1/3:   6%|▌         | 43/698 [00:28<05:30,  1.98it/s, loss=4.0838]\u001b[A\n",
            "Train Epoch 1/3:   6%|▋         | 44/698 [00:28<05:57,  1.83it/s, loss=4.0838]\u001b[A\n",
            "Train Epoch 1/3:   6%|▋         | 44/698 [00:28<05:57,  1.83it/s, loss=3.4755]\u001b[A\n",
            "Train Epoch 1/3:   6%|▋         | 45/698 [00:28<06:17,  1.73it/s, loss=3.4755]\u001b[A\n",
            "Train Epoch 1/3:   6%|▋         | 45/698 [00:29<06:17,  1.73it/s, loss=4.3363]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 46/698 [00:29<06:40,  1.63it/s, loss=4.3363]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 46/698 [00:30<06:40,  1.63it/s, loss=4.5032]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 47/698 [00:30<07:12,  1.51it/s, loss=4.5032]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 47/698 [00:30<07:12,  1.51it/s, loss=5.9763]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 48/698 [00:30<07:11,  1.51it/s, loss=5.9763]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 48/698 [00:31<07:11,  1.51it/s, loss=4.0414]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 49/698 [00:31<06:45,  1.60it/s, loss=4.0414]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 49/698 [00:32<06:45,  1.60it/s, loss=3.2613]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 50/698 [00:32<06:43,  1.60it/s, loss=3.2613]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 50/698 [00:32<06:43,  1.60it/s, loss=3.8204]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 51/698 [00:32<06:24,  1.68it/s, loss=3.8204]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 51/698 [00:33<06:24,  1.68it/s, loss=5.0963]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 52/698 [00:33<06:02,  1.78it/s, loss=5.0963]\u001b[A\n",
            "Train Epoch 1/3:   7%|▋         | 52/698 [00:33<06:02,  1.78it/s, loss=3.9899]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 53/698 [00:33<05:54,  1.82it/s, loss=3.9899]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 53/698 [00:34<05:54,  1.82it/s, loss=3.5317]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 54/698 [00:34<06:08,  1.75it/s, loss=3.5317]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 54/698 [00:34<06:08,  1.75it/s, loss=3.8465]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 55/698 [00:34<05:50,  1.84it/s, loss=3.8465]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 55/698 [00:35<05:50,  1.84it/s, loss=3.9561]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 56/698 [00:35<05:55,  1.81it/s, loss=3.9561]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 56/698 [00:35<05:55,  1.81it/s, loss=5.4854]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 57/698 [00:35<05:49,  1.83it/s, loss=5.4854]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 57/698 [00:36<05:49,  1.83it/s, loss=3.9786]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 58/698 [00:36<05:42,  1.87it/s, loss=3.9786]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 58/698 [00:36<05:42,  1.87it/s, loss=3.7271]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 59/698 [00:36<05:39,  1.88it/s, loss=3.7271]\u001b[A\n",
            "Train Epoch 1/3:   8%|▊         | 59/698 [00:37<05:39,  1.88it/s, loss=4.0586]\u001b[A\n",
            "Train Epoch 1/3:   9%|▊         | 60/698 [00:37<05:31,  1.93it/s, loss=4.0586]\u001b[A\n",
            "Train Epoch 1/3:   9%|▊         | 60/698 [00:37<05:31,  1.93it/s, loss=4.5221]\u001b[A\n",
            "Train Epoch 1/3:   9%|▊         | 61/698 [00:37<05:31,  1.92it/s, loss=4.5221]\u001b[A\n",
            "Train Epoch 1/3:   9%|▊         | 61/698 [00:38<05:31,  1.92it/s, loss=4.2515]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 62/698 [00:38<05:25,  1.95it/s, loss=4.2515]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 62/698 [00:38<05:25,  1.95it/s, loss=3.9450]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 63/698 [00:38<05:20,  1.98it/s, loss=3.9450]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 63/698 [00:39<05:20,  1.98it/s, loss=3.5654]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 64/698 [00:39<05:32,  1.91it/s, loss=3.5654]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 64/698 [00:39<05:32,  1.91it/s, loss=3.7374]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 65/698 [00:39<05:27,  1.93it/s, loss=3.7374]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 65/698 [00:40<05:27,  1.93it/s, loss=3.5298]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 66/698 [00:40<05:41,  1.85it/s, loss=3.5298]\u001b[A\n",
            "Train Epoch 1/3:   9%|▉         | 66/698 [00:41<05:41,  1.85it/s, loss=3.4273]\u001b[A\n",
            "Train Epoch 1/3:  10%|▉         | 67/698 [00:41<06:44,  1.56it/s, loss=3.4273]\u001b[A\n",
            "Train Epoch 1/3:  10%|▉         | 67/698 [00:41<06:44,  1.56it/s, loss=3.7912]\u001b[A\n",
            "Train Epoch 1/3:  10%|▉         | 68/698 [00:41<06:30,  1.61it/s, loss=3.7912]\u001b[A\n",
            "Train Epoch 1/3:  10%|▉         | 68/698 [00:42<06:30,  1.61it/s, loss=4.5946]\u001b[A\n",
            "Train Epoch 1/3:  10%|▉         | 69/698 [00:42<06:25,  1.63it/s, loss=4.5946]\u001b[A\n",
            "Train Epoch 1/3:  10%|▉         | 69/698 [00:43<06:25,  1.63it/s, loss=4.2666]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 70/698 [00:43<06:52,  1.52it/s, loss=4.2666]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 70/698 [00:43<06:52,  1.52it/s, loss=4.8352]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 71/698 [00:43<06:38,  1.57it/s, loss=4.8352]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 71/698 [00:44<06:38,  1.57it/s, loss=3.3531]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 72/698 [00:44<06:21,  1.64it/s, loss=3.3531]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 72/698 [00:44<06:21,  1.64it/s, loss=3.2919]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 73/698 [00:44<06:02,  1.72it/s, loss=3.2919]\u001b[A\n",
            "Train Epoch 1/3:  10%|█         | 73/698 [00:45<06:02,  1.72it/s, loss=3.5208]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 74/698 [00:45<05:52,  1.77it/s, loss=3.5208]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 74/698 [00:46<05:52,  1.77it/s, loss=3.8823]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 75/698 [00:46<06:17,  1.65it/s, loss=3.8823]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 75/698 [00:46<06:17,  1.65it/s, loss=4.3372]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 76/698 [00:46<05:56,  1.74it/s, loss=4.3372]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 76/698 [00:47<05:56,  1.74it/s, loss=4.7563]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 77/698 [00:47<05:45,  1.80it/s, loss=4.7563]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 77/698 [00:47<05:45,  1.80it/s, loss=3.1725]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 78/698 [00:47<05:39,  1.82it/s, loss=3.1725]\u001b[A\n",
            "Train Epoch 1/3:  11%|█         | 78/698 [00:48<05:39,  1.82it/s, loss=4.0376]\u001b[A\n",
            "Train Epoch 1/3:  11%|█▏        | 79/698 [00:48<06:29,  1.59it/s, loss=4.0376]\u001b[A\n",
            "Train Epoch 1/3:  11%|█▏        | 79/698 [00:49<06:29,  1.59it/s, loss=3.0937]\u001b[A\n",
            "Train Epoch 1/3:  11%|█▏        | 80/698 [00:49<06:25,  1.60it/s, loss=3.0937]\u001b[A\n",
            "Train Epoch 1/3:  11%|█▏        | 80/698 [00:49<06:25,  1.60it/s, loss=4.8425]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 81/698 [00:49<06:14,  1.65it/s, loss=4.8425]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 81/698 [00:50<06:14,  1.65it/s, loss=2.9609]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 82/698 [00:50<05:57,  1.72it/s, loss=2.9609]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 82/698 [00:50<05:57,  1.72it/s, loss=3.9986]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 83/698 [00:50<05:58,  1.72it/s, loss=3.9986]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 83/698 [00:51<05:58,  1.72it/s, loss=3.9281]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 84/698 [00:51<05:54,  1.73it/s, loss=3.9281]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 84/698 [00:52<05:54,  1.73it/s, loss=6.9005]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 85/698 [00:52<05:58,  1.71it/s, loss=6.9005]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 85/698 [00:52<05:58,  1.71it/s, loss=4.5454]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 86/698 [00:52<05:34,  1.83it/s, loss=4.5454]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 86/698 [00:52<05:34,  1.83it/s, loss=3.3498]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 87/698 [00:52<05:26,  1.87it/s, loss=3.3498]\u001b[A\n",
            "Train Epoch 1/3:  12%|█▏        | 87/698 [00:53<05:26,  1.87it/s, loss=3.4505]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 88/698 [00:53<05:35,  1.82it/s, loss=3.4505]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 88/698 [00:54<05:35,  1.82it/s, loss=3.4088]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 89/698 [00:54<06:00,  1.69it/s, loss=3.4088]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 89/698 [00:54<06:00,  1.69it/s, loss=3.4392]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 90/698 [00:54<06:16,  1.62it/s, loss=3.4392]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 90/698 [00:55<06:16,  1.62it/s, loss=2.9161]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 91/698 [00:55<06:22,  1.59it/s, loss=2.9161]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 91/698 [00:56<06:22,  1.59it/s, loss=3.2657]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 92/698 [00:56<06:44,  1.50it/s, loss=3.2657]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 92/698 [00:56<06:44,  1.50it/s, loss=2.8033]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 93/698 [00:56<06:28,  1.56it/s, loss=2.8033]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 93/698 [00:57<06:28,  1.56it/s, loss=3.6102]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 94/698 [00:57<06:08,  1.64it/s, loss=3.6102]\u001b[A\n",
            "Train Epoch 1/3:  13%|█▎        | 94/698 [00:58<06:08,  1.64it/s, loss=2.7666]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▎        | 95/698 [00:58<06:02,  1.66it/s, loss=2.7666]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▎        | 95/698 [00:58<06:02,  1.66it/s, loss=3.1401]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 96/698 [00:58<05:47,  1.73it/s, loss=3.1401]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 96/698 [00:59<05:47,  1.73it/s, loss=3.3799]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 97/698 [00:59<05:32,  1.81it/s, loss=3.3799]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 97/698 [00:59<05:32,  1.81it/s, loss=3.4609]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 98/698 [00:59<05:29,  1.82it/s, loss=3.4609]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 98/698 [01:00<05:29,  1.82it/s, loss=3.2527]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 99/698 [01:00<05:28,  1.82it/s, loss=3.2527]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 99/698 [01:00<05:28,  1.82it/s, loss=3.7519]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 100/698 [01:00<05:21,  1.86it/s, loss=3.7519]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 100/698 [01:01<05:21,  1.86it/s, loss=4.0279]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 101/698 [01:01<05:11,  1.91it/s, loss=4.0279]\u001b[A\n",
            "Train Epoch 1/3:  14%|█▍        | 101/698 [01:01<05:11,  1.91it/s, loss=3.2632]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▍        | 102/698 [01:01<05:18,  1.87it/s, loss=3.2632]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▍        | 102/698 [01:02<05:18,  1.87it/s, loss=3.3977]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▍        | 103/698 [01:02<05:42,  1.74it/s, loss=3.3977]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▍        | 103/698 [01:02<05:42,  1.74it/s, loss=2.8456]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▍        | 104/698 [01:02<05:30,  1.80it/s, loss=2.8456]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▍        | 104/698 [01:03<05:30,  1.80it/s, loss=3.4812]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 105/698 [01:03<05:20,  1.85it/s, loss=3.4812]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 105/698 [01:03<05:20,  1.85it/s, loss=3.4807]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 106/698 [01:03<05:20,  1.84it/s, loss=3.4807]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 106/698 [01:04<05:20,  1.84it/s, loss=2.6061]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 107/698 [01:04<05:10,  1.90it/s, loss=2.6061]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 107/698 [01:05<05:10,  1.90it/s, loss=4.7331]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 108/698 [01:05<05:34,  1.76it/s, loss=4.7331]\u001b[A\n",
            "Train Epoch 1/3:  15%|█▌        | 108/698 [01:07<05:34,  1.76it/s, loss=3.3355]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 109/698 [01:07<09:39,  1.02it/s, loss=3.3355]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 109/698 [01:10<09:39,  1.02it/s, loss=2.7569]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 110/698 [01:10<16:17,  1.66s/it, loss=2.7569]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 110/698 [01:12<16:17,  1.66s/it, loss=4.3851]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 111/698 [01:12<16:33,  1.69s/it, loss=4.3851]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 111/698 [01:13<16:33,  1.69s/it, loss=3.1923]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 112/698 [01:13<15:25,  1.58s/it, loss=3.1923]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 112/698 [01:15<15:25,  1.58s/it, loss=4.6719]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 113/698 [01:15<16:25,  1.68s/it, loss=4.6719]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▌        | 113/698 [01:16<16:25,  1.68s/it, loss=2.4819]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▋        | 114/698 [01:16<13:46,  1.41s/it, loss=2.4819]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▋        | 114/698 [01:16<13:46,  1.41s/it, loss=5.5404]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▋        | 115/698 [01:16<11:15,  1.16s/it, loss=5.5404]\u001b[A\n",
            "Train Epoch 1/3:  16%|█▋        | 115/698 [01:17<11:15,  1.16s/it, loss=3.5592]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 116/698 [01:17<09:35,  1.01it/s, loss=3.5592]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 116/698 [01:17<09:35,  1.01it/s, loss=2.6982]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 117/698 [01:17<08:06,  1.19it/s, loss=2.6982]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 117/698 [01:18<08:06,  1.19it/s, loss=2.8334]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 118/698 [01:18<07:22,  1.31it/s, loss=2.8334]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 118/698 [01:18<07:22,  1.31it/s, loss=2.4156]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 119/698 [01:18<06:40,  1.44it/s, loss=2.4156]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 119/698 [01:19<06:40,  1.44it/s, loss=2.5229]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 120/698 [01:19<06:14,  1.54it/s, loss=2.5229]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 120/698 [01:19<06:14,  1.54it/s, loss=3.6926]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 121/698 [01:19<05:38,  1.70it/s, loss=3.6926]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 121/698 [01:20<05:38,  1.70it/s, loss=2.8570]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 122/698 [01:20<05:23,  1.78it/s, loss=2.8570]\u001b[A\n",
            "Train Epoch 1/3:  17%|█▋        | 122/698 [01:21<05:23,  1.78it/s, loss=2.2711]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 123/698 [01:21<05:41,  1.68it/s, loss=2.2711]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 123/698 [01:21<05:41,  1.68it/s, loss=2.9603]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 124/698 [01:21<05:50,  1.64it/s, loss=2.9603]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 124/698 [01:22<05:50,  1.64it/s, loss=2.2981]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 125/698 [01:22<06:01,  1.58it/s, loss=2.2981]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 125/698 [01:23<06:01,  1.58it/s, loss=2.4025]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 126/698 [01:23<06:08,  1.55it/s, loss=2.4025]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 126/698 [01:23<06:08,  1.55it/s, loss=4.0199]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 127/698 [01:23<06:41,  1.42it/s, loss=4.0199]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 127/698 [01:24<06:41,  1.42it/s, loss=2.0085]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 128/698 [01:24<06:01,  1.58it/s, loss=2.0085]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 128/698 [01:24<06:01,  1.58it/s, loss=3.0219]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 129/698 [01:24<05:38,  1.68it/s, loss=3.0219]\u001b[A\n",
            "Train Epoch 1/3:  18%|█▊        | 129/698 [01:25<05:38,  1.68it/s, loss=2.6302]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▊        | 130/698 [01:25<05:33,  1.70it/s, loss=2.6302]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▊        | 130/698 [01:25<05:33,  1.70it/s, loss=2.9605]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 131/698 [01:25<05:16,  1.79it/s, loss=2.9605]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 131/698 [01:26<05:16,  1.79it/s, loss=5.0120]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 132/698 [01:26<05:13,  1.81it/s, loss=5.0120]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 132/698 [01:26<05:13,  1.81it/s, loss=2.3337]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 133/698 [01:26<05:01,  1.88it/s, loss=2.3337]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 133/698 [01:27<05:01,  1.88it/s, loss=3.6282]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 134/698 [01:27<04:52,  1.93it/s, loss=3.6282]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 134/698 [01:27<04:52,  1.93it/s, loss=2.4971]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 135/698 [01:27<04:47,  1.96it/s, loss=2.4971]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 135/698 [01:28<04:47,  1.96it/s, loss=3.3274]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 136/698 [01:28<04:46,  1.96it/s, loss=3.3274]\u001b[A\n",
            "Train Epoch 1/3:  19%|█▉        | 136/698 [01:28<04:46,  1.96it/s, loss=4.0049]\u001b[A\n",
            "Train Epoch 1/3:  20%|█▉        | 137/698 [01:28<04:38,  2.01it/s, loss=4.0049]\u001b[A\n",
            "Train Epoch 1/3:  20%|█▉        | 137/698 [01:29<04:38,  2.01it/s, loss=2.9206]\u001b[A\n",
            "Train Epoch 1/3:  20%|█▉        | 138/698 [01:29<04:50,  1.93it/s, loss=2.9206]\u001b[A\n",
            "Train Epoch 1/3:  20%|█▉        | 138/698 [01:30<04:50,  1.93it/s, loss=3.2844]\u001b[A\n",
            "Train Epoch 1/3:  20%|█▉        | 139/698 [01:30<04:56,  1.89it/s, loss=3.2844]\u001b[A\n",
            "Train Epoch 1/3:  20%|█▉        | 139/698 [01:30<04:56,  1.89it/s, loss=2.3451]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 140/698 [01:30<05:05,  1.83it/s, loss=2.3451]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 140/698 [01:31<05:05,  1.83it/s, loss=3.1291]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 141/698 [01:31<05:14,  1.77it/s, loss=3.1291]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 141/698 [01:31<05:14,  1.77it/s, loss=3.7563]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 142/698 [01:31<05:18,  1.75it/s, loss=3.7563]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 142/698 [01:32<05:18,  1.75it/s, loss=3.2499]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 143/698 [01:32<05:03,  1.83it/s, loss=3.2499]\u001b[A\n",
            "Train Epoch 1/3:  20%|██        | 143/698 [01:32<05:03,  1.83it/s, loss=2.7588]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 144/698 [01:32<04:55,  1.87it/s, loss=2.7588]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 144/698 [01:33<04:55,  1.87it/s, loss=2.8709]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 145/698 [01:33<04:58,  1.85it/s, loss=2.8709]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 145/698 [01:33<04:58,  1.85it/s, loss=2.5268]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 146/698 [01:33<05:10,  1.78it/s, loss=2.5268]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 146/698 [01:34<05:10,  1.78it/s, loss=2.8359]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 147/698 [01:34<05:20,  1.72it/s, loss=2.8359]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 147/698 [01:35<05:20,  1.72it/s, loss=2.8057]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 148/698 [01:35<05:58,  1.53it/s, loss=2.8057]\u001b[A\n",
            "Train Epoch 1/3:  21%|██        | 148/698 [01:36<05:58,  1.53it/s, loss=2.6889]\u001b[A\n",
            "Train Epoch 1/3:  21%|██▏       | 149/698 [01:36<06:10,  1.48it/s, loss=2.6889]\u001b[A\n",
            "Train Epoch 1/3:  21%|██▏       | 149/698 [01:36<06:10,  1.48it/s, loss=3.4343]\u001b[A\n",
            "Train Epoch 1/3:  21%|██▏       | 150/698 [01:36<06:19,  1.44it/s, loss=3.4343]\u001b[A\n",
            "Train Epoch 1/3:  21%|██▏       | 150/698 [01:37<06:19,  1.44it/s, loss=3.2017]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 151/698 [01:37<05:47,  1.57it/s, loss=3.2017]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 151/698 [01:37<05:47,  1.57it/s, loss=1.9428]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 152/698 [01:37<05:19,  1.71it/s, loss=1.9428]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 152/698 [01:38<05:19,  1.71it/s, loss=1.9757]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 153/698 [01:38<05:20,  1.70it/s, loss=1.9757]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 153/698 [01:38<05:20,  1.70it/s, loss=3.0182]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 154/698 [01:38<05:12,  1.74it/s, loss=3.0182]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 154/698 [01:39<05:12,  1.74it/s, loss=2.6284]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 155/698 [01:39<05:00,  1.81it/s, loss=2.6284]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 155/698 [01:40<05:00,  1.81it/s, loss=2.3252]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 156/698 [01:40<05:01,  1.80it/s, loss=2.3252]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 156/698 [01:40<05:01,  1.80it/s, loss=1.7543]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 157/698 [01:40<04:52,  1.85it/s, loss=1.7543]\u001b[A\n",
            "Train Epoch 1/3:  22%|██▏       | 157/698 [01:41<04:52,  1.85it/s, loss=3.7283]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 158/698 [01:41<04:53,  1.84it/s, loss=3.7283]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 158/698 [01:41<04:53,  1.84it/s, loss=2.0822]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 159/698 [01:41<04:48,  1.87it/s, loss=2.0822]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 159/698 [01:42<04:48,  1.87it/s, loss=3.1320]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 160/698 [01:42<04:47,  1.87it/s, loss=3.1320]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 160/698 [01:42<04:47,  1.87it/s, loss=3.3971]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 161/698 [01:42<04:48,  1.86it/s, loss=3.3971]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 161/698 [01:43<04:48,  1.86it/s, loss=2.3970]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 162/698 [01:43<04:59,  1.79it/s, loss=2.3970]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 162/698 [01:43<04:59,  1.79it/s, loss=2.4402]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 163/698 [01:43<04:49,  1.85it/s, loss=2.4402]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 163/698 [01:44<04:49,  1.85it/s, loss=1.9916]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 164/698 [01:44<04:51,  1.83it/s, loss=1.9916]\u001b[A\n",
            "Train Epoch 1/3:  23%|██▎       | 164/698 [01:44<04:51,  1.83it/s, loss=3.0467]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▎       | 165/698 [01:44<04:37,  1.92it/s, loss=3.0467]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▎       | 165/698 [01:45<04:37,  1.92it/s, loss=3.1360]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 166/698 [01:45<04:30,  1.97it/s, loss=3.1360]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 166/698 [01:45<04:30,  1.97it/s, loss=4.2169]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 167/698 [01:45<04:40,  1.90it/s, loss=4.2169]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 167/698 [01:46<04:40,  1.90it/s, loss=2.7231]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 168/698 [01:46<04:46,  1.85it/s, loss=2.7231]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 168/698 [01:47<04:46,  1.85it/s, loss=1.8628]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 169/698 [01:47<05:00,  1.76it/s, loss=1.8628]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 169/698 [01:47<05:00,  1.76it/s, loss=2.5215]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 170/698 [01:47<05:21,  1.64it/s, loss=2.5215]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 170/698 [01:48<05:21,  1.64it/s, loss=3.3560]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 171/698 [01:48<05:19,  1.65it/s, loss=3.3560]\u001b[A\n",
            "Train Epoch 1/3:  24%|██▍       | 171/698 [01:49<05:19,  1.65it/s, loss=2.6790]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▍       | 172/698 [01:49<05:28,  1.60it/s, loss=2.6790]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▍       | 172/698 [01:49<05:28,  1.60it/s, loss=1.9548]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▍       | 173/698 [01:49<05:34,  1.57it/s, loss=1.9548]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▍       | 173/698 [01:50<05:34,  1.57it/s, loss=2.5020]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▍       | 174/698 [01:50<05:30,  1.59it/s, loss=2.5020]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▍       | 174/698 [01:50<05:30,  1.59it/s, loss=2.9165]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▌       | 175/698 [01:50<05:16,  1.65it/s, loss=2.9165]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▌       | 175/698 [01:51<05:16,  1.65it/s, loss=2.3854]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▌       | 176/698 [01:51<05:13,  1.67it/s, loss=2.3854]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▌       | 176/698 [01:52<05:13,  1.67it/s, loss=3.9391]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▌       | 177/698 [01:52<05:14,  1.66it/s, loss=3.9391]\u001b[A\n",
            "Train Epoch 1/3:  25%|██▌       | 177/698 [01:52<05:14,  1.66it/s, loss=2.3612]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 178/698 [01:52<05:11,  1.67it/s, loss=2.3612]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 178/698 [01:53<05:11,  1.67it/s, loss=2.2036]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 179/698 [01:53<05:07,  1.69it/s, loss=2.2036]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 179/698 [01:53<05:07,  1.69it/s, loss=2.6643]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 180/698 [01:53<04:52,  1.77it/s, loss=2.6643]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 180/698 [01:54<04:52,  1.77it/s, loss=3.0376]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 181/698 [01:54<05:12,  1.65it/s, loss=3.0376]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 181/698 [01:54<05:12,  1.65it/s, loss=1.7857]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 182/698 [01:54<04:57,  1.73it/s, loss=1.7857]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 182/698 [01:55<04:57,  1.73it/s, loss=3.8257]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 183/698 [01:55<04:45,  1.81it/s, loss=3.8257]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▌       | 183/698 [01:56<04:45,  1.81it/s, loss=2.9585]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▋       | 184/698 [01:56<04:53,  1.75it/s, loss=2.9585]\u001b[A\n",
            "Train Epoch 1/3:  26%|██▋       | 184/698 [01:56<04:53,  1.75it/s, loss=3.0610]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 185/698 [01:56<04:36,  1.86it/s, loss=3.0610]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 185/698 [01:56<04:36,  1.86it/s, loss=2.0425]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 186/698 [01:56<04:26,  1.92it/s, loss=2.0425]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 186/698 [01:57<04:26,  1.92it/s, loss=2.9174]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 187/698 [01:57<04:20,  1.97it/s, loss=2.9174]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 187/698 [01:57<04:20,  1.97it/s, loss=2.4915]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 188/698 [01:57<04:21,  1.95it/s, loss=2.4915]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 188/698 [01:58<04:21,  1.95it/s, loss=3.7424]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 189/698 [01:58<04:14,  2.00it/s, loss=3.7424]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 189/698 [01:59<04:14,  2.00it/s, loss=1.7964]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 190/698 [01:59<04:22,  1.94it/s, loss=1.7964]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 190/698 [01:59<04:22,  1.94it/s, loss=2.9753]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 191/698 [01:59<04:32,  1.86it/s, loss=2.9753]\u001b[A\n",
            "Train Epoch 1/3:  27%|██▋       | 191/698 [02:00<04:32,  1.86it/s, loss=1.9215]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 192/698 [02:00<04:54,  1.72it/s, loss=1.9215]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 192/698 [02:00<04:54,  1.72it/s, loss=2.9935]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 193/698 [02:00<05:08,  1.64it/s, loss=2.9935]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 193/698 [02:01<05:08,  1.64it/s, loss=3.1583]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 194/698 [02:01<05:04,  1.66it/s, loss=3.1583]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 194/698 [02:02<05:04,  1.66it/s, loss=2.4142]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 195/698 [02:02<05:17,  1.58it/s, loss=2.4142]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 195/698 [02:02<05:17,  1.58it/s, loss=1.5133]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 196/698 [02:02<05:35,  1.50it/s, loss=1.5133]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 196/698 [02:03<05:35,  1.50it/s, loss=1.8205]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 197/698 [02:03<05:05,  1.64it/s, loss=1.8205]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 197/698 [02:04<05:05,  1.64it/s, loss=3.0364]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 198/698 [02:04<05:18,  1.57it/s, loss=3.0364]\u001b[A\n",
            "Train Epoch 1/3:  28%|██▊       | 198/698 [02:04<05:18,  1.57it/s, loss=2.2418]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▊       | 199/698 [02:04<04:58,  1.67it/s, loss=2.2418]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▊       | 199/698 [02:05<04:58,  1.67it/s, loss=2.5145]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▊       | 200/698 [02:05<04:40,  1.78it/s, loss=2.5145]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▊       | 200/698 [02:05<04:40,  1.78it/s, loss=2.7984]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 201/698 [02:05<04:42,  1.76it/s, loss=2.7984]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 201/698 [02:06<04:42,  1.76it/s, loss=2.3558]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 202/698 [02:06<04:30,  1.83it/s, loss=2.3558]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 202/698 [02:06<04:30,  1.83it/s, loss=2.6747]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 203/698 [02:06<04:35,  1.80it/s, loss=2.6747]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 203/698 [02:07<04:35,  1.80it/s, loss=1.6586]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 204/698 [02:07<04:34,  1.80it/s, loss=1.6586]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 204/698 [02:07<04:34,  1.80it/s, loss=1.7040]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 205/698 [02:07<04:30,  1.82it/s, loss=1.7040]\u001b[A\n",
            "Train Epoch 1/3:  29%|██▉       | 205/698 [02:08<04:30,  1.82it/s, loss=2.8913]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 206/698 [02:08<04:22,  1.88it/s, loss=2.8913]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 206/698 [02:08<04:22,  1.88it/s, loss=2.3161]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 207/698 [02:08<04:15,  1.92it/s, loss=2.3161]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 207/698 [02:09<04:15,  1.92it/s, loss=3.5133]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 208/698 [02:09<04:34,  1.78it/s, loss=3.5133]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 208/698 [02:10<04:34,  1.78it/s, loss=1.7305]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 209/698 [02:10<04:38,  1.76it/s, loss=1.7305]\u001b[A\n",
            "Train Epoch 1/3:  30%|██▉       | 209/698 [02:10<04:38,  1.76it/s, loss=2.0246]\u001b[A\n",
            "Train Epoch 1/3:  30%|███       | 210/698 [02:10<04:40,  1.74it/s, loss=2.0246]\u001b[A\n",
            "Train Epoch 1/3:  30%|███       | 210/698 [02:11<04:40,  1.74it/s, loss=2.8033]\u001b[A\n",
            "Train Epoch 1/3:  30%|███       | 211/698 [02:11<04:29,  1.81it/s, loss=2.8033]\u001b[A\n",
            "Train Epoch 1/3:  30%|███       | 211/698 [02:11<04:29,  1.81it/s, loss=2.5419]\u001b[A\n",
            "Train Epoch 1/3:  30%|███       | 212/698 [02:11<04:17,  1.88it/s, loss=2.5419]\u001b[A\n",
            "Train Epoch 1/3:  30%|███       | 212/698 [02:12<04:17,  1.88it/s, loss=1.6938]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 213/698 [02:12<04:06,  1.97it/s, loss=1.6938]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 213/698 [02:12<04:06,  1.97it/s, loss=2.2040]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 214/698 [02:12<04:02,  2.00it/s, loss=2.2040]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 214/698 [02:13<04:02,  2.00it/s, loss=3.5015]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 215/698 [02:13<04:28,  1.80it/s, loss=3.5015]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 215/698 [02:14<04:28,  1.80it/s, loss=1.8815]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 216/698 [02:14<04:49,  1.66it/s, loss=1.8815]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 216/698 [02:14<04:49,  1.66it/s, loss=1.5127]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 217/698 [02:14<05:17,  1.52it/s, loss=1.5127]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 217/698 [02:15<05:17,  1.52it/s, loss=1.6603]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 218/698 [02:15<05:13,  1.53it/s, loss=1.6603]\u001b[A\n",
            "Train Epoch 1/3:  31%|███       | 218/698 [02:16<05:13,  1.53it/s, loss=1.4311]\u001b[A\n",
            "Train Epoch 1/3:  31%|███▏      | 219/698 [02:16<05:14,  1.53it/s, loss=1.4311]\u001b[A\n",
            "Train Epoch 1/3:  31%|███▏      | 219/698 [02:16<05:14,  1.53it/s, loss=3.1138]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 220/698 [02:16<04:44,  1.68it/s, loss=3.1138]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 220/698 [02:17<04:44,  1.68it/s, loss=2.3594]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 221/698 [02:17<04:26,  1.79it/s, loss=2.3594]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 221/698 [02:17<04:26,  1.79it/s, loss=1.9856]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 222/698 [02:17<04:16,  1.85it/s, loss=1.9856]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 222/698 [02:18<04:16,  1.85it/s, loss=2.2078]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 223/698 [02:18<04:26,  1.78it/s, loss=2.2078]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 223/698 [02:18<04:26,  1.78it/s, loss=4.3565]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 224/698 [02:18<04:24,  1.80it/s, loss=4.3565]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 224/698 [02:19<04:24,  1.80it/s, loss=3.1346]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 225/698 [02:19<04:16,  1.85it/s, loss=3.1346]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 225/698 [02:19<04:16,  1.85it/s, loss=2.2431]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 226/698 [02:19<04:15,  1.84it/s, loss=2.2431]\u001b[A\n",
            "Train Epoch 1/3:  32%|███▏      | 226/698 [02:20<04:15,  1.84it/s, loss=1.7472]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 227/698 [02:20<04:25,  1.78it/s, loss=1.7472]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 227/698 [02:20<04:25,  1.78it/s, loss=1.6457]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 228/698 [02:20<04:13,  1.85it/s, loss=1.6457]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 228/698 [02:21<04:13,  1.85it/s, loss=2.8699]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 229/698 [02:21<04:26,  1.76it/s, loss=2.8699]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 229/698 [02:22<04:26,  1.76it/s, loss=2.3801]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 230/698 [02:22<04:18,  1.81it/s, loss=2.3801]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 230/698 [02:22<04:18,  1.81it/s, loss=2.9221]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 231/698 [02:22<04:12,  1.85it/s, loss=2.9221]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 231/698 [02:23<04:12,  1.85it/s, loss=2.1544]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 232/698 [02:23<04:05,  1.90it/s, loss=2.1544]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 232/698 [02:23<04:05,  1.90it/s, loss=4.0235]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 233/698 [02:23<04:08,  1.87it/s, loss=4.0235]\u001b[A\n",
            "Train Epoch 1/3:  33%|███▎      | 233/698 [02:24<04:08,  1.87it/s, loss=1.9830]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▎      | 234/698 [02:24<04:05,  1.89it/s, loss=1.9830]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▎      | 234/698 [02:24<04:05,  1.89it/s, loss=3.9741]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▎      | 235/698 [02:24<04:22,  1.77it/s, loss=3.9741]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▎      | 235/698 [02:25<04:22,  1.77it/s, loss=1.8857]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 236/698 [02:25<04:04,  1.89it/s, loss=1.8857]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 236/698 [02:25<04:04,  1.89it/s, loss=1.8767]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 237/698 [02:25<03:57,  1.94it/s, loss=1.8767]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 237/698 [02:26<03:57,  1.94it/s, loss=3.2398]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 238/698 [02:26<04:16,  1.79it/s, loss=3.2398]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 238/698 [02:27<04:16,  1.79it/s, loss=2.3615]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 239/698 [02:27<06:11,  1.24it/s, loss=2.3615]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 239/698 [02:28<06:11,  1.24it/s, loss=2.0046]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 240/698 [02:28<05:51,  1.30it/s, loss=2.0046]\u001b[A\n",
            "Train Epoch 1/3:  34%|███▍      | 240/698 [02:29<05:51,  1.30it/s, loss=1.4456]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 241/698 [02:29<05:46,  1.32it/s, loss=1.4456]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 241/698 [02:29<05:46,  1.32it/s, loss=3.1557]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 242/698 [02:29<05:29,  1.38it/s, loss=3.1557]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 242/698 [02:30<05:29,  1.38it/s, loss=1.8550]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 243/698 [02:30<04:53,  1.55it/s, loss=1.8550]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 243/698 [02:30<04:53,  1.55it/s, loss=2.9745]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 244/698 [02:30<04:36,  1.64it/s, loss=2.9745]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▍      | 244/698 [02:31<04:36,  1.64it/s, loss=2.1422]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▌      | 245/698 [02:31<04:21,  1.73it/s, loss=2.1422]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▌      | 245/698 [02:31<04:21,  1.73it/s, loss=2.9194]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▌      | 246/698 [02:31<04:08,  1.82it/s, loss=2.9194]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▌      | 246/698 [02:32<04:08,  1.82it/s, loss=3.7137]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▌      | 247/698 [02:32<03:55,  1.91it/s, loss=3.7137]\u001b[A\n",
            "Train Epoch 1/3:  35%|███▌      | 247/698 [02:32<03:55,  1.91it/s, loss=1.7750]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 248/698 [02:32<03:50,  1.95it/s, loss=1.7750]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 248/698 [02:33<03:50,  1.95it/s, loss=1.4456]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 249/698 [02:33<03:54,  1.91it/s, loss=1.4456]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 249/698 [02:33<03:54,  1.91it/s, loss=2.1210]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 250/698 [02:33<03:49,  1.96it/s, loss=2.1210]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 250/698 [02:34<03:49,  1.96it/s, loss=1.8592]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 251/698 [02:34<04:05,  1.82it/s, loss=1.8592]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 251/698 [02:34<04:05,  1.82it/s, loss=1.9159]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 252/698 [02:34<04:03,  1.83it/s, loss=1.9159]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 252/698 [02:35<04:03,  1.83it/s, loss=2.1919]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 253/698 [02:35<04:07,  1.80it/s, loss=2.1919]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▌      | 253/698 [02:36<04:07,  1.80it/s, loss=2.3791]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▋      | 254/698 [02:36<04:13,  1.75it/s, loss=2.3791]\u001b[A\n",
            "Train Epoch 1/3:  36%|███▋      | 254/698 [02:36<04:13,  1.75it/s, loss=1.8503]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 255/698 [02:36<04:01,  1.83it/s, loss=1.8503]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 255/698 [02:37<04:01,  1.83it/s, loss=1.7220]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 256/698 [02:37<03:46,  1.95it/s, loss=1.7220]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 256/698 [02:37<03:46,  1.95it/s, loss=3.4725]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 257/698 [02:37<03:51,  1.90it/s, loss=3.4725]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 257/698 [02:38<03:51,  1.90it/s, loss=1.4002]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 258/698 [02:38<03:59,  1.84it/s, loss=1.4002]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 258/698 [02:38<03:59,  1.84it/s, loss=3.9889]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 259/698 [02:38<04:02,  1.81it/s, loss=3.9889]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 259/698 [02:39<04:02,  1.81it/s, loss=2.7501]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 260/698 [02:39<04:09,  1.75it/s, loss=2.7501]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 260/698 [02:39<04:09,  1.75it/s, loss=2.3614]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 261/698 [02:39<04:17,  1.70it/s, loss=2.3614]\u001b[A\n",
            "Train Epoch 1/3:  37%|███▋      | 261/698 [02:40<04:17,  1.70it/s, loss=1.8129]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 262/698 [02:40<04:19,  1.68it/s, loss=1.8129]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 262/698 [02:41<04:19,  1.68it/s, loss=2.6935]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 263/698 [02:41<04:17,  1.69it/s, loss=2.6935]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 263/698 [02:41<04:17,  1.69it/s, loss=2.0977]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 264/698 [02:41<04:30,  1.60it/s, loss=2.0977]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 264/698 [02:42<04:30,  1.60it/s, loss=2.7946]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 265/698 [02:42<04:24,  1.64it/s, loss=2.7946]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 265/698 [02:43<04:24,  1.64it/s, loss=3.0195]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 266/698 [02:43<04:29,  1.61it/s, loss=3.0195]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 266/698 [02:43<04:29,  1.61it/s, loss=1.5316]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 267/698 [02:43<04:17,  1.67it/s, loss=1.5316]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 267/698 [02:44<04:17,  1.67it/s, loss=2.0834]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 268/698 [02:44<04:01,  1.78it/s, loss=2.0834]\u001b[A\n",
            "Train Epoch 1/3:  38%|███▊      | 268/698 [02:44<04:01,  1.78it/s, loss=1.7744]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▊      | 269/698 [02:44<04:00,  1.78it/s, loss=1.7744]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▊      | 269/698 [02:45<04:00,  1.78it/s, loss=2.9858]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▊      | 270/698 [02:45<03:48,  1.87it/s, loss=2.9858]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▊      | 270/698 [02:45<03:48,  1.87it/s, loss=2.3998]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 271/698 [02:45<03:45,  1.89it/s, loss=2.3998]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 271/698 [02:46<03:45,  1.89it/s, loss=1.8876]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 272/698 [02:46<04:04,  1.74it/s, loss=1.8876]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 272/698 [02:46<04:04,  1.74it/s, loss=2.2508]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 273/698 [02:46<04:00,  1.77it/s, loss=2.2508]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 273/698 [02:47<04:00,  1.77it/s, loss=2.0674]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 274/698 [02:47<04:09,  1.70it/s, loss=2.0674]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 274/698 [02:48<04:09,  1.70it/s, loss=1.4617]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 275/698 [02:48<04:07,  1.71it/s, loss=1.4617]\u001b[A\n",
            "Train Epoch 1/3:  39%|███▉      | 275/698 [02:48<04:07,  1.71it/s, loss=1.7539]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 276/698 [02:48<04:03,  1.73it/s, loss=1.7539]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 276/698 [02:49<04:03,  1.73it/s, loss=1.5876]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 277/698 [02:49<03:50,  1.82it/s, loss=1.5876]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 277/698 [02:49<03:50,  1.82it/s, loss=2.9704]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 278/698 [02:49<03:46,  1.85it/s, loss=2.9704]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 278/698 [02:50<03:46,  1.85it/s, loss=2.2982]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 279/698 [02:50<03:40,  1.90it/s, loss=2.2982]\u001b[A\n",
            "Train Epoch 1/3:  40%|███▉      | 279/698 [02:50<03:40,  1.90it/s, loss=3.5371]\u001b[A\n",
            "Train Epoch 1/3:  40%|████      | 280/698 [02:50<03:32,  1.96it/s, loss=3.5371]\u001b[A\n",
            "Train Epoch 1/3:  40%|████      | 280/698 [02:51<03:32,  1.96it/s, loss=2.2073]\u001b[A\n",
            "Train Epoch 1/3:  40%|████      | 281/698 [02:51<03:30,  1.98it/s, loss=2.2073]\u001b[A\n",
            "Train Epoch 1/3:  40%|████      | 281/698 [02:51<03:30,  1.98it/s, loss=1.7378]\u001b[A\n",
            "Train Epoch 1/3:  40%|████      | 282/698 [02:51<03:31,  1.96it/s, loss=1.7378]\u001b[A\n",
            "Train Epoch 1/3:  40%|████      | 282/698 [02:52<03:31,  1.96it/s, loss=1.8126]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 283/698 [02:52<03:35,  1.93it/s, loss=1.8126]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 283/698 [02:52<03:35,  1.93it/s, loss=2.2304]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 284/698 [02:52<03:47,  1.82it/s, loss=2.2304]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 284/698 [02:53<03:47,  1.82it/s, loss=1.3988]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 285/698 [02:53<04:06,  1.68it/s, loss=1.3988]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 285/698 [02:54<04:06,  1.68it/s, loss=1.6715]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 286/698 [02:54<04:11,  1.64it/s, loss=1.6715]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 286/698 [02:54<04:11,  1.64it/s, loss=1.5745]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 287/698 [02:54<04:21,  1.57it/s, loss=1.5745]\u001b[A\n",
            "Train Epoch 1/3:  41%|████      | 287/698 [02:55<04:21,  1.57it/s, loss=1.1650]\u001b[A\n",
            "Train Epoch 1/3:  41%|████▏     | 288/698 [02:55<04:11,  1.63it/s, loss=1.1650]\u001b[A\n",
            "Train Epoch 1/3:  41%|████▏     | 288/698 [02:55<04:11,  1.63it/s, loss=2.0692]\u001b[A\n",
            "Train Epoch 1/3:  41%|████▏     | 289/698 [02:55<03:54,  1.75it/s, loss=2.0692]\u001b[A\n",
            "Train Epoch 1/3:  41%|████▏     | 289/698 [02:56<03:54,  1.75it/s, loss=1.4522]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 290/698 [02:56<03:48,  1.79it/s, loss=1.4522]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 290/698 [02:56<03:48,  1.79it/s, loss=1.1179]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 291/698 [02:56<03:44,  1.82it/s, loss=1.1179]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 291/698 [02:57<03:44,  1.82it/s, loss=2.9308]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 292/698 [02:57<03:35,  1.89it/s, loss=2.9308]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 292/698 [02:57<03:35,  1.89it/s, loss=1.5300]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 293/698 [02:57<03:37,  1.86it/s, loss=1.5300]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 293/698 [02:58<03:37,  1.86it/s, loss=3.9859]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 294/698 [02:58<03:30,  1.92it/s, loss=3.9859]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 294/698 [02:58<03:30,  1.92it/s, loss=1.3314]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 295/698 [02:58<03:29,  1.93it/s, loss=1.3314]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 295/698 [02:59<03:29,  1.93it/s, loss=1.1485]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 296/698 [02:59<03:35,  1.86it/s, loss=1.1485]\u001b[A\n",
            "Train Epoch 1/3:  42%|████▏     | 296/698 [03:00<03:35,  1.86it/s, loss=2.1079]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 297/698 [03:00<03:52,  1.73it/s, loss=2.1079]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 297/698 [03:00<03:52,  1.73it/s, loss=1.9904]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 298/698 [03:00<03:59,  1.67it/s, loss=1.9904]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 298/698 [03:01<03:59,  1.67it/s, loss=1.6821]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 299/698 [03:01<03:52,  1.72it/s, loss=1.6821]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 299/698 [03:01<03:52,  1.72it/s, loss=1.0747]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 300/698 [03:01<03:48,  1.74it/s, loss=1.0747]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 300/698 [03:02<03:48,  1.74it/s, loss=3.1504]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 301/698 [03:02<03:42,  1.78it/s, loss=3.1504]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 301/698 [03:03<03:42,  1.78it/s, loss=1.9099]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 302/698 [03:03<03:35,  1.84it/s, loss=1.9099]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 302/698 [03:03<03:35,  1.84it/s, loss=1.2748]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 303/698 [03:03<03:31,  1.86it/s, loss=1.2748]\u001b[A\n",
            "Train Epoch 1/3:  43%|████▎     | 303/698 [03:04<03:31,  1.86it/s, loss=1.1676]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▎     | 304/698 [03:04<03:26,  1.90it/s, loss=1.1676]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▎     | 304/698 [03:04<03:26,  1.90it/s, loss=2.2632]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▎     | 305/698 [03:04<03:27,  1.89it/s, loss=2.2632]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▎     | 305/698 [03:05<03:27,  1.89it/s, loss=2.3918]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 306/698 [03:05<03:20,  1.95it/s, loss=2.3918]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 306/698 [03:05<03:20,  1.95it/s, loss=1.8206]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 307/698 [03:05<03:40,  1.78it/s, loss=1.8206]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 307/698 [03:06<03:40,  1.78it/s, loss=3.1645]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 308/698 [03:06<04:03,  1.60it/s, loss=3.1645]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 308/698 [03:07<04:03,  1.60it/s, loss=1.2963]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 309/698 [03:07<04:20,  1.49it/s, loss=1.2963]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 309/698 [03:08<04:20,  1.49it/s, loss=1.7741]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 310/698 [03:08<04:37,  1.40it/s, loss=1.7741]\u001b[A\n",
            "Train Epoch 1/3:  44%|████▍     | 310/698 [03:08<04:37,  1.40it/s, loss=2.7655]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 311/698 [03:08<04:12,  1.53it/s, loss=2.7655]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 311/698 [03:09<04:12,  1.53it/s, loss=1.6244]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 312/698 [03:09<04:10,  1.54it/s, loss=1.6244]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 312/698 [03:09<04:10,  1.54it/s, loss=3.4346]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 313/698 [03:09<04:00,  1.60it/s, loss=3.4346]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 313/698 [03:10<04:00,  1.60it/s, loss=2.0932]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 314/698 [03:10<03:44,  1.71it/s, loss=2.0932]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▍     | 314/698 [03:10<03:44,  1.71it/s, loss=1.2909]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▌     | 315/698 [03:10<03:40,  1.74it/s, loss=1.2909]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▌     | 315/698 [03:11<03:40,  1.74it/s, loss=1.7747]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▌     | 316/698 [03:11<03:28,  1.83it/s, loss=1.7747]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▌     | 316/698 [03:11<03:28,  1.83it/s, loss=1.7119]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▌     | 317/698 [03:11<03:31,  1.80it/s, loss=1.7119]\u001b[A\n",
            "Train Epoch 1/3:  45%|████▌     | 317/698 [03:12<03:31,  1.80it/s, loss=1.0282]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 318/698 [03:12<03:31,  1.79it/s, loss=1.0282]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 318/698 [03:13<03:31,  1.79it/s, loss=1.4381]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 319/698 [03:13<03:43,  1.69it/s, loss=1.4381]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 319/698 [03:13<03:43,  1.69it/s, loss=2.7469]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 320/698 [03:13<03:38,  1.73it/s, loss=2.7469]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 320/698 [03:14<03:38,  1.73it/s, loss=0.9774]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 321/698 [03:14<03:29,  1.80it/s, loss=0.9774]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 321/698 [03:14<03:29,  1.80it/s, loss=0.8296]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 322/698 [03:14<03:25,  1.83it/s, loss=0.8296]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▌     | 322/698 [03:15<03:25,  1.83it/s, loss=2.4529]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▋     | 323/698 [03:15<03:21,  1.86it/s, loss=2.4529]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▋     | 323/698 [03:15<03:21,  1.86it/s, loss=1.8356]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▋     | 324/698 [03:15<03:21,  1.86it/s, loss=1.8356]\u001b[A\n",
            "Train Epoch 1/3:  46%|████▋     | 324/698 [03:16<03:21,  1.86it/s, loss=1.3223]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 325/698 [03:16<03:20,  1.86it/s, loss=1.3223]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 325/698 [03:16<03:20,  1.86it/s, loss=2.2465]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 326/698 [03:16<03:15,  1.90it/s, loss=2.2465]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 326/698 [03:17<03:15,  1.90it/s, loss=1.9399]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 327/698 [03:17<03:09,  1.96it/s, loss=1.9399]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 327/698 [03:17<03:09,  1.96it/s, loss=2.6169]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 328/698 [03:17<03:04,  2.01it/s, loss=2.6169]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 328/698 [03:18<03:04,  2.01it/s, loss=3.8153]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 329/698 [03:18<03:07,  1.97it/s, loss=3.8153]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 329/698 [03:18<03:07,  1.97it/s, loss=2.0609]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 330/698 [03:18<03:28,  1.76it/s, loss=2.0609]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 330/698 [03:19<03:28,  1.76it/s, loss=1.5367]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 331/698 [03:19<03:29,  1.75it/s, loss=1.5367]\u001b[A\n",
            "Train Epoch 1/3:  47%|████▋     | 331/698 [03:20<03:29,  1.75it/s, loss=1.7907]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 332/698 [03:20<03:39,  1.66it/s, loss=1.7907]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 332/698 [03:20<03:39,  1.66it/s, loss=2.9132]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 333/698 [03:20<03:50,  1.59it/s, loss=2.9132]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 333/698 [03:21<03:50,  1.59it/s, loss=1.1569]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 334/698 [03:21<03:49,  1.58it/s, loss=1.1569]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 334/698 [03:22<03:49,  1.58it/s, loss=2.2773]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 335/698 [03:22<03:46,  1.60it/s, loss=2.2773]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 335/698 [03:22<03:46,  1.60it/s, loss=2.2888]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 336/698 [03:22<03:28,  1.74it/s, loss=2.2888]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 336/698 [03:23<03:28,  1.74it/s, loss=1.8977]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 337/698 [03:23<03:21,  1.80it/s, loss=1.8977]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 337/698 [03:23<03:21,  1.80it/s, loss=1.9565]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 338/698 [03:23<03:18,  1.82it/s, loss=1.9565]\u001b[A\n",
            "Train Epoch 1/3:  48%|████▊     | 338/698 [03:24<03:18,  1.82it/s, loss=2.0237]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▊     | 339/698 [03:24<03:17,  1.82it/s, loss=2.0237]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▊     | 339/698 [03:24<03:17,  1.82it/s, loss=2.3039]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▊     | 340/698 [03:24<03:17,  1.82it/s, loss=2.3039]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▊     | 340/698 [03:25<03:17,  1.82it/s, loss=2.1299]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 341/698 [03:25<03:19,  1.79it/s, loss=2.1299]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 341/698 [03:25<03:19,  1.79it/s, loss=1.5212]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 342/698 [03:25<03:03,  1.94it/s, loss=1.5212]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 342/698 [03:26<03:03,  1.94it/s, loss=1.3677]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 343/698 [03:26<03:10,  1.87it/s, loss=1.3677]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 343/698 [03:27<03:10,  1.87it/s, loss=1.1230]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 344/698 [03:27<03:24,  1.73it/s, loss=1.1230]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 344/698 [03:27<03:24,  1.73it/s, loss=1.2453]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 345/698 [03:27<03:14,  1.82it/s, loss=1.2453]\u001b[A\n",
            "Train Epoch 1/3:  49%|████▉     | 345/698 [03:28<03:14,  1.82it/s, loss=1.7913]\u001b[A\n",
            "Train Epoch 1/3:  50%|████▉     | 346/698 [03:28<03:26,  1.70it/s, loss=1.7913]\u001b[A\n",
            "Train Epoch 1/3:  50%|████▉     | 346/698 [03:28<03:26,  1.70it/s, loss=0.8187]\u001b[A\n",
            "Train Epoch 1/3:  50%|████▉     | 347/698 [03:28<03:22,  1.73it/s, loss=0.8187]\u001b[A\n",
            "Train Epoch 1/3:  50%|████▉     | 347/698 [03:29<03:22,  1.73it/s, loss=1.8654]\u001b[A\n",
            "Train Epoch 1/3:  50%|████▉     | 348/698 [03:29<03:16,  1.78it/s, loss=1.8654]\u001b[A\n",
            "Train Epoch 1/3:  50%|████▉     | 348/698 [03:29<03:16,  1.78it/s, loss=1.4791]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 349/698 [03:29<03:07,  1.86it/s, loss=1.4791]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 349/698 [03:30<03:07,  1.86it/s, loss=1.6377]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 350/698 [03:30<03:09,  1.83it/s, loss=1.6377]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 350/698 [03:30<03:09,  1.83it/s, loss=1.4123]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 351/698 [03:30<03:01,  1.91it/s, loss=1.4123]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 351/698 [03:31<03:01,  1.91it/s, loss=1.6402]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 352/698 [03:31<03:05,  1.86it/s, loss=1.6402]\u001b[A\n",
            "Train Epoch 1/3:  50%|█████     | 352/698 [03:31<03:05,  1.86it/s, loss=1.2849]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 353/698 [03:31<03:13,  1.78it/s, loss=1.2849]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 353/698 [03:32<03:13,  1.78it/s, loss=1.1094]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 354/698 [03:32<03:17,  1.74it/s, loss=1.1094]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 354/698 [03:33<03:17,  1.74it/s, loss=1.4703]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 355/698 [03:33<03:19,  1.72it/s, loss=1.4703]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 355/698 [03:33<03:19,  1.72it/s, loss=2.2923]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 356/698 [03:33<03:38,  1.56it/s, loss=2.2923]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 356/698 [03:34<03:38,  1.56it/s, loss=1.8332]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 357/698 [03:34<03:43,  1.52it/s, loss=1.8332]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████     | 357/698 [03:35<03:43,  1.52it/s, loss=1.4477]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████▏    | 358/698 [03:35<03:25,  1.66it/s, loss=1.4477]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████▏    | 358/698 [03:35<03:25,  1.66it/s, loss=2.4045]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████▏    | 359/698 [03:35<03:16,  1.72it/s, loss=2.4045]\u001b[A\n",
            "Train Epoch 1/3:  51%|█████▏    | 359/698 [03:36<03:16,  1.72it/s, loss=1.1691]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 360/698 [03:36<03:09,  1.78it/s, loss=1.1691]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 360/698 [03:36<03:09,  1.78it/s, loss=1.1679]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 361/698 [03:36<03:04,  1.82it/s, loss=1.1679]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 361/698 [03:37<03:04,  1.82it/s, loss=2.2159]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 362/698 [03:37<03:07,  1.79it/s, loss=2.2159]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 362/698 [03:37<03:07,  1.79it/s, loss=0.8838]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 363/698 [03:37<02:59,  1.86it/s, loss=0.8838]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 363/698 [03:38<02:59,  1.86it/s, loss=2.3379]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 364/698 [03:38<02:55,  1.90it/s, loss=2.3379]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 364/698 [03:38<02:55,  1.90it/s, loss=1.4496]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 365/698 [03:38<02:57,  1.88it/s, loss=1.4496]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 365/698 [03:39<02:57,  1.88it/s, loss=1.5002]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 366/698 [03:39<02:56,  1.88it/s, loss=1.5002]\u001b[A\n",
            "Train Epoch 1/3:  52%|█████▏    | 366/698 [03:39<02:56,  1.88it/s, loss=0.8422]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 367/698 [03:39<02:58,  1.86it/s, loss=0.8422]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 367/698 [03:40<02:58,  1.86it/s, loss=1.2555]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 368/698 [03:40<02:59,  1.84it/s, loss=1.2555]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 368/698 [03:40<02:59,  1.84it/s, loss=1.8661]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 369/698 [03:40<02:54,  1.88it/s, loss=1.8661]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 369/698 [03:41<02:54,  1.88it/s, loss=1.4144]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 370/698 [03:41<02:52,  1.90it/s, loss=1.4144]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 370/698 [03:41<02:52,  1.90it/s, loss=1.0855]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 371/698 [03:41<02:50,  1.92it/s, loss=1.0855]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 371/698 [03:42<02:50,  1.92it/s, loss=3.8274]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 372/698 [03:42<02:42,  2.01it/s, loss=3.8274]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 372/698 [03:42<02:42,  2.01it/s, loss=1.2691]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 373/698 [03:42<02:43,  1.99it/s, loss=1.2691]\u001b[A\n",
            "Train Epoch 1/3:  53%|█████▎    | 373/698 [03:43<02:43,  1.99it/s, loss=1.8046]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▎    | 374/698 [03:43<02:54,  1.85it/s, loss=1.8046]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▎    | 374/698 [03:44<02:54,  1.85it/s, loss=0.8620]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▎    | 375/698 [03:44<02:55,  1.84it/s, loss=0.8620]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▎    | 375/698 [03:44<02:55,  1.84it/s, loss=2.1401]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 376/698 [03:44<02:59,  1.80it/s, loss=2.1401]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 376/698 [03:45<02:59,  1.80it/s, loss=1.2720]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 377/698 [03:45<03:08,  1.70it/s, loss=1.2720]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 377/698 [03:45<03:08,  1.70it/s, loss=0.9112]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 378/698 [03:45<03:13,  1.65it/s, loss=0.9112]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 378/698 [03:46<03:13,  1.65it/s, loss=0.9739]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 379/698 [03:46<03:13,  1.65it/s, loss=0.9739]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 379/698 [03:47<03:13,  1.65it/s, loss=2.0982]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 380/698 [03:47<03:19,  1.59it/s, loss=2.0982]\u001b[A\n",
            "Train Epoch 1/3:  54%|█████▍    | 380/698 [03:47<03:19,  1.59it/s, loss=0.9578]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▍    | 381/698 [03:47<03:24,  1.55it/s, loss=0.9578]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▍    | 381/698 [03:48<03:24,  1.55it/s, loss=1.6728]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▍    | 382/698 [03:48<03:06,  1.69it/s, loss=1.6728]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▍    | 382/698 [03:48<03:06,  1.69it/s, loss=1.8274]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▍    | 383/698 [03:49<03:05,  1.70it/s, loss=1.8274]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▍    | 383/698 [03:49<03:05,  1.70it/s, loss=2.0183]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 384/698 [03:49<02:56,  1.78it/s, loss=2.0183]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 384/698 [03:50<02:56,  1.78it/s, loss=1.4070]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 385/698 [03:50<02:52,  1.81it/s, loss=1.4070]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 385/698 [03:50<02:52,  1.81it/s, loss=3.6229]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 386/698 [03:50<02:43,  1.91it/s, loss=3.6229]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 386/698 [03:51<02:43,  1.91it/s, loss=1.5175]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 387/698 [03:51<02:45,  1.88it/s, loss=1.5175]\u001b[A\n",
            "Train Epoch 1/3:  55%|█████▌    | 387/698 [03:51<02:45,  1.88it/s, loss=1.3478]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 388/698 [03:51<02:49,  1.82it/s, loss=1.3478]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 388/698 [03:52<02:49,  1.82it/s, loss=1.3151]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 389/698 [03:52<02:45,  1.86it/s, loss=1.3151]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 389/698 [03:52<02:45,  1.86it/s, loss=1.5735]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 390/698 [03:52<02:38,  1.95it/s, loss=1.5735]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 390/698 [03:53<02:38,  1.95it/s, loss=1.1695]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 391/698 [03:53<02:37,  1.94it/s, loss=1.1695]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 391/698 [03:53<02:37,  1.94it/s, loss=0.9841]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 392/698 [03:53<02:40,  1.91it/s, loss=0.9841]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▌    | 392/698 [03:54<02:40,  1.91it/s, loss=1.2158]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▋    | 393/698 [03:54<02:51,  1.77it/s, loss=1.2158]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▋    | 393/698 [03:54<02:51,  1.77it/s, loss=2.5402]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▋    | 394/698 [03:54<02:51,  1.77it/s, loss=2.5402]\u001b[A\n",
            "Train Epoch 1/3:  56%|█████▋    | 394/698 [03:55<02:51,  1.77it/s, loss=1.8817]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 395/698 [03:55<02:42,  1.87it/s, loss=1.8817]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 395/698 [03:55<02:42,  1.87it/s, loss=1.8015]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 396/698 [03:55<02:42,  1.85it/s, loss=1.8015]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 396/698 [03:56<02:42,  1.85it/s, loss=0.9809]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 397/698 [03:56<02:43,  1.84it/s, loss=0.9809]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 397/698 [03:57<02:43,  1.84it/s, loss=1.3187]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 398/698 [03:57<02:45,  1.81it/s, loss=1.3187]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 398/698 [03:57<02:45,  1.81it/s, loss=1.7040]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 399/698 [03:57<03:02,  1.64it/s, loss=1.7040]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 399/698 [03:58<03:02,  1.64it/s, loss=0.8878]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 400/698 [03:58<03:03,  1.63it/s, loss=0.8878]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 400/698 [03:59<03:03,  1.63it/s, loss=1.1652]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 401/698 [03:59<03:03,  1.62it/s, loss=1.1652]\u001b[A\n",
            "Train Epoch 1/3:  57%|█████▋    | 401/698 [03:59<03:03,  1.62it/s, loss=0.7062]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 402/698 [03:59<03:06,  1.59it/s, loss=0.7062]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 402/698 [04:00<03:06,  1.59it/s, loss=1.0678]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 403/698 [04:00<03:15,  1.51it/s, loss=1.0678]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 403/698 [04:00<03:15,  1.51it/s, loss=0.9788]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 404/698 [04:00<03:05,  1.58it/s, loss=0.9788]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 404/698 [04:01<03:05,  1.58it/s, loss=1.3596]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 405/698 [04:01<02:59,  1.63it/s, loss=1.3596]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 405/698 [04:02<02:59,  1.63it/s, loss=1.9445]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 406/698 [04:02<03:00,  1.61it/s, loss=1.9445]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 406/698 [04:02<03:00,  1.61it/s, loss=0.6532]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 407/698 [04:02<02:57,  1.64it/s, loss=0.6532]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 407/698 [04:03<02:57,  1.64it/s, loss=0.9913]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 408/698 [04:03<02:47,  1.73it/s, loss=0.9913]\u001b[A\n",
            "Train Epoch 1/3:  58%|█████▊    | 408/698 [04:03<02:47,  1.73it/s, loss=2.0008]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▊    | 409/698 [04:03<02:49,  1.70it/s, loss=2.0008]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▊    | 409/698 [04:04<02:49,  1.70it/s, loss=1.3215]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▊    | 410/698 [04:04<02:59,  1.60it/s, loss=1.3215]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▊    | 410/698 [04:05<02:59,  1.60it/s, loss=0.9794]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 411/698 [04:05<02:54,  1.65it/s, loss=0.9794]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 411/698 [04:05<02:54,  1.65it/s, loss=1.9543]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 412/698 [04:05<02:55,  1.63it/s, loss=1.9543]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 412/698 [04:06<02:55,  1.63it/s, loss=1.2482]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 413/698 [04:06<02:41,  1.76it/s, loss=1.2482]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 413/698 [04:06<02:41,  1.76it/s, loss=0.7784]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 414/698 [04:06<02:37,  1.80it/s, loss=0.7784]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 414/698 [04:07<02:37,  1.80it/s, loss=0.7318]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 415/698 [04:07<02:35,  1.81it/s, loss=0.7318]\u001b[A\n",
            "Train Epoch 1/3:  59%|█████▉    | 415/698 [04:07<02:35,  1.81it/s, loss=1.0241]\u001b[A\n",
            "Train Epoch 1/3:  60%|█████▉    | 416/698 [04:07<02:45,  1.70it/s, loss=1.0241]\u001b[A\n",
            "Train Epoch 1/3:  60%|█████▉    | 416/698 [04:08<02:45,  1.70it/s, loss=2.4323]\u001b[A\n",
            "Train Epoch 1/3:  60%|█████▉    | 417/698 [04:08<02:42,  1.73it/s, loss=2.4323]\u001b[A\n",
            "Train Epoch 1/3:  60%|█████▉    | 417/698 [04:09<02:42,  1.73it/s, loss=1.5217]\u001b[A\n",
            "Train Epoch 1/3:  60%|█████▉    | 418/698 [04:09<02:37,  1.78it/s, loss=1.5217]\u001b[A\n",
            "Train Epoch 1/3:  60%|█████▉    | 418/698 [04:09<02:37,  1.78it/s, loss=2.7000]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 419/698 [04:09<02:32,  1.83it/s, loss=2.7000]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 419/698 [04:10<02:32,  1.83it/s, loss=1.2216]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 420/698 [04:10<02:38,  1.76it/s, loss=1.2216]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 420/698 [04:10<02:38,  1.76it/s, loss=1.7098]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 421/698 [04:10<02:36,  1.77it/s, loss=1.7098]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 421/698 [04:11<02:36,  1.77it/s, loss=0.9571]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 422/698 [04:11<02:59,  1.54it/s, loss=0.9571]\u001b[A\n",
            "Train Epoch 1/3:  60%|██████    | 422/698 [04:12<02:59,  1.54it/s, loss=0.9688]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 423/698 [04:12<03:00,  1.53it/s, loss=0.9688]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 423/698 [04:12<03:00,  1.53it/s, loss=1.1265]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 424/698 [04:12<02:59,  1.53it/s, loss=1.1265]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 424/698 [04:13<02:59,  1.53it/s, loss=0.7677]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 425/698 [04:13<03:02,  1.49it/s, loss=0.7677]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 425/698 [04:14<03:02,  1.49it/s, loss=1.5066]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 426/698 [04:14<02:50,  1.59it/s, loss=1.5066]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 426/698 [04:14<02:50,  1.59it/s, loss=1.5238]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 427/698 [04:14<02:36,  1.73it/s, loss=1.5238]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████    | 427/698 [04:15<02:36,  1.73it/s, loss=1.1565]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████▏   | 428/698 [04:15<02:30,  1.79it/s, loss=1.1565]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████▏   | 428/698 [04:15<02:30,  1.79it/s, loss=1.5117]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████▏   | 429/698 [04:15<02:29,  1.80it/s, loss=1.5117]\u001b[A\n",
            "Train Epoch 1/3:  61%|██████▏   | 429/698 [04:16<02:29,  1.80it/s, loss=2.0462]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 430/698 [04:16<02:22,  1.88it/s, loss=2.0462]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 430/698 [04:16<02:22,  1.88it/s, loss=1.0285]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 431/698 [04:16<02:18,  1.93it/s, loss=1.0285]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 431/698 [04:17<02:18,  1.93it/s, loss=1.7476]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 432/698 [04:17<02:19,  1.90it/s, loss=1.7476]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 432/698 [04:17<02:19,  1.90it/s, loss=2.9813]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 433/698 [04:17<02:13,  1.99it/s, loss=2.9813]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 433/698 [04:18<02:13,  1.99it/s, loss=1.9122]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 434/698 [04:18<02:17,  1.92it/s, loss=1.9122]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 434/698 [04:18<02:17,  1.92it/s, loss=0.8688]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 435/698 [04:18<02:18,  1.90it/s, loss=0.8688]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 435/698 [04:19<02:18,  1.90it/s, loss=1.0636]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 436/698 [04:19<02:25,  1.80it/s, loss=1.0636]\u001b[A\n",
            "Train Epoch 1/3:  62%|██████▏   | 436/698 [04:19<02:25,  1.80it/s, loss=1.2264]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 437/698 [04:19<02:26,  1.78it/s, loss=1.2264]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 437/698 [04:20<02:26,  1.78it/s, loss=1.2622]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 438/698 [04:20<02:37,  1.65it/s, loss=1.2622]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 438/698 [04:21<02:37,  1.65it/s, loss=3.3856]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 439/698 [04:21<02:34,  1.68it/s, loss=3.3856]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 439/698 [04:21<02:34,  1.68it/s, loss=1.9455]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 440/698 [04:21<02:30,  1.71it/s, loss=1.9455]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 440/698 [04:22<02:30,  1.71it/s, loss=0.6293]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 441/698 [04:22<02:31,  1.70it/s, loss=0.6293]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 441/698 [04:22<02:31,  1.70it/s, loss=1.1753]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 442/698 [04:22<02:29,  1.71it/s, loss=1.1753]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 442/698 [04:23<02:29,  1.71it/s, loss=2.1213]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 443/698 [04:23<02:31,  1.68it/s, loss=2.1213]\u001b[A\n",
            "Train Epoch 1/3:  63%|██████▎   | 443/698 [04:24<02:31,  1.68it/s, loss=3.9214]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▎   | 444/698 [04:24<02:45,  1.53it/s, loss=3.9214]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▎   | 444/698 [04:24<02:45,  1.53it/s, loss=1.4609]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 445/698 [04:24<02:44,  1.54it/s, loss=1.4609]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 445/698 [04:25<02:44,  1.54it/s, loss=1.8233]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 446/698 [04:25<02:47,  1.50it/s, loss=1.8233]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 446/698 [04:26<02:47,  1.50it/s, loss=1.4974]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 447/698 [04:26<02:54,  1.44it/s, loss=1.4974]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 447/698 [04:27<02:54,  1.44it/s, loss=1.4832]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 448/698 [04:27<02:42,  1.54it/s, loss=1.4832]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 448/698 [04:27<02:42,  1.54it/s, loss=0.7738]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 449/698 [04:27<02:37,  1.58it/s, loss=0.7738]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 449/698 [04:28<02:37,  1.58it/s, loss=1.5215]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 450/698 [04:28<02:30,  1.65it/s, loss=1.5215]\u001b[A\n",
            "Train Epoch 1/3:  64%|██████▍   | 450/698 [04:28<02:30,  1.65it/s, loss=1.2428]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▍   | 451/698 [04:28<02:22,  1.73it/s, loss=1.2428]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▍   | 451/698 [04:29<02:22,  1.73it/s, loss=2.0923]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▍   | 452/698 [04:29<02:13,  1.84it/s, loss=2.0923]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▍   | 452/698 [04:29<02:13,  1.84it/s, loss=1.9871]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▍   | 453/698 [04:29<02:20,  1.74it/s, loss=1.9871]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▍   | 453/698 [04:30<02:20,  1.74it/s, loss=0.6410]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 454/698 [04:30<02:18,  1.76it/s, loss=0.6410]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 454/698 [04:30<02:18,  1.76it/s, loss=1.1437]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 455/698 [04:30<02:18,  1.75it/s, loss=1.1437]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 455/698 [04:31<02:18,  1.75it/s, loss=0.8675]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 456/698 [04:31<02:21,  1.71it/s, loss=0.8675]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 456/698 [04:32<02:21,  1.71it/s, loss=0.9701]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 457/698 [04:32<02:18,  1.74it/s, loss=0.9701]\u001b[A\n",
            "Train Epoch 1/3:  65%|██████▌   | 457/698 [04:32<02:18,  1.74it/s, loss=1.6623]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 458/698 [04:32<02:18,  1.73it/s, loss=1.6623]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 458/698 [04:33<02:18,  1.73it/s, loss=0.8521]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 459/698 [04:33<02:11,  1.81it/s, loss=0.8521]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 459/698 [04:33<02:11,  1.81it/s, loss=1.3670]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 460/698 [04:33<02:12,  1.79it/s, loss=1.3670]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 460/698 [04:34<02:12,  1.79it/s, loss=1.3470]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 461/698 [04:34<02:12,  1.79it/s, loss=1.3470]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 461/698 [04:34<02:12,  1.79it/s, loss=1.1139]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 462/698 [04:34<02:07,  1.85it/s, loss=1.1139]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▌   | 462/698 [04:35<02:07,  1.85it/s, loss=0.6612]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▋   | 463/698 [04:35<02:06,  1.86it/s, loss=0.6612]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▋   | 463/698 [04:35<02:06,  1.86it/s, loss=1.0857]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▋   | 464/698 [04:35<02:01,  1.92it/s, loss=1.0857]\u001b[A\n",
            "Train Epoch 1/3:  66%|██████▋   | 464/698 [04:36<02:01,  1.92it/s, loss=1.5266]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 465/698 [04:36<01:58,  1.97it/s, loss=1.5266]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 465/698 [04:36<01:58,  1.97it/s, loss=1.1989]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 466/698 [04:36<01:53,  2.04it/s, loss=1.1989]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 466/698 [04:37<01:53,  2.04it/s, loss=0.8875]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 467/698 [04:37<02:05,  1.84it/s, loss=0.8875]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 467/698 [04:38<02:05,  1.84it/s, loss=0.7428]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 468/698 [04:38<02:27,  1.56it/s, loss=0.7428]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 468/698 [04:38<02:27,  1.56it/s, loss=1.4098]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 469/698 [04:38<02:28,  1.54it/s, loss=1.4098]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 469/698 [04:39<02:28,  1.54it/s, loss=0.9261]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 470/698 [04:39<02:31,  1.50it/s, loss=0.9261]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 470/698 [04:40<02:31,  1.50it/s, loss=1.1562]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 471/698 [04:40<02:25,  1.56it/s, loss=1.1562]\u001b[A\n",
            "Train Epoch 1/3:  67%|██████▋   | 471/698 [04:40<02:25,  1.56it/s, loss=1.0368]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 472/698 [04:40<02:15,  1.66it/s, loss=1.0368]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 472/698 [04:41<02:15,  1.66it/s, loss=2.2685]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 473/698 [04:41<02:12,  1.69it/s, loss=2.2685]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 473/698 [04:41<02:12,  1.69it/s, loss=1.1305]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 474/698 [04:41<02:08,  1.75it/s, loss=1.1305]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 474/698 [04:42<02:08,  1.75it/s, loss=2.7007]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 475/698 [04:42<02:09,  1.72it/s, loss=2.7007]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 475/698 [04:42<02:09,  1.72it/s, loss=1.0234]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 476/698 [04:42<02:02,  1.82it/s, loss=1.0234]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 476/698 [04:43<02:02,  1.82it/s, loss=1.5135]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 477/698 [04:43<02:00,  1.84it/s, loss=1.5135]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 477/698 [04:43<02:00,  1.84it/s, loss=0.6392]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 478/698 [04:43<01:56,  1.89it/s, loss=0.6392]\u001b[A\n",
            "Train Epoch 1/3:  68%|██████▊   | 478/698 [04:44<01:56,  1.89it/s, loss=1.3676]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▊   | 479/698 [04:44<01:57,  1.86it/s, loss=1.3676]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▊   | 479/698 [04:45<01:57,  1.86it/s, loss=0.8754]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 480/698 [04:45<02:00,  1.81it/s, loss=0.8754]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 480/698 [04:45<02:00,  1.81it/s, loss=1.4151]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 481/698 [04:45<01:55,  1.89it/s, loss=1.4151]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 481/698 [04:46<01:55,  1.89it/s, loss=1.1228]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 482/698 [04:46<01:54,  1.88it/s, loss=1.1228]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 482/698 [04:46<01:54,  1.88it/s, loss=0.9329]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 483/698 [04:46<01:54,  1.88it/s, loss=0.9329]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 483/698 [04:47<01:54,  1.88it/s, loss=2.4371]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 484/698 [04:47<01:52,  1.91it/s, loss=2.4371]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 484/698 [04:47<01:52,  1.91it/s, loss=0.5484]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 485/698 [04:47<01:52,  1.89it/s, loss=0.5484]\u001b[A\n",
            "Train Epoch 1/3:  69%|██████▉   | 485/698 [04:48<01:52,  1.89it/s, loss=1.1089]\u001b[A\n",
            "Train Epoch 1/3:  70%|██████▉   | 486/698 [04:48<01:54,  1.85it/s, loss=1.1089]\u001b[A\n",
            "Train Epoch 1/3:  70%|██████▉   | 486/698 [04:48<01:54,  1.85it/s, loss=2.0827]\u001b[A\n",
            "Train Epoch 1/3:  70%|██████▉   | 487/698 [04:48<01:53,  1.85it/s, loss=2.0827]\u001b[A\n",
            "Train Epoch 1/3:  70%|██████▉   | 487/698 [04:49<01:53,  1.85it/s, loss=1.3722]\u001b[A\n",
            "Train Epoch 1/3:  70%|██████▉   | 488/698 [04:49<01:51,  1.88it/s, loss=1.3722]\u001b[A\n",
            "Train Epoch 1/3:  70%|██████▉   | 488/698 [04:49<01:51,  1.88it/s, loss=0.8432]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 489/698 [04:49<01:50,  1.89it/s, loss=0.8432]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 489/698 [04:50<01:50,  1.89it/s, loss=1.0950]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 490/698 [04:50<01:56,  1.79it/s, loss=1.0950]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 490/698 [04:51<01:56,  1.79it/s, loss=1.8909]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 491/698 [04:51<01:57,  1.75it/s, loss=1.8909]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 491/698 [04:51<01:57,  1.75it/s, loss=1.0460]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 492/698 [04:51<02:04,  1.66it/s, loss=1.0460]\u001b[A\n",
            "Train Epoch 1/3:  70%|███████   | 492/698 [04:52<02:04,  1.66it/s, loss=1.0967]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 493/698 [04:52<02:12,  1.55it/s, loss=1.0967]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 493/698 [04:53<02:12,  1.55it/s, loss=0.9332]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 494/698 [04:53<02:14,  1.52it/s, loss=0.9332]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 494/698 [04:53<02:14,  1.52it/s, loss=1.1455]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 495/698 [04:53<02:09,  1.56it/s, loss=1.1455]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 495/698 [04:54<02:09,  1.56it/s, loss=0.7334]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 496/698 [04:54<02:03,  1.64it/s, loss=0.7334]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 496/698 [04:54<02:03,  1.64it/s, loss=1.3107]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 497/698 [04:54<01:59,  1.68it/s, loss=1.3107]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████   | 497/698 [04:55<01:59,  1.68it/s, loss=1.1846]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████▏  | 498/698 [04:55<01:52,  1.77it/s, loss=1.1846]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████▏  | 498/698 [04:55<01:52,  1.77it/s, loss=1.0093]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████▏  | 499/698 [04:55<01:47,  1.86it/s, loss=1.0093]\u001b[A\n",
            "Train Epoch 1/3:  71%|███████▏  | 499/698 [04:56<01:47,  1.86it/s, loss=1.2090]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 500/698 [04:56<01:46,  1.86it/s, loss=1.2090]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 500/698 [04:56<01:46,  1.86it/s, loss=1.4542]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 501/698 [04:56<01:47,  1.83it/s, loss=1.4542]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 501/698 [04:57<01:47,  1.83it/s, loss=1.0573]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 502/698 [04:57<01:46,  1.84it/s, loss=1.0573]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 502/698 [04:57<01:46,  1.84it/s, loss=0.9744]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 503/698 [04:57<01:47,  1.82it/s, loss=0.9744]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 503/698 [04:58<01:47,  1.82it/s, loss=1.4306]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 504/698 [04:58<01:45,  1.85it/s, loss=1.4306]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 504/698 [04:59<01:45,  1.85it/s, loss=0.9875]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 505/698 [04:59<01:44,  1.84it/s, loss=0.9875]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 505/698 [04:59<01:44,  1.84it/s, loss=1.6469]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 506/698 [04:59<01:41,  1.89it/s, loss=1.6469]\u001b[A\n",
            "Train Epoch 1/3:  72%|███████▏  | 506/698 [05:00<01:41,  1.89it/s, loss=0.9020]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 507/698 [05:00<01:45,  1.82it/s, loss=0.9020]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 507/698 [05:00<01:45,  1.82it/s, loss=1.2366]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 508/698 [05:00<01:46,  1.78it/s, loss=1.2366]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 508/698 [05:01<01:46,  1.78it/s, loss=1.8559]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 509/698 [05:01<01:44,  1.81it/s, loss=1.8559]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 509/698 [05:01<01:44,  1.81it/s, loss=0.9413]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 510/698 [05:01<01:42,  1.84it/s, loss=0.9413]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 510/698 [05:02<01:42,  1.84it/s, loss=0.7729]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 511/698 [05:02<01:40,  1.86it/s, loss=0.7729]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 511/698 [05:02<01:40,  1.86it/s, loss=0.6691]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 512/698 [05:02<01:38,  1.89it/s, loss=0.6691]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 512/698 [05:03<01:38,  1.89it/s, loss=1.0921]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 513/698 [05:03<01:42,  1.80it/s, loss=1.0921]\u001b[A\n",
            "Train Epoch 1/3:  73%|███████▎  | 513/698 [05:04<01:42,  1.80it/s, loss=1.0838]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▎  | 514/698 [05:04<01:43,  1.78it/s, loss=1.0838]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▎  | 514/698 [05:04<01:43,  1.78it/s, loss=1.2539]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 515/698 [05:04<01:47,  1.70it/s, loss=1.2539]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 515/698 [05:05<01:47,  1.70it/s, loss=2.1440]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 516/698 [05:05<01:51,  1.63it/s, loss=2.1440]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 516/698 [05:06<01:51,  1.63it/s, loss=0.5777]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 517/698 [05:06<01:55,  1.57it/s, loss=0.5777]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 517/698 [05:06<01:55,  1.57it/s, loss=0.8431]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 518/698 [05:06<01:50,  1.63it/s, loss=0.8431]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 518/698 [05:07<01:50,  1.63it/s, loss=0.7860]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 519/698 [05:07<01:48,  1.65it/s, loss=0.7860]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 519/698 [05:07<01:48,  1.65it/s, loss=1.5578]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 520/698 [05:07<01:44,  1.70it/s, loss=1.5578]\u001b[A\n",
            "Train Epoch 1/3:  74%|███████▍  | 520/698 [05:08<01:44,  1.70it/s, loss=1.0610]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▍  | 521/698 [05:08<01:38,  1.80it/s, loss=1.0610]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▍  | 521/698 [05:08<01:38,  1.80it/s, loss=1.6288]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▍  | 522/698 [05:08<01:34,  1.85it/s, loss=1.6288]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▍  | 522/698 [05:09<01:34,  1.85it/s, loss=1.1690]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▍  | 523/698 [05:09<01:30,  1.94it/s, loss=1.1690]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▍  | 523/698 [05:09<01:30,  1.94it/s, loss=0.9039]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▌  | 524/698 [05:09<01:37,  1.79it/s, loss=0.9039]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▌  | 524/698 [05:10<01:37,  1.79it/s, loss=1.5697]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▌  | 525/698 [05:10<01:32,  1.87it/s, loss=1.5697]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▌  | 525/698 [05:10<01:32,  1.87it/s, loss=1.5396]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▌  | 526/698 [05:10<01:30,  1.90it/s, loss=1.5396]\u001b[A\n",
            "Train Epoch 1/3:  75%|███████▌  | 526/698 [05:11<01:30,  1.90it/s, loss=0.8470]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 527/698 [05:11<01:32,  1.85it/s, loss=0.8470]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 527/698 [05:11<01:32,  1.85it/s, loss=4.0954]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 528/698 [05:11<01:32,  1.84it/s, loss=4.0954]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 528/698 [05:12<01:32,  1.84it/s, loss=2.5906]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 529/698 [05:12<01:34,  1.78it/s, loss=2.5906]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 529/698 [05:13<01:34,  1.78it/s, loss=1.0276]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 530/698 [05:13<01:30,  1.86it/s, loss=1.0276]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 530/698 [05:13<01:30,  1.86it/s, loss=1.0362]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 531/698 [05:13<01:26,  1.94it/s, loss=1.0362]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 531/698 [05:14<01:26,  1.94it/s, loss=1.5647]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 532/698 [05:14<01:32,  1.80it/s, loss=1.5647]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▌  | 532/698 [05:14<01:32,  1.80it/s, loss=0.5737]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▋  | 533/698 [05:14<01:35,  1.73it/s, loss=0.5737]\u001b[A\n",
            "Train Epoch 1/3:  76%|███████▋  | 533/698 [05:15<01:35,  1.73it/s, loss=1.1172]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 534/698 [05:15<01:34,  1.74it/s, loss=1.1172]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 534/698 [05:15<01:34,  1.74it/s, loss=0.8111]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 535/698 [05:15<01:32,  1.77it/s, loss=0.8111]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 535/698 [05:16<01:32,  1.77it/s, loss=0.6342]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 536/698 [05:16<01:37,  1.66it/s, loss=0.6342]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 536/698 [05:17<01:37,  1.66it/s, loss=1.7447]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 537/698 [05:17<01:38,  1.64it/s, loss=1.7447]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 537/698 [05:17<01:38,  1.64it/s, loss=0.7032]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 538/698 [05:17<01:36,  1.65it/s, loss=0.7032]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 538/698 [05:18<01:36,  1.65it/s, loss=0.7122]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 539/698 [05:18<01:40,  1.58it/s, loss=0.7122]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 539/698 [05:19<01:40,  1.58it/s, loss=0.9310]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 540/698 [05:19<01:41,  1.56it/s, loss=0.9310]\u001b[A\n",
            "Train Epoch 1/3:  77%|███████▋  | 540/698 [05:19<01:41,  1.56it/s, loss=1.2908]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 541/698 [05:19<01:34,  1.66it/s, loss=1.2908]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 541/698 [05:20<01:34,  1.66it/s, loss=1.3352]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 542/698 [05:20<01:32,  1.68it/s, loss=1.3352]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 542/698 [05:20<01:32,  1.68it/s, loss=1.3878]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 543/698 [05:20<01:31,  1.69it/s, loss=1.3878]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 543/698 [05:21<01:31,  1.69it/s, loss=2.4174]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 544/698 [05:21<01:33,  1.66it/s, loss=2.4174]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 544/698 [05:22<01:33,  1.66it/s, loss=0.7851]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 545/698 [05:22<01:33,  1.63it/s, loss=0.7851]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 545/698 [05:22<01:33,  1.63it/s, loss=1.6909]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 546/698 [05:22<01:28,  1.71it/s, loss=1.6909]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 546/698 [05:23<01:28,  1.71it/s, loss=1.1120]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 547/698 [05:23<01:23,  1.80it/s, loss=1.1120]\u001b[A\n",
            "Train Epoch 1/3:  78%|███████▊  | 547/698 [05:23<01:23,  1.80it/s, loss=0.8642]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▊  | 548/698 [05:23<01:23,  1.80it/s, loss=0.8642]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▊  | 548/698 [05:24<01:23,  1.80it/s, loss=1.5512]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▊  | 549/698 [05:24<01:20,  1.85it/s, loss=1.5512]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▊  | 549/698 [05:24<01:20,  1.85it/s, loss=1.9923]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 550/698 [05:24<01:19,  1.86it/s, loss=1.9923]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 550/698 [05:25<01:19,  1.86it/s, loss=1.0885]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 551/698 [05:25<01:19,  1.86it/s, loss=1.0885]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 551/698 [05:25<01:19,  1.86it/s, loss=0.8135]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 552/698 [05:25<01:23,  1.74it/s, loss=0.8135]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 552/698 [05:26<01:23,  1.74it/s, loss=0.6141]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 553/698 [05:26<01:26,  1.68it/s, loss=0.6141]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 553/698 [05:27<01:26,  1.68it/s, loss=0.9268]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 554/698 [05:27<01:22,  1.74it/s, loss=0.9268]\u001b[A\n",
            "Train Epoch 1/3:  79%|███████▉  | 554/698 [05:27<01:22,  1.74it/s, loss=1.5478]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 555/698 [05:27<01:18,  1.83it/s, loss=1.5478]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 555/698 [05:27<01:18,  1.83it/s, loss=0.9563]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 556/698 [05:27<01:13,  1.94it/s, loss=0.9563]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 556/698 [05:28<01:13,  1.94it/s, loss=0.9081]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 557/698 [05:28<01:13,  1.92it/s, loss=0.9081]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 557/698 [05:29<01:13,  1.92it/s, loss=0.9626]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 558/698 [05:29<01:14,  1.89it/s, loss=0.9626]\u001b[A\n",
            "Train Epoch 1/3:  80%|███████▉  | 558/698 [05:29<01:14,  1.89it/s, loss=1.1657]\u001b[A\n",
            "Train Epoch 1/3:  80%|████████  | 559/698 [05:29<01:24,  1.65it/s, loss=1.1657]\u001b[A\n",
            "Train Epoch 1/3:  80%|████████  | 559/698 [05:30<01:24,  1.65it/s, loss=1.1708]\u001b[A\n",
            "Train Epoch 1/3:  80%|████████  | 560/698 [05:30<01:24,  1.64it/s, loss=1.1708]\u001b[A\n",
            "Train Epoch 1/3:  80%|████████  | 560/698 [05:31<01:24,  1.64it/s, loss=1.2002]\u001b[A\n",
            "Train Epoch 1/3:  80%|████████  | 561/698 [05:31<01:24,  1.62it/s, loss=1.2002]\u001b[A\n",
            "Train Epoch 1/3:  80%|████████  | 561/698 [05:31<01:24,  1.62it/s, loss=0.8062]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 562/698 [05:31<01:24,  1.60it/s, loss=0.8062]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 562/698 [05:32<01:24,  1.60it/s, loss=2.1799]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 563/698 [05:32<01:25,  1.58it/s, loss=2.1799]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 563/698 [05:32<01:25,  1.58it/s, loss=2.1887]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 564/698 [05:32<01:19,  1.69it/s, loss=2.1887]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 564/698 [05:33<01:19,  1.69it/s, loss=1.8927]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 565/698 [05:33<01:16,  1.74it/s, loss=1.8927]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 565/698 [05:34<01:16,  1.74it/s, loss=1.1383]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 566/698 [05:34<01:17,  1.71it/s, loss=1.1383]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 566/698 [05:34<01:17,  1.71it/s, loss=1.6201]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 567/698 [05:34<01:19,  1.65it/s, loss=1.6201]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████  | 567/698 [05:35<01:19,  1.65it/s, loss=0.8736]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████▏ | 568/698 [05:35<01:16,  1.70it/s, loss=0.8736]\u001b[A\n",
            "Train Epoch 1/3:  81%|████████▏ | 568/698 [05:35<01:16,  1.70it/s, loss=0.9947]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 569/698 [05:35<01:11,  1.79it/s, loss=0.9947]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 569/698 [05:36<01:11,  1.79it/s, loss=0.9320]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 570/698 [05:36<01:10,  1.82it/s, loss=0.9320]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 570/698 [05:36<01:10,  1.82it/s, loss=0.8424]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 571/698 [05:36<01:08,  1.86it/s, loss=0.8424]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 571/698 [05:37<01:08,  1.86it/s, loss=1.0264]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 572/698 [05:37<01:08,  1.85it/s, loss=1.0264]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 572/698 [05:37<01:08,  1.85it/s, loss=1.0340]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 573/698 [05:37<01:07,  1.85it/s, loss=1.0340]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 573/698 [05:38<01:07,  1.85it/s, loss=1.6315]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 574/698 [05:38<01:05,  1.88it/s, loss=1.6315]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 574/698 [05:38<01:05,  1.88it/s, loss=0.8179]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 575/698 [05:38<01:06,  1.86it/s, loss=0.8179]\u001b[A\n",
            "Train Epoch 1/3:  82%|████████▏ | 575/698 [05:39<01:06,  1.86it/s, loss=1.1789]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 576/698 [05:39<01:04,  1.88it/s, loss=1.1789]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 576/698 [05:39<01:04,  1.88it/s, loss=0.9032]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 577/698 [05:39<01:04,  1.88it/s, loss=0.9032]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 577/698 [05:40<01:04,  1.88it/s, loss=0.7339]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 578/698 [05:40<01:07,  1.79it/s, loss=0.7339]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 578/698 [05:41<01:07,  1.79it/s, loss=1.8671]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 579/698 [05:41<01:03,  1.89it/s, loss=1.8671]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 579/698 [05:41<01:03,  1.89it/s, loss=1.0698]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 580/698 [05:41<01:02,  1.88it/s, loss=1.0698]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 580/698 [05:42<01:02,  1.88it/s, loss=0.7496]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 581/698 [05:42<01:03,  1.83it/s, loss=0.7496]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 581/698 [05:42<01:03,  1.83it/s, loss=1.0478]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 582/698 [05:42<01:05,  1.77it/s, loss=1.0478]\u001b[A\n",
            "Train Epoch 1/3:  83%|████████▎ | 582/698 [05:43<01:05,  1.77it/s, loss=1.0239]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▎ | 583/698 [05:43<01:15,  1.52it/s, loss=1.0239]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▎ | 583/698 [05:44<01:15,  1.52it/s, loss=1.1345]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▎ | 584/698 [05:44<01:15,  1.51it/s, loss=1.1345]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▎ | 584/698 [05:45<01:15,  1.51it/s, loss=0.8725]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 585/698 [05:45<01:19,  1.42it/s, loss=0.8725]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 585/698 [05:45<01:19,  1.42it/s, loss=1.1677]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 586/698 [05:45<01:16,  1.46it/s, loss=1.1677]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 586/698 [05:46<01:16,  1.46it/s, loss=1.0609]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 587/698 [05:46<01:11,  1.55it/s, loss=1.0609]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 587/698 [05:46<01:11,  1.55it/s, loss=0.3899]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 588/698 [05:46<01:07,  1.62it/s, loss=0.3899]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 588/698 [05:47<01:07,  1.62it/s, loss=1.4743]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 589/698 [05:47<01:04,  1.68it/s, loss=1.4743]\u001b[A\n",
            "Train Epoch 1/3:  84%|████████▍ | 589/698 [05:47<01:04,  1.68it/s, loss=0.4492]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 590/698 [05:47<01:01,  1.74it/s, loss=0.4492]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 590/698 [05:48<01:01,  1.74it/s, loss=1.6365]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 591/698 [05:48<00:58,  1.82it/s, loss=1.6365]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 591/698 [05:49<00:58,  1.82it/s, loss=1.5383]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 592/698 [05:49<00:59,  1.79it/s, loss=1.5383]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 592/698 [05:49<00:59,  1.79it/s, loss=0.4758]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 593/698 [05:49<00:59,  1.78it/s, loss=0.4758]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▍ | 593/698 [05:50<00:59,  1.78it/s, loss=0.9751]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▌ | 594/698 [05:50<01:04,  1.61it/s, loss=0.9751]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▌ | 594/698 [05:50<01:04,  1.61it/s, loss=0.8878]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▌ | 595/698 [05:50<01:04,  1.59it/s, loss=0.8878]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▌ | 595/698 [05:51<01:04,  1.59it/s, loss=1.3726]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▌ | 596/698 [05:51<00:59,  1.72it/s, loss=1.3726]\u001b[A\n",
            "Train Epoch 1/3:  85%|████████▌ | 596/698 [05:51<00:59,  1.72it/s, loss=1.2536]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 597/698 [05:51<00:56,  1.78it/s, loss=1.2536]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 597/698 [05:52<00:56,  1.78it/s, loss=0.8407]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 598/698 [05:52<00:56,  1.78it/s, loss=0.8407]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 598/698 [05:53<00:56,  1.78it/s, loss=1.5895]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 599/698 [05:53<00:54,  1.82it/s, loss=1.5895]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 599/698 [05:53<00:54,  1.82it/s, loss=0.9282]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 600/698 [05:53<00:59,  1.63it/s, loss=0.9282]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 600/698 [05:54<00:59,  1.63it/s, loss=0.9262]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 601/698 [05:54<00:59,  1.64it/s, loss=0.9262]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 601/698 [05:54<00:59,  1.64it/s, loss=1.5100]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 602/698 [05:54<00:56,  1.69it/s, loss=1.5100]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▌ | 602/698 [05:55<00:56,  1.69it/s, loss=2.4510]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▋ | 603/698 [05:55<00:55,  1.71it/s, loss=2.4510]\u001b[A\n",
            "Train Epoch 1/3:  86%|████████▋ | 603/698 [05:56<00:55,  1.71it/s, loss=0.8253]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 604/698 [05:56<00:58,  1.61it/s, loss=0.8253]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 604/698 [05:56<00:58,  1.61it/s, loss=1.2834]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 605/698 [05:56<00:57,  1.61it/s, loss=1.2834]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 605/698 [05:57<00:57,  1.61it/s, loss=0.4737]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 606/698 [05:57<00:59,  1.55it/s, loss=0.4737]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 606/698 [05:58<00:59,  1.55it/s, loss=1.1348]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 607/698 [05:58<01:03,  1.44it/s, loss=1.1348]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 607/698 [05:58<01:03,  1.44it/s, loss=1.0081]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 608/698 [05:58<00:58,  1.53it/s, loss=1.0081]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 608/698 [05:59<00:58,  1.53it/s, loss=0.5691]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 609/698 [05:59<00:57,  1.56it/s, loss=0.5691]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 609/698 [06:00<00:57,  1.56it/s, loss=1.2556]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 610/698 [06:00<00:56,  1.56it/s, loss=1.2556]\u001b[A\n",
            "Train Epoch 1/3:  87%|████████▋ | 610/698 [06:00<00:56,  1.56it/s, loss=1.3776]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 611/698 [06:00<00:52,  1.65it/s, loss=1.3776]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 611/698 [06:01<00:52,  1.65it/s, loss=0.6327]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 612/698 [06:01<00:48,  1.78it/s, loss=0.6327]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 612/698 [06:01<00:48,  1.78it/s, loss=0.7027]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 613/698 [06:01<00:49,  1.71it/s, loss=0.7027]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 613/698 [06:02<00:49,  1.71it/s, loss=0.6083]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 614/698 [06:02<00:46,  1.80it/s, loss=0.6083]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 614/698 [06:02<00:46,  1.80it/s, loss=0.9467]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 615/698 [06:02<00:45,  1.83it/s, loss=0.9467]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 615/698 [06:03<00:45,  1.83it/s, loss=0.5357]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 616/698 [06:03<00:43,  1.88it/s, loss=0.5357]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 616/698 [06:03<00:43,  1.88it/s, loss=1.0297]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 617/698 [06:03<00:42,  1.88it/s, loss=1.0297]\u001b[A\n",
            "Train Epoch 1/3:  88%|████████▊ | 617/698 [06:04<00:42,  1.88it/s, loss=1.2632]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▊ | 618/698 [06:04<00:41,  1.91it/s, loss=1.2632]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▊ | 618/698 [06:04<00:41,  1.91it/s, loss=0.8424]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▊ | 619/698 [06:04<00:41,  1.91it/s, loss=0.8424]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▊ | 619/698 [06:05<00:41,  1.91it/s, loss=0.8313]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 620/698 [06:05<00:41,  1.88it/s, loss=0.8313]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 620/698 [06:05<00:41,  1.88it/s, loss=1.1906]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 621/698 [06:05<00:41,  1.87it/s, loss=1.1906]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 621/698 [06:06<00:41,  1.87it/s, loss=1.0865]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 622/698 [06:06<00:40,  1.86it/s, loss=1.0865]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 622/698 [06:06<00:40,  1.86it/s, loss=1.8846]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 623/698 [06:06<00:38,  1.93it/s, loss=1.8846]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 623/698 [06:07<00:38,  1.93it/s, loss=1.0046]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 624/698 [06:07<00:37,  1.97it/s, loss=1.0046]\u001b[A\n",
            "Train Epoch 1/3:  89%|████████▉ | 624/698 [06:08<00:37,  1.97it/s, loss=0.6830]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 625/698 [06:08<00:38,  1.89it/s, loss=0.6830]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 625/698 [06:08<00:38,  1.89it/s, loss=1.0019]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 626/698 [06:08<00:38,  1.87it/s, loss=1.0019]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 626/698 [06:09<00:38,  1.87it/s, loss=0.8663]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 627/698 [06:09<00:39,  1.80it/s, loss=0.8663]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 627/698 [06:09<00:39,  1.80it/s, loss=1.2267]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 628/698 [06:09<00:39,  1.79it/s, loss=1.2267]\u001b[A\n",
            "Train Epoch 1/3:  90%|████████▉ | 628/698 [06:10<00:39,  1.79it/s, loss=1.3267]\u001b[A\n",
            "Train Epoch 1/3:  90%|█████████ | 629/698 [06:10<00:41,  1.65it/s, loss=1.3267]\u001b[A\n",
            "Train Epoch 1/3:  90%|█████████ | 629/698 [06:11<00:41,  1.65it/s, loss=1.5070]\u001b[A\n",
            "Train Epoch 1/3:  90%|█████████ | 630/698 [06:11<00:45,  1.49it/s, loss=1.5070]\u001b[A\n",
            "Train Epoch 1/3:  90%|█████████ | 630/698 [06:12<00:45,  1.49it/s, loss=1.1929]\u001b[A\n",
            "Train Epoch 1/3:  90%|█████████ | 631/698 [06:12<00:45,  1.46it/s, loss=1.1929]\u001b[A\n",
            "Train Epoch 1/3:  90%|█████████ | 631/698 [06:12<00:45,  1.46it/s, loss=0.7397]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 632/698 [06:12<00:43,  1.51it/s, loss=0.7397]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 632/698 [06:13<00:43,  1.51it/s, loss=1.1791]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 633/698 [06:13<00:41,  1.55it/s, loss=1.1791]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 633/698 [06:13<00:41,  1.55it/s, loss=1.0780]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 634/698 [06:13<00:38,  1.64it/s, loss=1.0780]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 634/698 [06:14<00:38,  1.64it/s, loss=0.5245]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 635/698 [06:14<00:38,  1.65it/s, loss=0.5245]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 635/698 [06:14<00:38,  1.65it/s, loss=0.8185]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 636/698 [06:14<00:37,  1.65it/s, loss=0.8185]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████ | 636/698 [06:15<00:37,  1.65it/s, loss=0.9160]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████▏| 637/698 [06:15<00:34,  1.76it/s, loss=0.9160]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████▏| 637/698 [06:15<00:34,  1.76it/s, loss=0.7445]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████▏| 638/698 [06:15<00:33,  1.78it/s, loss=0.7445]\u001b[A\n",
            "Train Epoch 1/3:  91%|█████████▏| 638/698 [06:16<00:33,  1.78it/s, loss=0.6570]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 639/698 [06:16<00:32,  1.79it/s, loss=0.6570]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 639/698 [06:17<00:32,  1.79it/s, loss=0.7229]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 640/698 [06:17<00:31,  1.82it/s, loss=0.7229]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 640/698 [06:17<00:31,  1.82it/s, loss=0.7328]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 641/698 [06:17<00:30,  1.86it/s, loss=0.7328]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 641/698 [06:18<00:30,  1.86it/s, loss=1.3060]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 642/698 [06:18<00:30,  1.85it/s, loss=1.3060]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 642/698 [06:18<00:30,  1.85it/s, loss=1.0379]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 643/698 [06:18<00:28,  1.91it/s, loss=1.0379]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 643/698 [06:19<00:28,  1.91it/s, loss=0.7389]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 644/698 [06:19<00:27,  1.97it/s, loss=0.7389]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 644/698 [06:19<00:27,  1.97it/s, loss=0.9448]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 645/698 [06:19<00:27,  1.92it/s, loss=0.9448]\u001b[A\n",
            "Train Epoch 1/3:  92%|█████████▏| 645/698 [06:20<00:27,  1.92it/s, loss=1.5196]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 646/698 [06:20<00:27,  1.90it/s, loss=1.5196]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 646/698 [06:20<00:27,  1.90it/s, loss=1.2968]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 647/698 [06:20<00:25,  1.96it/s, loss=1.2968]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 647/698 [06:21<00:25,  1.96it/s, loss=0.5787]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 648/698 [06:21<00:26,  1.87it/s, loss=0.5787]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 648/698 [06:21<00:26,  1.87it/s, loss=1.3890]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 649/698 [06:21<00:25,  1.91it/s, loss=1.3890]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 649/698 [06:22<00:25,  1.91it/s, loss=2.9682]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 650/698 [06:22<00:27,  1.72it/s, loss=2.9682]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 650/698 [06:23<00:27,  1.72it/s, loss=0.8118]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 651/698 [06:23<00:28,  1.65it/s, loss=0.8118]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 651/698 [06:23<00:28,  1.65it/s, loss=0.7843]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 652/698 [06:23<00:29,  1.54it/s, loss=0.7843]\u001b[A\n",
            "Train Epoch 1/3:  93%|█████████▎| 652/698 [06:24<00:29,  1.54it/s, loss=0.6935]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▎| 653/698 [06:24<00:30,  1.48it/s, loss=0.6935]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▎| 653/698 [06:25<00:30,  1.48it/s, loss=0.8371]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▎| 654/698 [06:25<00:29,  1.50it/s, loss=0.8371]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▎| 654/698 [06:25<00:29,  1.50it/s, loss=0.9350]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 655/698 [06:25<00:26,  1.61it/s, loss=0.9350]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 655/698 [06:26<00:26,  1.61it/s, loss=0.8836]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 656/698 [06:26<00:25,  1.62it/s, loss=0.8836]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 656/698 [06:26<00:25,  1.62it/s, loss=0.5355]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 657/698 [06:26<00:24,  1.68it/s, loss=0.5355]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 657/698 [06:27<00:24,  1.68it/s, loss=0.7286]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 658/698 [06:27<00:21,  1.84it/s, loss=0.7286]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 658/698 [06:27<00:21,  1.84it/s, loss=3.3449]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 659/698 [06:27<00:20,  1.86it/s, loss=3.3449]\u001b[A\n",
            "Train Epoch 1/3:  94%|█████████▍| 659/698 [06:28<00:20,  1.86it/s, loss=0.8244]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 660/698 [06:28<00:20,  1.82it/s, loss=0.8244]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 660/698 [06:28<00:20,  1.82it/s, loss=1.4657]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 661/698 [06:28<00:19,  1.89it/s, loss=1.4657]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 661/698 [06:29<00:19,  1.89it/s, loss=0.9594]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 662/698 [06:29<00:19,  1.86it/s, loss=0.9594]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 662/698 [06:30<00:19,  1.86it/s, loss=1.2218]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 663/698 [06:30<00:19,  1.82it/s, loss=1.2218]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▍| 663/698 [06:30<00:19,  1.82it/s, loss=1.6597]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▌| 664/698 [06:30<00:19,  1.74it/s, loss=1.6597]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▌| 664/698 [06:31<00:19,  1.74it/s, loss=0.6599]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▌| 665/698 [06:31<00:18,  1.78it/s, loss=0.6599]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▌| 665/698 [06:31<00:18,  1.78it/s, loss=1.2316]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▌| 666/698 [06:31<00:17,  1.86it/s, loss=1.2316]\u001b[A\n",
            "Train Epoch 1/3:  95%|█████████▌| 666/698 [06:32<00:17,  1.86it/s, loss=0.9116]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 667/698 [06:32<00:16,  1.93it/s, loss=0.9116]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 667/698 [06:32<00:16,  1.93it/s, loss=0.9972]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 668/698 [06:32<00:16,  1.85it/s, loss=0.9972]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 668/698 [06:33<00:16,  1.85it/s, loss=0.9453]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 669/698 [06:33<00:15,  1.93it/s, loss=0.9453]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 669/698 [06:33<00:15,  1.93it/s, loss=1.4406]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 670/698 [06:33<00:14,  1.90it/s, loss=1.4406]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 670/698 [06:34<00:14,  1.90it/s, loss=0.7725]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 671/698 [06:34<00:14,  1.82it/s, loss=0.7725]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▌| 671/698 [06:35<00:14,  1.82it/s, loss=0.7683]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▋| 672/698 [06:35<00:15,  1.69it/s, loss=0.7683]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▋| 672/698 [06:35<00:15,  1.69it/s, loss=1.1330]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▋| 673/698 [06:35<00:15,  1.66it/s, loss=1.1330]\u001b[A\n",
            "Train Epoch 1/3:  96%|█████████▋| 673/698 [06:36<00:15,  1.66it/s, loss=0.8504]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 674/698 [06:36<00:14,  1.67it/s, loss=0.8504]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 674/698 [06:36<00:14,  1.67it/s, loss=0.8945]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 675/698 [06:36<00:13,  1.66it/s, loss=0.8945]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 675/698 [06:37<00:13,  1.66it/s, loss=0.4878]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 676/698 [06:37<00:13,  1.62it/s, loss=0.4878]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 676/698 [06:38<00:13,  1.62it/s, loss=0.6236]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 677/698 [06:38<00:12,  1.63it/s, loss=0.6236]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 677/698 [06:38<00:12,  1.63it/s, loss=0.5260]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 678/698 [06:38<00:12,  1.66it/s, loss=0.5260]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 678/698 [06:39<00:12,  1.66it/s, loss=1.4603]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 679/698 [06:39<00:10,  1.76it/s, loss=1.4603]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 679/698 [06:39<00:10,  1.76it/s, loss=0.6608]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 680/698 [06:39<00:10,  1.78it/s, loss=0.6608]\u001b[A\n",
            "Train Epoch 1/3:  97%|█████████▋| 680/698 [06:40<00:10,  1.78it/s, loss=2.6010]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 681/698 [06:40<00:09,  1.85it/s, loss=2.6010]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 681/698 [06:40<00:09,  1.85it/s, loss=1.2213]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 682/698 [06:40<00:08,  1.87it/s, loss=1.2213]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 682/698 [06:41<00:08,  1.87it/s, loss=1.2903]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 683/698 [06:41<00:07,  1.94it/s, loss=1.2903]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 683/698 [06:41<00:07,  1.94it/s, loss=1.5218]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 684/698 [06:41<00:06,  2.06it/s, loss=1.5218]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 684/698 [06:42<00:06,  2.06it/s, loss=0.8831]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 685/698 [06:42<00:06,  1.98it/s, loss=0.8831]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 685/698 [06:42<00:06,  1.98it/s, loss=0.6657]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 686/698 [06:42<00:06,  1.96it/s, loss=0.6657]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 686/698 [06:43<00:06,  1.96it/s, loss=0.7420]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 687/698 [06:43<00:05,  1.95it/s, loss=0.7420]\u001b[A\n",
            "Train Epoch 1/3:  98%|█████████▊| 687/698 [06:43<00:05,  1.95it/s, loss=0.9588]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▊| 688/698 [06:43<00:05,  1.95it/s, loss=0.9588]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▊| 688/698 [06:44<00:05,  1.95it/s, loss=0.8281]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▊| 689/698 [06:44<00:04,  1.88it/s, loss=0.8281]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▊| 689/698 [06:45<00:04,  1.88it/s, loss=2.2854]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 690/698 [06:45<00:04,  1.75it/s, loss=2.2854]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 690/698 [06:45<00:04,  1.75it/s, loss=0.7210]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 691/698 [06:45<00:04,  1.70it/s, loss=0.7210]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 691/698 [06:46<00:04,  1.70it/s, loss=0.7735]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 692/698 [06:46<00:03,  1.77it/s, loss=0.7735]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 692/698 [06:46<00:03,  1.77it/s, loss=3.1018]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 693/698 [06:46<00:02,  1.81it/s, loss=3.1018]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 693/698 [06:47<00:02,  1.81it/s, loss=0.8346]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 694/698 [06:47<00:02,  1.85it/s, loss=0.8346]\u001b[A\n",
            "Train Epoch 1/3:  99%|█████████▉| 694/698 [06:47<00:02,  1.85it/s, loss=1.3612]\u001b[A\n",
            "Train Epoch 1/3: 100%|█████████▉| 695/698 [06:47<00:01,  1.85it/s, loss=1.3612]\u001b[A\n",
            "Train Epoch 1/3: 100%|█████████▉| 695/698 [06:48<00:01,  1.85it/s, loss=0.7711]\u001b[A\n",
            "Train Epoch 1/3: 100%|█████████▉| 696/698 [06:48<00:01,  1.66it/s, loss=0.7711]\u001b[A\n",
            "Train Epoch 1/3: 100%|█████████▉| 696/698 [06:49<00:01,  1.66it/s, loss=1.0197]\u001b[A\n",
            "Train Epoch 1/3: 100%|█████████▉| 697/698 [06:49<00:00,  1.54it/s, loss=1.0197]\u001b[A\n",
            "Train Epoch 1/3: 100%|█████████▉| 697/698 [06:49<00:00,  1.54it/s, loss=1.7824]\u001b[A\n",
            "Train Epoch 1/3: 100%|██████████| 698/698 [06:49<00:00,  1.70it/s, loss=1.7824]\n",
            "Dev Eval: 100%|██████████| 122/122 [01:06<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 -> train_loss: 2.117572  dev_loss: 1.061941  triples P/R/F: 0.0027/0.5860/0.0054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2/3: 100%|██████████| 698/698 [06:37<00:00,  1.76it/s, loss=0.3985]\n",
            "Dev Eval: 100%|██████████| 122/122 [00:55<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 -> train_loss: 0.774167  dev_loss: 0.918669  triples P/R/F: 0.0074/0.6210/0.0147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3/3: 100%|██████████| 698/698 [06:38<00:00,  1.75it/s, loss=0.3169]\n",
            "Dev Eval: 100%|██████████| 122/122 [00:50<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 -> train_loss: 0.506005  dev_loss: 0.993247  triples P/R/F: 0.0098/0.6045/0.0193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Final Eval: 100%|██████████| 122/122 [00:28<00:00,  4.34it/s]\n",
            "Final Eval: 100%|██████████| 130/130 [00:30<00:00,  4.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CASREL FINAL ===\n",
            "DEV triple P/R/F: 0.0098 / 0.6045 / 0.0193\n",
            "TEST triple P/R/F: 0.0097 / 0.6069 / 0.0191\n",
            "Saved CASREL model and metrics to: /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_finbert_model\n"
          ]
        }
      ],
      "source": [
        "# === CASREL-style pipeline (PyTorch) with FinBERT encoder ===\n",
        "# Single Colab cell: conversion -> dataset -> model -> train(3 epochs) -> eval -> save\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Imports\n",
        "import json, os, tqdm, math\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import BertTokenizerFast, BertModel, BertConfig\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# -------------------- USER PATHS --------------------\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset\")\n",
        "# source finred-style text (your file)\n",
        "SRC_TRAIN_TXT = BASE / \"finred_train.txt\"\n",
        "SRC_DEV_TXT   = BASE / \"finred_dev.txt\"\n",
        "SRC_TEST_TXT  = BASE / \"finred_test.txt\"\n",
        "# where to store converted CASREL jsonl files\n",
        "CASREL_TRAIN = BASE / \"casrel_train.jsonl\"\n",
        "CASREL_DEV   = BASE / \"casrel_dev.jsonl\"\n",
        "CASREL_TEST  = BASE / \"casrel_test.jsonl\"\n",
        "\n",
        "OUTPUT_DIR = BASE / \"casrel_finbert_model\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# -------------------- PARAMETERS --------------------\n",
        "MODEL_NAME = \"yiyanghkust/finbert-pretrain\"\n",
        "MAX_LEN = 128          # tune as needed; keep small for memory\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------- Helper: parse FinRED-style line --------------------\n",
        "def parse_finred_line(line):\n",
        "    # format: sentence | head ; tail ; relation | head ; tail ; relation | ...\n",
        "    parts = [p.strip() for p in line.strip().split(\"|\")]\n",
        "    text = parts[0]\n",
        "    triples = []\n",
        "    for p in parts[1:]:\n",
        "        if not p:\n",
        "            continue\n",
        "        fields = [x.strip() for x in p.split(\";\") if x.strip() != \"\"]\n",
        "        if len(fields) != 3:\n",
        "            continue\n",
        "        h,t,r = fields\n",
        "        triples.append((h,t,r))\n",
        "    return {\"text\": text, \"triples\": triples}\n",
        "\n",
        "# -------------------- Tokenizer --------------------\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "# ensure consistent tokenization for offsets: use add_special_tokens=False when mapping spans\n",
        "\n",
        "# -------------------- Convert FinRED -> CASREL JSONL --------------------\n",
        "# CASREL JSON structure per line:\n",
        "# { \"text\": \"...\", \"tokens\": [...], \"spo_list\":[{\"subject\": \"...\", \"predicate\":\"...\", \"object\":\"...\"}] }\n",
        "\n",
        "def convert_txt_to_casrel_jsonl(src_path, out_path):\n",
        "    if not src_path.exists():\n",
        "        print(f\"Source {src_path} not found — skipping conversion.\")\n",
        "        return 0\n",
        "    n = 0\n",
        "    with open(src_path, \"r\", encoding=\"utf-8\") as fr, open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for ln in fr:\n",
        "            if not ln.strip():\n",
        "                continue\n",
        "            parsed = parse_finred_line(ln)\n",
        "            text = parsed[\"text\"]\n",
        "            triples = parsed[\"triples\"]\n",
        "            # build spo_list\n",
        "            spo_list = []\n",
        "            # For subjects/objects we will store the text (not token spans) — CASREL conversion to labels happens later\n",
        "            for (h,t,r) in triples:\n",
        "                spo_list.append({\"subject\": h, \"predicate\": r, \"object\": t})\n",
        "            rec = {\"text\": text, \"spo_list\": spo_list}\n",
        "            fw.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            n += 1\n",
        "    print(f\"Wrote {n} records to {out_path}\")\n",
        "    return n\n",
        "\n",
        "# Convert train/dev/test if not already converted\n",
        "convert_txt_to_casrel_jsonl(SRC_TRAIN_TXT, CASREL_TRAIN)\n",
        "convert_txt_to_casrel_jsonl(SRC_DEV_TXT, CASREL_DEV)\n",
        "convert_txt_to_casrel_jsonl(SRC_TEST_TXT, CASREL_TEST)\n",
        "\n",
        "# -------------------- Collect relation set from converted files --------------------\n",
        "def collect_relations(jsonl_paths):\n",
        "    rels = set()\n",
        "    for p in jsonl_paths:\n",
        "        if not p.exists():\n",
        "            continue\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            for ln in f:\n",
        "                rec = json.loads(ln)\n",
        "                for spo in rec.get(\"spo_list\", []):\n",
        "                    rels.add(spo[\"predicate\"])\n",
        "    rels = sorted(rels)\n",
        "    return rels\n",
        "\n",
        "relation_list = collect_relations([CASREL_TRAIN, CASREL_DEV, CASREL_TEST])\n",
        "if len(relation_list) == 0:\n",
        "    print(\"Warning: no relations found in dataset. Check input files.\")\n",
        "# create mapping\n",
        "rel2id = {r:i for i,r in enumerate(relation_list)}\n",
        "id2rel = {i:r for r,i in rel2id.items()}\n",
        "num_rels = len(relation_list)\n",
        "print(\"Number of predicates:\", num_rels)\n",
        "\n",
        "# -------------------- Dataset building: create label matrices\n",
        "# For each sample:\n",
        "#  - tokenized input_ids, attention_mask\n",
        "#  - sub_head_labels: L (0/1)\n",
        "#  - sub_tail_labels: L (0/1)\n",
        "#  - obj_head_labels: R x L  (0/1)  (object heads for each relation)\n",
        "#  - obj_tail_labels: R x L  (0/1)\n",
        "#\n",
        "# We will find token-level spans by using tokenizer offsets and .find() of subject/object in text (lowercase)\n",
        "# If multiple occurrences, use the first match (this is acceptable for FinRED typical data).\n",
        "# If any mapping fails for a triple, skip that triple.\n",
        "\n",
        "def build_casrel_records(jsonl_path, tokenizer, max_len=MAX_LEN):\n",
        "    records = []\n",
        "    if not jsonl_path.exists():\n",
        "        return records\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            rec = json.loads(ln)\n",
        "            text = rec[\"text\"]\n",
        "            spo_list = rec.get(\"spo_list\", [])\n",
        "            # tokenize with offsets\n",
        "            enc = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "            offsets = enc[\"offset_mapping\"]\n",
        "            token_ids = enc[\"input_ids\"]\n",
        "            tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            L = len(tokens)\n",
        "            if L == 0 or L > max_len:\n",
        "                # skip too long samples (or you could truncate intelligently)\n",
        "                continue\n",
        "            # initialize label structures\n",
        "            sub_heads = [0]*L\n",
        "            sub_tails = [0]*L\n",
        "            # objects: R x L zeros\n",
        "            obj_heads = [[0]*L for _ in range(num_rels)]\n",
        "            obj_tails = [[0]*L for _ in range(num_rels)]\n",
        "            any_spo = False\n",
        "            for spo in spo_list:\n",
        "                subj = spo[\"subject\"]\n",
        "                obj = spo[\"object\"]\n",
        "                pred = spo[\"predicate\"]\n",
        "                if pred not in rel2id:\n",
        "                    continue\n",
        "                rid = rel2id[pred]\n",
        "                # find subject char span\n",
        "                s_pos = text.lower().find(subj.lower())\n",
        "                if s_pos == -1:\n",
        "                    # skip if can't find\n",
        "                    continue\n",
        "                s_end = s_pos + len(subj)\n",
        "                # map to token indices\n",
        "                s_tok = None; s_tok_end = None\n",
        "                for i,(a,b) in enumerate(offsets):\n",
        "                    if a <= s_pos < b:\n",
        "                        s_tok = i\n",
        "                    if a < s_end <= b:\n",
        "                        s_tok_end = i\n",
        "                if s_tok is None or s_tok_end is None:\n",
        "                    continue\n",
        "                # find object char span\n",
        "                o_pos = text.lower().find(obj.lower())\n",
        "                if o_pos == -1:\n",
        "                    continue\n",
        "                o_end = o_pos + len(obj)\n",
        "                o_tok = None; o_tok_end = None\n",
        "                for i,(a,b) in enumerate(offsets):\n",
        "                    if a <= o_pos < b:\n",
        "                        o_tok = i\n",
        "                    if a < o_end <= b:\n",
        "                        o_tok_end = i\n",
        "                if o_tok is None or o_tok_end is None:\n",
        "                    continue\n",
        "                # set labels\n",
        "                sub_heads[s_tok] = 1\n",
        "                sub_tails[s_tok_end] = 1\n",
        "                obj_heads[rid][o_tok] = 1\n",
        "                obj_tails[rid][o_tok_end] = 1\n",
        "                any_spo = True\n",
        "            # Only keep records with at least one SPO mapping successfully\n",
        "            # (CASREL training needs supervision). If none mapped, we can still keep as negative sample, but simpler to keep negative too:\n",
        "            records.append({\n",
        "                \"text\": text,\n",
        "                \"tokens\": tokens,\n",
        "                \"input_ids\": token_ids,\n",
        "                \"offsets\": offsets,\n",
        "                \"sub_heads\": sub_heads,\n",
        "                \"sub_tails\": sub_tails,\n",
        "                \"obj_heads\": obj_heads,\n",
        "                \"obj_tails\": obj_tails\n",
        "            })\n",
        "    return records\n",
        "\n",
        "# Build datasets\n",
        "train_records = build_casrel_records(CASREL_TRAIN, tokenizer, max_len=MAX_LEN)\n",
        "dev_records   = build_casrel_records(CASREL_DEV, tokenizer, max_len=MAX_LEN)\n",
        "test_records  = build_casrel_records(CASREL_TEST, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "print(f\"Records: train {len(train_records)} dev {len(dev_records)} test {len(test_records)}\")\n",
        "\n",
        "# -------------------- Compute class-level positive rates for weighting --------------------\n",
        "# For subject head/tail and each relation's obj head/tail compute pos weights\n",
        "def compute_pos_weights(records):\n",
        "    # subject\n",
        "    s_heads = 0; s_total = 0\n",
        "    s_tails = 0\n",
        "    obj_counts_head = [0]*num_rels\n",
        "    obj_counts_tail = [0]*num_rels\n",
        "    total_tokens = 0\n",
        "    for r in records:\n",
        "        L = len(r[\"sub_heads\"])\n",
        "        total_tokens += L\n",
        "        s_heads += sum(r[\"sub_heads\"])\n",
        "        s_tails += sum(r[\"sub_tails\"])\n",
        "        for rid in range(num_rels):\n",
        "            obj_counts_head[rid] += sum(r[\"obj_heads\"][rid])\n",
        "            obj_counts_tail[rid] += sum(r[\"obj_tails\"][rid])\n",
        "    # compute positive weights: weight = (neg / pos) or similar; BCEWithLogits allows pos_weight\n",
        "    s_head_pos = s_heads\n",
        "    s_tail_pos = s_tails\n",
        "    s_head_neg = total_tokens - s_head_pos\n",
        "    s_tail_neg = total_tokens - s_tail_pos\n",
        "    s_head_pos_weight = (s_head_neg / (s_head_pos+1e-6)) if s_head_pos>0 else 1.0\n",
        "    s_tail_pos_weight = (s_tail_neg / (s_tail_pos+1e-6)) if s_tail_pos>0 else 1.0\n",
        "    obj_pos_weights_head = []\n",
        "    obj_pos_weights_tail = []\n",
        "    for rid in range(num_rels):\n",
        "        pos_h = obj_counts_head[rid]\n",
        "        neg_h = total_tokens - pos_h\n",
        "        pos_t = obj_counts_tail[rid]\n",
        "        neg_t = total_tokens - pos_t\n",
        "        w_h = (neg_h / (pos_h+1e-6)) if pos_h>0 else 1.0\n",
        "        w_t = (neg_t / (pos_t+1e-6)) if pos_t>0 else 1.0\n",
        "        obj_pos_weights_head.append(w_h)\n",
        "        obj_pos_weights_tail.append(w_t)\n",
        "    return {\n",
        "        \"s_head\": s_head_pos_weight,\n",
        "        \"s_tail\": s_tail_pos_weight,\n",
        "        \"obj_head\": obj_pos_weights_head,\n",
        "        \"obj_tail\": obj_pos_weights_tail\n",
        "    }\n",
        "\n",
        "weights = compute_pos_weights(train_records)\n",
        "print(\"Computed pos-weights (approx):\", weights)\n",
        "\n",
        "# -------------------- Dataset & collate (pad to batch max length) --------------------\n",
        "class CASRELDataset(Dataset):\n",
        "    def __init__(self, records, tokenizer, max_len=MAX_LEN):\n",
        "        self.records = records\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        input_ids = r[\"input_ids\"]\n",
        "        attention_mask = [1]*len(input_ids)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"sub_head\": torch.tensor(r[\"sub_heads\"], dtype=torch.float),   # L\n",
        "            \"sub_tail\": torch.tensor(r[\"sub_tails\"], dtype=torch.float),   # L\n",
        "            \"obj_head\": torch.tensor(r[\"obj_heads\"], dtype=torch.float),   # R x L\n",
        "            \"obj_tail\": torch.tensor(r[\"obj_tails\"], dtype=torch.float)    # R x L\n",
        "        }\n",
        "\n",
        "def casrel_collate(batch):\n",
        "    # pad input_ids and attention_mask to max_len in batch; pad label matrices accordingly\n",
        "    max_len = max([b[\"input_ids\"].size(0) for b in batch])\n",
        "    R = num_rels\n",
        "    input_ids_p = []\n",
        "    attn_p = []\n",
        "    sub_head_p = []\n",
        "    sub_tail_p = []\n",
        "    obj_head_p = []\n",
        "    obj_tail_p = []\n",
        "    for b in batch:\n",
        "        l = b[\"input_ids\"].size(0)\n",
        "        pad_len = max_len - l\n",
        "        input_ids_p.append(torch.cat([b[\"input_ids\"], torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)]))\n",
        "        attn_p.append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        # for token labels\n",
        "        sub_head_p.append(torch.cat([b[\"sub_head\"], torch.zeros(pad_len)]))\n",
        "        sub_tail_p.append(torch.cat([b[\"sub_tail\"], torch.zeros(pad_len)]))\n",
        "        # obj labels shape R x L -> pad each row\n",
        "        oh = b[\"obj_head\"]\n",
        "        ot = b[\"obj_tail\"]\n",
        "        # oh shape: R x L_cur\n",
        "        # pad to R x max_len\n",
        "        oh_p = torch.cat([oh, torch.zeros((R, pad_len))], dim=1)\n",
        "        ot_p = torch.cat([ot, torch.zeros((R, pad_len))], dim=1)\n",
        "        obj_head_p.append(oh_p)\n",
        "        obj_tail_p.append(ot_p)\n",
        "    batch_out = {\n",
        "        \"input_ids\": torch.stack(input_ids_p),\n",
        "        \"attention_mask\": torch.stack(attn_p),\n",
        "        \"sub_head\": torch.stack(sub_head_p),\n",
        "        \"sub_tail\": torch.stack(sub_tail_p),\n",
        "        \"obj_head\": torch.stack(obj_head_p),  # B x R x L\n",
        "        \"obj_tail\": torch.stack(obj_tail_p)\n",
        "    }\n",
        "    return batch_out\n",
        "\n",
        "# Create DataLoaders\n",
        "train_ds = CASRELDataset(train_records, tokenizer, max_len=MAX_LEN)\n",
        "dev_ds   = CASRELDataset(dev_records, tokenizer, max_len=MAX_LEN)\n",
        "test_ds  = CASRELDataset(test_records, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# Optional: WeightedRandomSampler for records (here we keep it simple)\n",
        "# For CASREL we balance via pos_weights in BCE losses rather than sampler\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=casrel_collate)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "\n",
        "print(\"DataLoaders ready. Example batch sizes:\", BATCH_SIZE)\n",
        "\n",
        "# -------------------- CASREL-like Model (simplified & robust) --------------------\n",
        "class SimpleCasRel(nn.Module):\n",
        "    def __init__(self, bert_name, hidden_size=768, num_rels=0):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.num_rels = num_rels\n",
        "        # subject taggers (binary per token)\n",
        "        self.sub_head_proj = nn.Linear(self.hidden_size, 1)\n",
        "        self.sub_tail_proj = nn.Linear(self.hidden_size, 1)\n",
        "        # object taggers: conditioned on subject representation\n",
        "        # we will concat token repr and subject repr -> project to hidden -> predict R heads and R tails\n",
        "        self.obj_fc = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
        "        self.obj_head_proj = nn.Linear(self.hidden_size, self.num_rels)\n",
        "        self.obj_tail_proj = nn.Linear(self.hidden_size, self.num_rels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, subject_span=None):\n",
        "        # input: B x L\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        seq_out = bert_out.last_hidden_state  # B x L x H\n",
        "        # subject logits\n",
        "        sub_head_logits = self.sub_head_proj(seq_out).squeeze(-1)  # B x L\n",
        "        sub_tail_logits = self.sub_tail_proj(seq_out).squeeze(-1)\n",
        "        # If subject_span is provided (B x 2: start,end) during inference, or B x list of gold during training:\n",
        "        # For training we will pass subject_span as a tensor B x 2 (the first subject occurrence) -- but many sentences have multiple subjects.\n",
        "        # To keep it practical: during training we compute object logits conditioned on *each gold subject* separately.\n",
        "        # Here, for a batch-level forward we support subject_span as:\n",
        "        # - None: return subject logits only\n",
        "        # - tensor of shape (B, 2): single subject per sample (start,end)\n",
        "        subj_cond_obj_head = None\n",
        "        subj_cond_obj_tail = None\n",
        "        if subject_span is not None:\n",
        "            # subject_span: B x 2 (start_idx, end_idx) - we compute subject representation as mean of token vectors in span\n",
        "            # subject_span can be tensor of ints\n",
        "            B, L, H = seq_out.size()\n",
        "            # build subj_repr: B x H\n",
        "            start = subject_span[:,0].clamp(0, L-1)\n",
        "            end = subject_span[:,1].clamp(0, L-1)\n",
        "            subj_repr = []\n",
        "            for i in range(B):\n",
        "                s = start[i].item(); e = end[i].item()\n",
        "                # average pooling of tokens s..e inclusive\n",
        "                if e < s:\n",
        "                    e = s\n",
        "                vec = seq_out[i, s:e+1, :].mean(dim=0)\n",
        "                subj_repr.append(vec)\n",
        "            subj_repr = torch.stack(subj_repr, dim=0)  # B x H\n",
        "            # expand subj repr to tokens and concat\n",
        "            subj_exp = subj_repr.unsqueeze(1).expand(-1, seq_out.size(1), -1)  # B x L x H\n",
        "            concat = torch.cat([seq_out, subj_exp], dim=-1)  # B x L x 2H\n",
        "            h = self.relu(self.obj_fc(concat))  # B x L x H\n",
        "            # project to R classes per token\n",
        "            # we want output shaped B x R x L -> transpose\n",
        "            oh = self.obj_head_proj(h)  # B x L x R\n",
        "            ot = self.obj_tail_proj(h)  # B x L x R\n",
        "            # transpose to B x R x L\n",
        "            subj_cond_obj_head = oh.permute(0,2,1)\n",
        "            subj_cond_obj_tail = ot.permute(0,2,1)\n",
        "        return sub_head_logits, sub_tail_logits, subj_cond_obj_head, subj_cond_obj_tail\n",
        "\n",
        "# instantiate\n",
        "model = SimpleCasRel(MODEL_NAME, num_rels=num_rels)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# -------------------- Loss functions with pos-weights --------------------\n",
        "# subject BCE losses use scalar pos weight\n",
        "s_head_pos_weight = torch.tensor(weights[\"s_head\"], dtype=torch.float).to(DEVICE)\n",
        "s_tail_pos_weight = torch.tensor(weights[\"s_tail\"], dtype=torch.float).to(DEVICE)\n",
        "sub_head_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_head_pos_weight)\n",
        "sub_tail_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_tail_pos_weight)\n",
        "\n",
        "# object pos weights per relation -> create tensors of shape (R,) for pos_weight used per class\n",
        "obj_head_pos_weight = torch.tensor(weights[\"obj_head\"], dtype=torch.float).to(DEVICE) if num_rels>0 else torch.ones((1,), device=DEVICE)\n",
        "obj_tail_pos_weight = torch.tensor(weights[\"obj_tail\"], dtype=torch.float).to(DEVICE) if num_rels>0 else torch.ones((1,), device=DEVICE)\n",
        "# For BCEWithLogitsLoss with multi-label, we can pass pos_weight as (R,) by reshaping logits to (B*L, R).\n",
        "# We'll compute loss manually by flattening.\n",
        "\n",
        "# optimizer + scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * EPOCHS if len(train_loader)>0 else 1\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1,int(0.1*total_steps)), num_training_steps=total_steps)\n",
        "\n",
        "# -------------------- Utility: decode predicted triples from model outputs --------------------\n",
        "def decode_triples_from_preds(tokens, sub_head_logits, sub_tail_logits, obj_head_logits=None, obj_tail_logits=None, th_sub=0.5, th_obj=0.5):\n",
        "    # logits are raw (not sigmoid). We'll apply sigmoid thresholding.\n",
        "    # sub_head_logits, sub_tail_logits: L floats\n",
        "    # obj_head_logits, obj_tail_logits: R x L (optional) conditioned on a subject\n",
        "    import numpy as np\n",
        "    sub_h = (torch.sigmoid(sub_head_logits) > th_sub).cpu().numpy().astype(int)\n",
        "    sub_t = (torch.sigmoid(sub_tail_logits) > th_sub).cpu().numpy().astype(int)\n",
        "    L = len(sub_h)\n",
        "    subjects = []\n",
        "    # find all subject spans by pairing heads and tails naively: for each head index, find nearest tail >= head\n",
        "    for i in range(L):\n",
        "        if sub_h[i]==1:\n",
        "            # find tail j >= i where sub_t[j]==1; choose the first\n",
        "            j = None\n",
        "            for k in range(i, L):\n",
        "                if sub_t[k]==1:\n",
        "                    j = k\n",
        "                    break\n",
        "            if j is not None:\n",
        "                subjects.append((i,j))\n",
        "    triples = []\n",
        "    if obj_head_logits is None or obj_tail_logits is None:\n",
        "        return triples  # no objects predicted\n",
        "    # obj_head_logits: R x L tensor (for this subject)\n",
        "    oh_sig = torch.sigmoid(obj_head_logits).cpu().numpy()\n",
        "    ot_sig = torch.sigmoid(obj_tail_logits).cpu().numpy()\n",
        "    R, L = oh_sig.shape\n",
        "    for (s_start, s_end) in subjects:\n",
        "        for rid in range(R):\n",
        "            # find object heads where prob>th_obj\n",
        "            for i in range(L):\n",
        "                if oh_sig[rid, i] > th_obj:\n",
        "                    # find tail j >= i where ot_sig[rid,j] > th_obj\n",
        "                    j = None\n",
        "                    for k in range(i, L):\n",
        "                        if ot_sig[rid, k] > th_obj:\n",
        "                            j = k\n",
        "                            break\n",
        "                    if j is not None:\n",
        "                        triples.append({\n",
        "                            \"subject_span\": (s_start, s_end),\n",
        "                            \"object_span\": (i,j),\n",
        "                            \"predicate\": id2rel[rid]\n",
        "                        })\n",
        "    return triples\n",
        "\n",
        "# -------------------- Training loop (cascade-style)\n",
        "def train_and_evaluate(model, train_loader, dev_loader, test_loader, epochs=EPOCHS):\n",
        "    history = {\"train_loss\": [], \"dev_loss\":[]}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f\"Train Epoch {epoch}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)            # B x L\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            B, L = input_ids.shape\n",
        "            # sub labels: B x L\n",
        "            sub_head_gold = batch[\"sub_head\"].to(DEVICE)\n",
        "            sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "            # obj labels: B x R x L\n",
        "            obj_head_gold = batch[\"obj_head\"].to(DEVICE)\n",
        "            obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            # forward -> sub logits\n",
        "            sub_head_logits, sub_tail_logits, _, _ = model(input_ids=input_ids, attention_mask=attn, subject_span=None)\n",
        "            # sub losses\n",
        "            loss_sh = sub_head_loss_fn(sub_head_logits, sub_head_gold)\n",
        "            loss_st = sub_tail_loss_fn(sub_tail_logits, sub_tail_gold)\n",
        "            # For object losses: for each sample, iterate over gold subject spans and compute object predictions conditioned on that subject\n",
        "            # To keep computation reasonable we will find all gold subject spans from sub_head_gold & sub_tail_gold and for each subject compute object logits\n",
        "            loss_obj_total = 0.0\n",
        "            obj_loss_count = 0\n",
        "            for i in range(B):\n",
        "                # find gold subject spans in sample i\n",
        "                sh = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                st = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                subj_spans = []\n",
        "                for p in range(L):\n",
        "                    if sh[p]==1:\n",
        "                        # find tail\n",
        "                        q = None\n",
        "                        for k in range(p, L):\n",
        "                            if st[k]==1:\n",
        "                                q = k\n",
        "                                break\n",
        "                        if q is not None:\n",
        "                            subj_spans.append((p,q))\n",
        "                # if no gold subject spans, skip object loss for this sample\n",
        "                if len(subj_spans)==0:\n",
        "                    continue\n",
        "                for (s_start, s_end) in subj_spans:\n",
        "                    subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)  # 1 x 2\n",
        "                    # forward conditioned on this gold subject\n",
        "                    _, _, obj_head_logits, obj_tail_logits = model(input_ids=input_ids[i:i+1,:], attention_mask=attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                    # obj logits: 1 x R x L\n",
        "                    # gold obj labels for this sample: R x L\n",
        "                    gold_oh = obj_head_gold[i]  # R x L\n",
        "                    gold_ot = obj_tail_gold[i]\n",
        "                    # Flatten to (R, L) -> compute BCEWithLogits per relation class using pos_weights\n",
        "                    # To match torch's pos_weight shape, we reshape logits to (R, L) -> (R,L) and apply BCE via flattening\n",
        "                    logits_oh = obj_head_logits.squeeze(0)  # R x L\n",
        "                    logits_ot = obj_tail_logits.squeeze(0)\n",
        "                    # compute BCEWithLogits per relation using pos weights vector\n",
        "                    # manual compute: BCEWithLogitsLoss with pos_weight for each class expects input shape (N,*) and pos_weight aligned with last dim\n",
        "                    # We'll compute loss relation-wise and average\n",
        "                    loss_rel_h = 0.0\n",
        "                    loss_rel_t = 0.0\n",
        "                    for rid in range(num_rels):\n",
        "                        # flatten over tokens\n",
        "                        logit_r_h = logits_oh[rid]       # L\n",
        "                        logit_r_t = logits_ot[rid]\n",
        "                        gold_r_h = gold_oh[rid].to(DEVICE)\n",
        "                        gold_r_t = gold_ot[rid].to(DEVICE)\n",
        "                        # create BCEWithLogits with pos_weight specific to this relation\n",
        "                        pos_w_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float).to(DEVICE)\n",
        "                        pos_w_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float).to(DEVICE)\n",
        "                        loss_h = nn.BCEWithLogitsLoss(pos_weight=pos_w_h)(logit_r_h, gold_r_h)\n",
        "                        loss_t = nn.BCEWithLogitsLoss(pos_weight=pos_w_t)(logit_r_t, gold_r_t)\n",
        "                        loss_rel_h += loss_h\n",
        "                        loss_rel_t += loss_t\n",
        "                    loss_rel = (loss_rel_h + loss_rel_t) / max(1, num_rels)\n",
        "                    loss_obj_total += loss_rel\n",
        "                    obj_loss_count += 1\n",
        "            if obj_loss_count>0:\n",
        "                loss_obj_total = loss_obj_total / obj_loss_count\n",
        "            else:\n",
        "                loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "            loss = loss_sh + loss_st + loss_obj_total\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        # Validation loss + metrics\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        preds_triples = []\n",
        "        gold_triples = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm.tqdm(dev_loader, desc=\"Dev Eval\"):\n",
        "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "                attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "                B,L = input_ids.shape\n",
        "                sub_head_gold = batch[\"sub_head\"].to(DEVICE)\n",
        "                sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "                obj_head_gold = batch[\"obj_head\"].to(DEVICE)\n",
        "                obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "                # subject predictions\n",
        "                sub_head_logits, sub_tail_logits, _, _ = model(input_ids=input_ids, attention_mask=attn, subject_span=None)\n",
        "                # loss on subject\n",
        "                loss_sh = sub_head_loss_fn(sub_head_logits, sub_head_gold)\n",
        "                loss_st = sub_tail_loss_fn(sub_tail_logits, sub_tail_gold)\n",
        "                # compute object loss using gold subjects (same as training)\n",
        "                loss_obj_total = 0.0; obj_loss_count=0\n",
        "                for i in range(B):\n",
        "                    sh = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                    st = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                    subj_spans=[]\n",
        "                    for p in range(L):\n",
        "                        if sh[p]==1:\n",
        "                            q=None\n",
        "                            for k in range(p,L):\n",
        "                                if st[k]==1:\n",
        "                                    q=k; break\n",
        "                            if q is not None:\n",
        "                                subj_spans.append((p,q))\n",
        "                    if len(subj_spans)==0:\n",
        "                        continue\n",
        "                    for (s_start, s_end) in subj_spans:\n",
        "                        subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                        _, _, obj_head_logits, obj_tail_logits = model(input_ids=input_ids[i:i+1,:], attention_mask=attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                        logits_oh = obj_head_logits.squeeze(0)\n",
        "                        logits_ot = obj_tail_logits.squeeze(0)\n",
        "                        gold_oh = obj_head_gold[i]\n",
        "                        gold_ot = obj_tail_gold[i]\n",
        "                        loss_rel_h=0.0; loss_rel_t=0.0\n",
        "                        for rid in range(num_rels):\n",
        "                            pos_w_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float).to(DEVICE)\n",
        "                            pos_w_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float).to(DEVICE)\n",
        "                            loss_rel_h += nn.BCEWithLogitsLoss(pos_weight=pos_w_h)(logits_oh[rid], gold_oh[rid].to(DEVICE))\n",
        "                            loss_rel_t += nn.BCEWithLogitsLoss(pos_weight=pos_w_t)(logits_ot[rid], gold_ot[rid].to(DEVICE))\n",
        "                        loss_obj_total += (loss_rel_h + loss_rel_t)/max(1,num_rels)\n",
        "                        obj_loss_count += 1\n",
        "                if obj_loss_count>0:\n",
        "                    loss_obj_total = loss_obj_total / obj_loss_count\n",
        "                else:\n",
        "                    loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "                batch_loss = loss_sh + loss_st + loss_obj_total\n",
        "                val_loss += batch_loss.item()\n",
        "                # decode predictions for metrics:\n",
        "                # get predictions per sample: subject spans from predicted sub logits, then for each subject predict objects\n",
        "                for i in range(B):\n",
        "                    # tokens are not passed, but we only need spans for metric: predicted subj/object spans + predicate\n",
        "                    sub_h_logits_i = sub_head_logits[i]   # L\n",
        "                    sub_t_logits_i = sub_tail_logits[i]\n",
        "                    # find subject spans\n",
        "                    sub_spans=[]\n",
        "                    sh_sig = (torch.sigmoid(sub_h_logits_i)>0.5).cpu().numpy().astype(int)\n",
        "                    st_sig = (torch.sigmoid(sub_t_logits_i)>0.5).cpu().numpy().astype(int)\n",
        "                    for p in range(L):\n",
        "                        if sh_sig[p]==1:\n",
        "                            q=None\n",
        "                            for k in range(p, L):\n",
        "                                if st_sig[k]==1:\n",
        "                                    q=k; break\n",
        "                            if q is not None:\n",
        "                                sub_spans.append((p,q))\n",
        "                    gold_for_sample = []\n",
        "                    # collect gold triples from gold label matrices\n",
        "                    # for each rid, find object spans where gold obj head/tail are 1\n",
        "                    gold_sub_spans = []\n",
        "                    shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                    stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            q=None\n",
        "                            for k in range(p,L):\n",
        "                                if stg[k]==1:\n",
        "                                    q=k; break\n",
        "                            if q is not None:\n",
        "                                gold_sub_spans.append((p,q))\n",
        "                    # for each gold subj, collect gold obj spans and add to gold_triples\n",
        "                    for (s_start, s_end) in gold_sub_spans:\n",
        "                        for rid in range(num_rels):\n",
        "                            ohg = obj_head_gold[i, rid].cpu().numpy().astype(int)\n",
        "                            otg = obj_tail_gold[i, rid].cpu().numpy().astype(int)\n",
        "                            for a in range(L):\n",
        "                                if ohg[a]==1:\n",
        "                                    b=None\n",
        "                                    for k in range(a,L):\n",
        "                                        if otg[k]==1:\n",
        "                                            b=k; break\n",
        "                                    if b is not None:\n",
        "                                        gold_for_sample.append(((s_start, s_end),(a,b), id2rel[rid]))\n",
        "                    gold_triples.extend(gold_for_sample)\n",
        "                    # predictions for objects: for each predicted subject span do forward conditioned on that subject\n",
        "                    pred_for_sample = []\n",
        "                    for (s_start, s_end) in sub_spans:\n",
        "                        subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                        _, _, obj_head_logits, obj_tail_logits = model(input_ids=input_ids[i:i+1,:], attention_mask=attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                        obj_head_logits = obj_head_logits.squeeze(0)  # R x L\n",
        "                        obj_tail_logits = obj_tail_logits.squeeze(0)\n",
        "                        # threshold\n",
        "                        oh_sig = (torch.sigmoid(obj_head_logits) > 0.5).cpu().numpy().astype(int)\n",
        "                        ot_sig = (torch.sigmoid(obj_tail_logits) > 0.5).cpu().numpy().astype(int)\n",
        "                        for rid in range(num_rels):\n",
        "                            for a in range(L):\n",
        "                                if oh_sig[rid,a]==1:\n",
        "                                    b=None\n",
        "                                    for k in range(a,L):\n",
        "                                        if ot_sig[rid,k]==1:\n",
        "                                            b=k; break\n",
        "                                    if b is not None:\n",
        "                                        pred_for_sample.append(((s_start, s_end),(a,b), id2rel[rid]))\n",
        "                    preds_triples.extend(pred_for_sample)\n",
        "        avg_val_loss = val_loss / max(1, len(dev_loader))\n",
        "        history[\"dev_loss\"].append(avg_val_loss)\n",
        "        # compute triple-level metrics (exact match of subject span, object span, predicate)\n",
        "        # convert gold_triples and preds_triples into sets of (s_s,s_e,o_s,o_e,pred)\n",
        "        def to_set(triples_list):\n",
        "            s = set()\n",
        "            for t in triples_list:\n",
        "                (ss,se),(os,oe),pred = t\n",
        "                s.add((ss,se,os,oe,pred))\n",
        "            return s\n",
        "        gold_set = to_set(gold_triples)\n",
        "        pred_set = to_set(preds_triples)\n",
        "        # calculate precision/recall/f1\n",
        "        tp = len(pred_set & gold_set)\n",
        "        fp = len(pred_set - gold_set)\n",
        "        fn = len(gold_set - pred_set)\n",
        "        prec = tp / (tp+fp) if tp+fp>0 else 0.0\n",
        "        rec = tp / (tp+fn) if tp+fn>0 else 0.0\n",
        "        f1 = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "        print(f\"\\nEpoch {epoch} -> train_loss: {avg_train_loss:.6f}  dev_loss: {avg_val_loss:.6f}  triples P/R/F: {prec:.4f}/{rec:.4f}/{f1:.4f}\")\n",
        "    return model, history\n",
        "\n",
        "# Run training + evaluation\n",
        "model, history = train_and_evaluate(model, train_loader, dev_loader, test_loader, epochs=EPOCHS)\n",
        "\n",
        "# Final evaluation on test set (same procedure as used for dev)\n",
        "def evaluate_final(model, data_loader):\n",
        "    model.eval()\n",
        "    preds_triples = []\n",
        "    gold_triples = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(data_loader, desc=\"Final Eval\"):\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            B,L = input_ids.shape\n",
        "            sub_head_gold = batch[\"sub_head\"].to(DEVICE)\n",
        "            sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "            obj_head_gold = batch[\"obj_head\"].to(DEVICE)\n",
        "            obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "            # predict subjects\n",
        "            sub_head_logits, sub_tail_logits, _, _ = model(input_ids=input_ids, attention_mask=attn, subject_span=None)\n",
        "            for i in range(B):\n",
        "                sh_sig = (torch.sigmoid(sub_head_logits[i])>0.5).cpu().numpy().astype(int)\n",
        "                st_sig = (torch.sigmoid(sub_tail_logits[i])>0.5).cpu().numpy().astype(int)\n",
        "                pred_subs=[]\n",
        "                gold_subs=[]\n",
        "                for p in range(L):\n",
        "                    if sh_sig[p]==1:\n",
        "                        q=None\n",
        "                        for k in range(p,L):\n",
        "                            if st_sig[k]==1:\n",
        "                                q=k; break\n",
        "                        if q is not None: pred_subs.append((p,q))\n",
        "                    # gold\n",
        "                shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        q=None\n",
        "                        for k in range(p,L):\n",
        "                            if stg[k]==1:\n",
        "                                q=k; break\n",
        "                        if q is not None: gold_subs.append((p,q))\n",
        "                # for each predicted subj, predict objects\n",
        "                for (s_start, s_end) in pred_subs:\n",
        "                    subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                    _, _, obj_head_logits, obj_tail_logits = model(input_ids=input_ids[i:i+1,:], attention_mask=attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                    oh_sig = (torch.sigmoid(obj_head_logits.squeeze(0))>0.5).cpu().numpy().astype(int)\n",
        "                    ot_sig = (torch.sigmoid(obj_tail_logits.squeeze(0))>0.5).cpu().numpy().astype(int)\n",
        "                    for rid in range(num_rels):\n",
        "                        for a in range(L):\n",
        "                            if oh_sig[rid,a]==1:\n",
        "                                b=None\n",
        "                                for k in range(a,L):\n",
        "                                    if ot_sig[rid,k]==1:\n",
        "                                        b=k; break\n",
        "                                if b is not None:\n",
        "                                    preds_triples.append(((s_start,s_end),(a,b), id2rel[rid]))\n",
        "                # gold triples from gold labels\n",
        "                for (s_start,s_end) in gold_subs:\n",
        "                    for rid in range(num_rels):\n",
        "                        ohg = obj_head_gold[i,rid].cpu().numpy().astype(int)\n",
        "                        otg = obj_tail_gold[i,rid].cpu().numpy().astype(int)\n",
        "                        for a in range(L):\n",
        "                            if ohg[a]==1:\n",
        "                                b=None\n",
        "                                for k in range(a,L):\n",
        "                                    if otg[k]==1:\n",
        "                                        b=k; break\n",
        "                                if b is not None:\n",
        "                                    gold_triples.append(((s_start,s_end),(a,b), id2rel[rid]))\n",
        "    # compute metrics\n",
        "    def to_set(triples_list):\n",
        "        s = set()\n",
        "        for t in triples_list:\n",
        "            (ss,se),(os,oe),pred = t\n",
        "            s.add((ss,se,os,oe,pred))\n",
        "        return s\n",
        "    gset = to_set(gold_triples)\n",
        "    pset = to_set(preds_triples)\n",
        "    tp = len(pset & gset)\n",
        "    fp = len(pset - gset)\n",
        "    fn = len(gset - pset)\n",
        "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "    rec = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "    f1 = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "    return {\"precision\":prec, \"recall\":rec, \"f1\":f1, \"tp\":tp, \"fp\":fp, \"fn\":fn}\n",
        "\n",
        "final_dev = evaluate_final(model, dev_loader)\n",
        "final_test = evaluate_final(model, test_loader)\n",
        "\n",
        "print(\"\\n=== CASREL FINAL ===\")\n",
        "print(\"DEV triple P/R/F: {:.4f} / {:.4f} / {:.4f}\".format(final_dev[\"precision\"], final_dev[\"recall\"], final_dev[\"f1\"]))\n",
        "print(\"TEST triple P/R/F: {:.4f} / {:.4f} / {:.4f}\".format(final_test[\"precision\"], final_test[\"recall\"], final_test[\"f1\"]))\n",
        "\n",
        "# Save outputs\n",
        "metrics = {\n",
        "    \"relation_list\": relation_list,\n",
        "    \"weights\": weights,\n",
        "    \"history\": history,\n",
        "    \"dev_final\": final_dev,\n",
        "    \"test_final\": final_test,\n",
        "    \"records_counts\": {\"train\": len(train_records), \"dev\": len(dev_records), \"test\": len(test_records)}\n",
        "}\n",
        "with open(OUTPUT_DIR / \"casrel_metrics_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# save model + tokenizer\n",
        "model_to_save = model\n",
        "model_path = OUTPUT_DIR / \"model.pt\"\n",
        "torch.save(model_to_save.state_dict(), model_path)\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "\n",
        "print(\"Saved CASREL model and metrics to:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 1. GOOGLE DRIVE + PATHS\n",
        "# ===============================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset\")\n",
        "\n",
        "SRC_TRAIN_TXT = BASE / \"finred_train.txt\"\n",
        "SRC_DEV_TXT   = BASE / \"finred_dev.txt\"\n",
        "SRC_TEST_TXT  = BASE / \"finred_test.txt\"\n",
        "\n",
        "CASREL_TRAIN = BASE / \"casrel_train1.jsonl\"\n",
        "CASREL_DEV   = BASE / \"casrel_dev1.jsonl\"\n",
        "CASREL_TEST  = BASE / \"casrel_test1.jsonl\"\n",
        "\n",
        "OUTPUT_DIR = BASE / \"casrel_finbert_model_final\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# ===============================================\n",
        "# 2. FINRED → CASREL CONVERSION\n",
        "# ===============================================\n",
        "\n",
        "def load_finred_file(path):\n",
        "    items = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            items.append(json.loads(line))\n",
        "    return items\n",
        "\n",
        "def convert_finred_to_casrel(finred_items, output_path):\n",
        "    with open(output_path, \"w\") as out:\n",
        "        for item in finred_items:\n",
        "            text = item[\"text\"]\n",
        "            spo_list = []\n",
        "\n",
        "            for rel in item[\"relations\"]:\n",
        "                spo_list.append({\n",
        "                    \"subject\": rel[\"head\"][\"text\"],\n",
        "                    \"predicate\": rel[\"type\"],\n",
        "                    \"object\": rel[\"tail\"][\"text\"]\n",
        "                })\n",
        "\n",
        "            out.write(json.dumps({\n",
        "                \"text\": text,\n",
        "                \"spo_list\": spo_list\n",
        "            }) + \"\\n\")\n",
        "\n",
        "convert_finred_to_casrel(load_finred_file(SRC_TRAIN_TXT), CASREL_TRAIN)\n",
        "convert_finred_to_casrel(load_finred_file(SRC_DEV_TXT), CASREL_DEV)\n",
        "convert_finred_to_casrel(load_finred_file(SRC_TEST_TXT), CASREL_TEST)\n",
        "\n",
        "print(\"CASREL files generated successfully.\")\n",
        "\n",
        "# ===============================================\n",
        "# 3. DATASET + TOKENIZER\n",
        "# ===============================================\n",
        "!pip install transformers accelerate\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-pretrain\")\n",
        "\n",
        "class CasRelDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, tokenizer):\n",
        "        self.items = [json.loads(line) for line in open(jsonl_path)]\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.items[idx]\n",
        "        encoded = self.tokenizer(\n",
        "            item[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # === Label creation ===\n",
        "        # Binary subject/object head prediction\n",
        "        seq_len = encoded[\"input_ids\"].shape[-1]\n",
        "        subj_head = torch.zeros(seq_len)\n",
        "        obj_head = torch.zeros(seq_len)\n",
        "\n",
        "        # VERY IMPORTANT: improve precision with strict matching\n",
        "        for spo in item[\"spo_list\"]:\n",
        "            sub = spo[\"subject\"]\n",
        "            obj = spo[\"object\"]\n",
        "            sub_ids = self.tokenizer.encode(sub, add_special_tokens=False)\n",
        "            obj_ids = self.tokenizer.encode(obj, add_special_tokens=False)\n",
        "            ids = encoded[\"input_ids\"][0].tolist()\n",
        "\n",
        "            def find_start(ids, pattern):\n",
        "                for i in range(len(ids) - len(pattern)):\n",
        "                    if ids[i:i+len(pattern)] == pattern:\n",
        "                        return i\n",
        "                return None\n",
        "\n",
        "            si = find_start(ids, sub_ids)\n",
        "            oi = find_start(ids, obj_ids)\n",
        "\n",
        "            if si is not None:\n",
        "                subj_head[si] = 1.0\n",
        "            if oi is not None:\n",
        "                obj_head[oi] = 1.0\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"subj_head\": subj_head,\n",
        "            \"obj_head\": obj_head\n",
        "        }\n",
        "\n",
        "train_ds = CasRelDataset(CASREL_TRAIN, tokenizer)\n",
        "dev_ds   = CasRelDataset(CASREL_DEV, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=8)\n",
        "\n",
        "# ===============================================\n",
        "# 4. MODEL (CASREL + FinBERT encoder)\n",
        "#    – improved precision: focal loss + label smoothing\n",
        "# ===============================================\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce_loss = self.bce(logits, targets)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal = self.alpha * (1-pt)**self.gamma * bce_loss\n",
        "        return focal.mean()\n",
        "\n",
        "class CasRelFinBERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(\"yiyanghkust/finbert-pretrain\")\n",
        "        hidden = 768\n",
        "\n",
        "        # Two heads\n",
        "        self.subj_classifier = nn.Linear(hidden, 1)\n",
        "        self.obj_classifier  = nn.Linear(hidden, 1)\n",
        "\n",
        "        self.loss_fn = FocalLoss()   # stronger precision boosting + balanced loss\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, subj_head=None, obj_head=None):\n",
        "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state\n",
        "\n",
        "        subj_logits = self.subj_classifier(last_hidden).squeeze(-1)\n",
        "        obj_logits  = self.obj_classifier(last_hidden).squeeze(-1)\n",
        "\n",
        "        if subj_head is not None:\n",
        "            loss_s = self.loss_fn(subj_logits, subj_head)\n",
        "            loss_o = self.loss_fn(obj_logits, obj_head)\n",
        "            return loss_s + loss_o\n",
        "\n",
        "        return subj_logits, obj_logits\n",
        "\n",
        "model = CasRelFinBERT().cuda()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "# ===============================================\n",
        "# 5. TRAINING (precision oriented)\n",
        "# ===============================================\n",
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 3\n",
        "best_f1 = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(\n",
        "            batch[\"input_ids\"].cuda(),\n",
        "            batch[\"attention_mask\"].cuda(),\n",
        "            batch[\"subj_head\"].cuda(),\n",
        "            batch[\"obj_head\"].cuda()\n",
        "        )\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss = {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), OUTPUT_DIR / \"casrel_finbert.pt\")\n",
        "print(\"Saved model to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OXZV-t5MfW5I",
        "outputId": "7ed76dd5-17ea-49e2-b89c-da50a4938cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4114717341.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             }) + \"\\n\")\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mconvert_finred_to_casrel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_finred_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_TRAIN_TXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCASREL_TRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mconvert_finred_to_casrel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_finred_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_DEV_TXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCASREL_DEV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mconvert_finred_to_casrel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_finred_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_TEST_TXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCASREL_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4114717341.py\u001b[0m in \u001b[0;36mload_finred_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Full end-to-end CASREL + FinBERT single Colab cell ======\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Imports\n",
        "import json, os, tqdm, math, time\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# -------------------- USER PATHS (edit if necessary) --------------------\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset\")\n",
        "SRC_TRAIN_TXT = BASE / \"finred_train.txt\"\n",
        "SRC_DEV_TXT   = BASE / \"finred_dev.txt\"\n",
        "SRC_TEST_TXT  = BASE / \"finred_test.txt\"\n",
        "CASREL_TRAIN  = BASE / \"casrel_train.jsonl\"\n",
        "CASREL_DEV    = BASE / \"casrel_dev.jsonl\"\n",
        "CASREL_TEST   = BASE / \"casrel_test.jsonl\"\n",
        "OUTPUT_DIR = BASE / \"casrel_finbert_model_v3\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# -------------------- HYPERPARAMETERS (tweakable) --------------------\n",
        "MODEL_NAME = \"yiyanghkust/finbert-pretrain\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "SEED = 42\n",
        "SUBJECT_TH = 0.8   # higher threshold to improve precision\n",
        "OBJECT_TH  = 0.8  # idem\n",
        "RANDOM = random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# -------------------- Helpers: parse and convert FinRED -> CASREL jsonl --------------------\n",
        "def parse_finred_line(line):\n",
        "    parts = [p.strip() for p in line.strip().split(\"|\")]\n",
        "    if len(parts) == 0:\n",
        "        return None\n",
        "    text = parts[0]\n",
        "    triples = []\n",
        "    for p in parts[1:]:\n",
        "        if not p:\n",
        "            continue\n",
        "        fields = [x.strip() for x in p.split(\";\") if x.strip() != \"\"]\n",
        "        if len(fields) != 3:\n",
        "            continue\n",
        "        h,t,r = fields\n",
        "        triples.append((h,t,r))\n",
        "    return {\"text\": text, \"triples\": triples}\n",
        "\n",
        "def convert_txt_to_casrel_jsonl(src_path, out_path):\n",
        "    # Writes jsonl with {\"text\":..., \"spo_list\":[{\"subject\":..., \"predicate\":..., \"object\":...}, ...]}\n",
        "    if not src_path.exists():\n",
        "        print(f\"[convert] Source {src_path} not found; skipping.\")\n",
        "        return 0\n",
        "    n=0\n",
        "    with open(src_path, \"r\", encoding=\"utf-8\") as fr, open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for ln in fr:\n",
        "            if not ln.strip():\n",
        "                continue\n",
        "            parsed = parse_finred_line(ln)\n",
        "            if parsed is None:\n",
        "                continue\n",
        "            text = parsed[\"text\"]\n",
        "            spo_list = []\n",
        "            for (h,t,r) in parsed[\"triples\"]:\n",
        "                spo_list.append({\"subject\": h, \"predicate\": r, \"object\": t})\n",
        "            rec = {\"text\": text, \"spo_list\": spo_list}\n",
        "            fw.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            n += 1\n",
        "    print(f\"[convert] Wrote {n} records to {out_path}\")\n",
        "    return n\n",
        "\n",
        "# Convert if not present (safe to run even if present)\n",
        "convert_txt_to_casrel_jsonl(SRC_TRAIN_TXT, CASREL_TRAIN)\n",
        "convert_txt_to_casrel_jsonl(SRC_DEV_TXT, CASREL_DEV)\n",
        "convert_txt_to_casrel_jsonl(SRC_TEST_TXT, CASREL_TEST)\n",
        "\n",
        "# -------------------- Tokenizer and relation collection --------------------\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def collect_relations(jsonl_paths):\n",
        "    rels = set()\n",
        "    for p in jsonl_paths:\n",
        "        if not p.exists(): continue\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            for ln in f:\n",
        "                if not ln.strip(): continue\n",
        "                rec = json.loads(ln)\n",
        "                for spo in rec.get(\"spo_list\", []):\n",
        "                    pred = spo.get(\"predicate\")\n",
        "                    if pred:\n",
        "                        rels.add(pred)\n",
        "    rels = sorted(rels)\n",
        "    return rels\n",
        "\n",
        "relation_list = collect_relations([CASREL_TRAIN, CASREL_DEV, CASREL_TEST])\n",
        "if len(relation_list)==0:\n",
        "    print(\"Warning: no relation predicates found. Check source files.\")\n",
        "rel2id = {r:i for i,r in enumerate(relation_list)}\n",
        "id2rel = {i:r for r,i in rel2id.items()}\n",
        "num_rels = len(relation_list)\n",
        "print(\"Predicates found:\", num_rels, relation_list[:30])\n",
        "\n",
        "# -------------------- Build CASREL records (token-level labels) --------------------\n",
        "def build_casrel_records(jsonl_path, tokenizer, max_len=MAX_LEN):\n",
        "    records = []\n",
        "    if not jsonl_path.exists():\n",
        "        return records\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            if not ln.strip(): continue\n",
        "            rec = json.loads(ln)\n",
        "            text = rec.get(\"text\",\"\")\n",
        "            spo_list = rec.get(\"spo_list\", [])\n",
        "            # tokenize with offsets\n",
        "            enc = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "            offsets = enc[\"offset_mapping\"]\n",
        "            input_ids = enc[\"input_ids\"]\n",
        "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "            L = len(tokens)\n",
        "            # skip empty or too long\n",
        "            if L==0 or L>max_len:\n",
        "                continue\n",
        "            # init labels\n",
        "            sub_heads = [0]*L\n",
        "            sub_tails = [0]*L\n",
        "            obj_heads = [[0]*L for _ in range(num_rels)]\n",
        "            obj_tails = [[0]*L for _ in range(num_rels)]\n",
        "            any_spo = False\n",
        "            for spo in spo_list:\n",
        "                subj = spo.get(\"subject\",\"\")\n",
        "                obj = spo.get(\"object\",\"\")\n",
        "                pred = spo.get(\"predicate\",\"\")\n",
        "                if pred not in rel2id:\n",
        "                    continue\n",
        "                rid = rel2id[pred]\n",
        "                # find char positions (first match)\n",
        "                s_pos = text.lower().find(subj.lower())\n",
        "                o_pos = text.lower().find(obj.lower())\n",
        "                if s_pos == -1 or o_pos == -1:\n",
        "                    # skip mapping failure\n",
        "                    continue\n",
        "                s_end = s_pos + len(subj)\n",
        "                o_end = o_pos + len(obj)\n",
        "                # map to token indices\n",
        "                s_tok = s_tok_end = None\n",
        "                for i,(a,b) in enumerate(offsets):\n",
        "                    if a <= s_pos < b: s_tok = i\n",
        "                    if a < s_end <= b: s_tok_end = i\n",
        "                o_tok = o_tok_end = None\n",
        "                for i,(a,b) in enumerate(offsets):\n",
        "                    if a <= o_pos < b: o_tok = i\n",
        "                    if a < o_end <= b: o_tok_end = i\n",
        "                if s_tok is None or s_tok_end is None or o_tok is None or o_tok_end is None:\n",
        "                    continue\n",
        "                sub_heads[s_tok] = 1\n",
        "                sub_tails[s_tok_end] = 1\n",
        "                obj_heads[rid][o_tok] = 1\n",
        "                obj_tails[rid][o_tok_end] = 1\n",
        "                any_spo = True\n",
        "            # Keep record even if no SPO mapped (negative supervision helps)\n",
        "            records.append({\n",
        "                \"text\": text,\n",
        "                \"tokens\": tokens,\n",
        "                \"input_ids\": input_ids,\n",
        "                \"offsets\": offsets,\n",
        "                \"sub_heads\": sub_heads,\n",
        "                \"sub_tails\": sub_tails,\n",
        "                \"obj_heads\": obj_heads,\n",
        "                \"obj_tails\": obj_tails\n",
        "            })\n",
        "    return records\n",
        "\n",
        "train_records = build_casrel_records(CASREL_TRAIN, tokenizer, MAX_LEN)\n",
        "dev_records   = build_casrel_records(CASREL_DEV, tokenizer, MAX_LEN)\n",
        "test_records  = build_casrel_records(CASREL_TEST, tokenizer, MAX_LEN)\n",
        "print(f\"Records: train {len(train_records)} dev {len(dev_records)} test {len(test_records)}\")\n",
        "\n",
        "# -------------------- Compute pos-weights to mitigate imbalance --------------------\n",
        "def compute_pos_weights(records):\n",
        "    total_tokens = 0\n",
        "    s_heads = s_tails = 0\n",
        "    obj_head_counts = [0]*num_rels\n",
        "    obj_tail_counts = [0]*num_rels\n",
        "    for r in records:\n",
        "        L = len(r[\"sub_heads\"])\n",
        "        total_tokens += L\n",
        "        s_heads += sum(r[\"sub_heads\"])\n",
        "        s_tails += sum(r[\"sub_tails\"])\n",
        "        for rid in range(num_rels):\n",
        "            obj_head_counts[rid] += sum(r[\"obj_heads\"][rid])\n",
        "            obj_tail_counts[rid] += sum(r[\"obj_tails\"][rid])\n",
        "    # compute pos weights (neg/pos) but clamp to reasonable range to avoid exploding weights\n",
        "    def safe_weight(pos, total):\n",
        "        neg = max(total - pos, 0)\n",
        "        pos = max(pos, 1e-6)\n",
        "        w = neg / pos\n",
        "        # clamp\n",
        "        return float(min(max(w, 1.0), 200.0))\n",
        "    s_head_w = safe_weight(s_heads, total_tokens)\n",
        "    s_tail_w = safe_weight(s_tails, total_tokens)\n",
        "    obj_head_w = [safe_weight(obj_head_counts[i], total_tokens) for i in range(num_rels)]\n",
        "    obj_tail_w = [safe_weight(obj_tail_counts[i], total_tokens) for i in range(num_rels)]\n",
        "    return {\"s_head\": s_head_w, \"s_tail\": s_tail_w, \"obj_head\": obj_head_w, \"obj_tail\": obj_tail_w}\n",
        "\n",
        "weights = compute_pos_weights(train_records)\n",
        "print(\"Pos-weights sample:\", {k: weights[k] if not isinstance(weights[k], list) else f\"[{len(weights[k])}]\" for k in weights})\n",
        "\n",
        "# -------------------- Dataset and collate_fn (robust padding) --------------------\n",
        "class CASRELDataset(Dataset):\n",
        "    def __init__(self, records):\n",
        "        self.records = records\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        input_ids = torch.tensor(r[\"input_ids\"], dtype=torch.long)\n",
        "        attn = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        # convert lists -> tensors\n",
        "        sub_head = torch.tensor(r[\"sub_heads\"], dtype=torch.float)     # L\n",
        "        sub_tail = torch.tensor(r[\"sub_tails\"], dtype=torch.float)     # L\n",
        "        obj_head = torch.tensor(r[\"obj_heads\"], dtype=torch.float)     # R x L\n",
        "        obj_tail = torch.tensor(r[\"obj_tails\"], dtype=torch.float)     # R x L\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attn,\n",
        "            \"sub_head\": sub_head,\n",
        "            \"sub_tail\": sub_tail,\n",
        "            \"obj_head\": obj_head,\n",
        "            \"obj_tail\": obj_tail\n",
        "        }\n",
        "\n",
        "def casrel_collate(batch):\n",
        "    # Determine max length in batch\n",
        "    max_len = max([b[\"input_ids\"].size(0) for b in batch])\n",
        "    R = num_rels\n",
        "    input_ids_p, attn_p = [], []\n",
        "    sh_p, st_p = [], []\n",
        "    oh_p, ot_p = [], []\n",
        "    for b in batch:\n",
        "        L = b[\"input_ids\"].size(0)\n",
        "        pad_len = max_len - L\n",
        "        input_ids_p.append(torch.cat([b[\"input_ids\"], torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)]))\n",
        "        attn_p.append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        sh_p.append(torch.cat([b[\"sub_head\"], torch.zeros(pad_len)]))\n",
        "        st_p.append(torch.cat([b[\"sub_tail\"], torch.zeros(pad_len)]))\n",
        "        # obj arrays: R x L -> pad each row\n",
        "        oh = b[\"obj_head\"]\n",
        "        ot = b[\"obj_tail\"]\n",
        "        # if OH/OT shape mismatch (robustness), handle\n",
        "        if oh.dim()==1:\n",
        "            # rare; convert to (R, Lcur) assuming R==1\n",
        "            oh = oh.unsqueeze(0)\n",
        "        if oh.size(0) != R:\n",
        "            # pad or tile rows to R (rare); to be safe, create zeros\n",
        "            oh = torch.zeros((R, oh.size(1)))\n",
        "            ot = torch.zeros((R, ot.size(1)))\n",
        "        oh_pad = torch.cat([oh, torch.zeros((R, pad_len))], dim=1)\n",
        "        ot_pad = torch.cat([ot, torch.zeros((R, pad_len))], dim=1)\n",
        "        oh_p.append(oh_pad)\n",
        "        ot_p.append(ot_pad)\n",
        "    batch_out = {\n",
        "        \"input_ids\": torch.stack(input_ids_p),\n",
        "        \"attention_mask\": torch.stack(attn_p),\n",
        "        \"sub_head\": torch.stack(sh_p),\n",
        "        \"sub_tail\": torch.stack(st_p),\n",
        "        \"obj_head\": torch.stack(oh_p),  # B x R x L\n",
        "        \"obj_tail\": torch.stack(ot_p)\n",
        "    }\n",
        "    return batch_out\n",
        "\n",
        "train_ds = CASRELDataset(train_records)\n",
        "dev_ds = CASRELDataset(dev_records)\n",
        "test_ds = CASRELDataset(test_records)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=casrel_collate)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "print(\"DataLoaders ready. Examples:\", len(train_ds), len(dev_ds), len(test_ds))\n",
        "\n",
        "# -------------------- Model (CASREL-like) --------------------\n",
        "class CASRELModel(nn.Module):\n",
        "    def __init__(self, bert_name, num_rels):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        H = self.bert.config.hidden_size\n",
        "        self.sub_head_proj = nn.Linear(H, 1)\n",
        "        self.sub_tail_proj = nn.Linear(H, 1)\n",
        "        # object predictor conditioned on subject: concat token repr + subject repr -> project -> per-rel heads/tails\n",
        "        self.obj_fc = nn.Linear(H*2, H)\n",
        "        self.obj_head_proj = nn.Linear(H, num_rels)\n",
        "        self.obj_tail_proj = nn.Linear(H, num_rels)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_mask, subject_span=None):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        seq_out = bert_out.last_hidden_state   # B x L x H\n",
        "        sub_head_logits = self.sub_head_proj(seq_out).squeeze(-1)  # B x L\n",
        "        sub_tail_logits = self.sub_tail_proj(seq_out).squeeze(-1)\n",
        "        subj_cond_obj_head = None\n",
        "        subj_cond_obj_tail = None\n",
        "        if subject_span is not None:\n",
        "            # subject_span: B x 2 or list of spans length B\n",
        "            # Accept a tensor (B,2)\n",
        "            if isinstance(subject_span, torch.Tensor):\n",
        "                spans = subject_span\n",
        "            else:\n",
        "                spans = torch.tensor(subject_span, dtype=torch.long, device=seq_out.device)\n",
        "            B, L, H = seq_out.size()\n",
        "            spans = spans.clamp(0, L-1)\n",
        "            subj_repr = []\n",
        "            for i in range(B):\n",
        "                s = spans[i,0].item()\n",
        "                e = spans[i,1].item()\n",
        "                if e < s: e = s\n",
        "                vec = seq_out[i, s:e+1, :].mean(dim=0)\n",
        "                subj_repr.append(vec)\n",
        "            subj_repr = torch.stack(subj_repr, dim=0)  # B x H\n",
        "            subj_exp = subj_repr.unsqueeze(1).expand(-1, seq_out.size(1), -1)  # B x L x H\n",
        "            concat = torch.cat([seq_out, subj_exp], dim=-1)  # B x L x 2H\n",
        "            h = self.relu(self.obj_fc(concat))  # B x L x H\n",
        "            oh = self.obj_head_proj(h)  # B x L x R\n",
        "            ot = self.obj_tail_proj(h)  # B x L x R\n",
        "            # transpose to B x R x L for easier BCE with gold (R dimension first)\n",
        "            subj_cond_obj_head = oh.permute(0,2,1)\n",
        "            subj_cond_obj_tail = ot.permute(0,2,1)\n",
        "        return sub_head_logits, sub_tail_logits, subj_cond_obj_head, subj_cond_obj_tail\n",
        "\n",
        "model = CASRELModel(MODEL_NAME, num_rels)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# -------------------- Losses and optimizer (with pos_weight) --------------------\n",
        "s_head_pw = torch.tensor(weights[\"s_head\"], dtype=torch.float, device=DEVICE)\n",
        "s_tail_pw = torch.tensor(weights[\"s_tail\"], dtype=torch.float, device=DEVICE)\n",
        "sub_head_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_head_pw)\n",
        "sub_tail_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_tail_pw)\n",
        "\n",
        "# For objects we'll compute BCE per relation with per-relation pos_weight inside loop (safe & explicit)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = max(1, len(train_loader) * EPOCHS)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1,int(0.1*total_steps)), num_training_steps=total_steps)\n",
        "\n",
        "# -------------------- Utility decode function for triple-level metrics --------------------\n",
        "def decode_triples_from_batch(input_ids, sub_h_logits, sub_t_logits, model, attention_mask, subject_thresh=SUBJECT_TH, object_thresh=OBJECT_TH):\n",
        "    # sub logits: B x L, model will be used to compute conditioned object logits for each subject found\n",
        "    B, L = sub_h_logits.size()\n",
        "    all_preds = []  # list of list-of-triples per sample\n",
        "    with torch.no_grad():\n",
        "        for i in range(B):\n",
        "            sh = (torch.sigmoid(sub_h_logits[i]) > subject_thresh).cpu().numpy().astype(int)\n",
        "            st = (torch.sigmoid(sub_t_logits[i]) > subject_thresh).cpu().numpy().astype(int)\n",
        "            subj_spans = []\n",
        "            for p in range(L):\n",
        "                if sh[p]==1:\n",
        "                    q=None\n",
        "                    for k in range(p, L):\n",
        "                        if st[k]==1:\n",
        "                            q=k; break\n",
        "                    if q is not None:\n",
        "                        subj_spans.append((p,q))\n",
        "            preds_for_sample = []\n",
        "            # for each subject span, obtain object logits\n",
        "            for (s_start, s_end) in subj_spans:\n",
        "                subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long, device=DEVICE)\n",
        "                # use model to condition; note input must be 1 x L\n",
        "                input_i = input_ids[i:i+1,:]\n",
        "                attn_i = attention_mask[i:i+1,:]\n",
        "                _, _, obj_head_logits, obj_tail_logits = model(input_i, attn_i, subj_span_tensor)\n",
        "                if obj_head_logits is None:\n",
        "                    continue\n",
        "                oh = torch.sigmoid(obj_head_logits.squeeze(0)).cpu().numpy()  # R x L\n",
        "                ot = torch.sigmoid(obj_tail_logits.squeeze(0)).cpu().numpy()  # R x L\n",
        "                for rid in range(num_rels):\n",
        "                    for a in range(L):\n",
        "                        if oh[rid,a] > object_thresh:\n",
        "                            b = None\n",
        "                            for k in range(a, L):\n",
        "                                if ot[rid,k] > object_thresh:\n",
        "                                    b = k; break\n",
        "                            if b is not None:\n",
        "                                preds_for_sample.append(((s_start, s_end),(a,b), id2rel[rid]))\n",
        "            all_preds.append(preds_for_sample)\n",
        "    return all_preds\n",
        "\n",
        "# -------------------- Train & Eval (cascade-style) --------------------\n",
        "def train_and_evaluate(model, train_loader, dev_loader, test_loader, epochs=EPOCHS):\n",
        "    history = {\"train_loss\": [], \"dev_loss\": []}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        t0 = time.time()\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f\"Train epoch {epoch}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)           # B x L\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            B, L = input_ids.shape\n",
        "            sub_head_gold = batch[\"sub_head\"].to(DEVICE)       # B x L\n",
        "            sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "            obj_head_gold = batch[\"obj_head\"].to(DEVICE)       # B x R x L\n",
        "            obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            # subject logits\n",
        "            sub_head_logits, sub_tail_logits, _, _ = model(input_ids, attn, subject_span=None)\n",
        "            # subject losses (BCE with scalar pos_weight)\n",
        "            loss_sh = sub_head_loss_fn(sub_head_logits, sub_head_gold)\n",
        "            loss_st = sub_tail_loss_fn(sub_tail_logits, sub_tail_gold)\n",
        "            # object loss: iterate gold subject spans per sample\n",
        "            loss_obj_total = 0.0\n",
        "            obj_count = 0\n",
        "            for i in range(B):\n",
        "                shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                subj_spans = []\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        q=None\n",
        "                        for k in range(p,L):\n",
        "                            if stg[k]==1:\n",
        "                                q=k; break\n",
        "                        if q is not None:\n",
        "                            subj_spans.append((p,q))\n",
        "                if len(subj_spans)==0:\n",
        "                    continue\n",
        "                for (s_start, s_end) in subj_spans:\n",
        "                    subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                    _, _, obj_head_logits, obj_tail_logits = model(input_ids[i:i+1,:], attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                    logits_oh = obj_head_logits.squeeze(0)  # R x L\n",
        "                    logits_ot = obj_tail_logits.squeeze(0)\n",
        "                    # compute per-relation BCE with pos_weight for that relation\n",
        "                    loss_rel = 0.0\n",
        "                    for rid in range(num_rels):\n",
        "                        pos_w_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                        pos_w_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                        loss_h = nn.BCEWithLogitsLoss(pos_weight=pos_w_h)(logits_oh[rid], obj_head_gold[i,rid])\n",
        "                        loss_t = nn.BCEWithLogitsLoss(pos_weight=pos_w_t)(logits_ot[rid], obj_tail_gold[i,rid])\n",
        "                        loss_rel += (loss_h + loss_t) / 2.0\n",
        "                    loss_obj_total += loss_rel / max(1, num_rels)\n",
        "                    obj_count += 1\n",
        "            if obj_count>0:\n",
        "                loss_obj_total = loss_obj_total / obj_count\n",
        "            else:\n",
        "                loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "            loss = loss_sh + loss_st + loss_obj_total\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        t1 = time.time()\n",
        "        # Validation: compute loss and triple-level metrics\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        preds_all = []\n",
        "        gold_all = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm.tqdm(dev_loader, desc=\"Dev evaluation\"):\n",
        "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "                attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "                B,L = input_ids.shape\n",
        "                sub_head_gold = batch[\"sub_head\"].to(DEVICE)\n",
        "                sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "                obj_head_gold = batch[\"obj_head\"].to(DEVICE)\n",
        "                obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "                sub_head_logits, sub_tail_logits, _, _ = model(input_ids, attn, subject_span=None)\n",
        "                # subject loss\n",
        "                loss_sh = sub_head_loss_fn(sub_head_logits, sub_head_gold)\n",
        "                loss_st = sub_tail_loss_fn(sub_tail_logits, sub_tail_gold)\n",
        "                # object loss computed same as training (using gold subjects)\n",
        "                loss_obj_total = 0.0\n",
        "                obj_count = 0\n",
        "                for i in range(B):\n",
        "                    shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                    stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                    subj_spans = []\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            q=None\n",
        "                            for k in range(p,L):\n",
        "                                if stg[k]==1:\n",
        "                                    q=k; break\n",
        "                            if q is not None:\n",
        "                                subj_spans.append((p,q))\n",
        "                    if len(subj_spans)==0: continue\n",
        "                    for (s_start, s_end) in subj_spans:\n",
        "                        subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                        _, _, obj_head_logits, obj_tail_logits = model(input_ids[i:i+1,:], attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                        logits_oh = obj_head_logits.squeeze(0)\n",
        "                        logits_ot = obj_tail_logits.squeeze(0)\n",
        "                        loss_rel = 0.0\n",
        "                        for rid in range(num_rels):\n",
        "                            pos_w_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                            pos_w_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                            loss_h = nn.BCEWithLogitsLoss(pos_weight=pos_w_h)(logits_oh[rid], obj_head_gold[i,rid])\n",
        "                            loss_t = nn.BCEWithLogitsLoss(pos_weight=pos_w_t)(logits_ot[rid], obj_tail_gold[i,rid])\n",
        "                            loss_rel += (loss_h + loss_t) / 2.0\n",
        "                        loss_obj_total += loss_rel / max(1, num_rels)\n",
        "                        obj_count += 1\n",
        "                if obj_count>0:\n",
        "                    loss_obj_total = loss_obj_total / obj_count\n",
        "                else:\n",
        "                    loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "                batch_loss = loss_sh + loss_st + loss_obj_total\n",
        "                val_loss += batch_loss.item()\n",
        "                # decode predictions for triples using thresholds\n",
        "                preds_batch = decode_triples_from_batch(input_ids, sub_head_logits, sub_tail_logits, model, attn, SUBJECT_TH, OBJECT_TH)\n",
        "                # gold triples from label matrices\n",
        "                for i in range(B):\n",
        "                    # collect gold triples for sample i\n",
        "                    gold_sample = []\n",
        "                    shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                    stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                    gold_subs = []\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            q=None\n",
        "                            for k in range(p,L):\n",
        "                                if stg[k]==1:\n",
        "                                    q=k; break\n",
        "                            if q is not None:\n",
        "                                gold_subs.append((p,q))\n",
        "                    for (s_start, s_end) in gold_subs:\n",
        "                        for rid in range(num_rels):\n",
        "                            ohg = obj_head_gold[i,rid].cpu().numpy().astype(int)\n",
        "                            otg = obj_tail_gold[i,rid].cpu().numpy().astype(int)\n",
        "                            for a in range(L):\n",
        "                                if ohg[a]==1:\n",
        "                                    b=None\n",
        "                                    for k in range(a,L):\n",
        "                                        if otg[k]==1:\n",
        "                                            b=k; break\n",
        "                                    if b is not None:\n",
        "                                        gold_sample.append(((s_start,s_end),(a,b),id2rel[rid]))\n",
        "                    gold_all.extend(gold_sample)\n",
        "                # extend preds list\n",
        "                for sample_preds in preds_batch:\n",
        "                    preds_all.extend(sample_preds)\n",
        "        avg_dev_loss = val_loss / max(1, len(dev_loader))\n",
        "        history[\"dev_loss\"].append(avg_dev_loss)\n",
        "        # compute triple-level micro metrics\n",
        "        def to_set(triples_list):\n",
        "            s = set()\n",
        "            for t in triples_list:\n",
        "                (ss,se),(os,oe),pred = t\n",
        "                s.add((ss,se,os,oe,pred))\n",
        "            return s\n",
        "        gold_set = to_set(gold_all)\n",
        "        pred_set = to_set(preds_all)\n",
        "        tp = len(pred_set & gold_set)\n",
        "        fp = len(pred_set - gold_set)\n",
        "        fn = len(gold_set - pred_set)\n",
        "        prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "        rec = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "        f1  = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "        print(f\"\\nEpoch {epoch} summary: train_loss={avg_train_loss:.6f} dev_loss={avg_dev_loss:.6f} triples P/R/F={prec:.4f}/{rec:.4f}/{f1:.4f} time={t1-t0:.1f}s\")\n",
        "    return model, history\n",
        "\n",
        "# Run training + evaluation\n",
        "model, history = train_and_evaluate(model, train_loader, dev_loader, test_loader, epochs=EPOCHS)\n",
        "\n",
        "# -------------------- Final evaluation on dev & test --------------------\n",
        "def final_eval(model, loader):\n",
        "    model.eval()\n",
        "    preds_all = []\n",
        "    gold_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(loader, desc=\"Final Eval\"):\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            sub_head_logits, sub_tail_logits, _, _ = model(input_ids, attn, subject_span=None)\n",
        "            preds_batch = decode_triples_from_batch(input_ids, sub_head_logits, sub_tail_logits, model, attn, SUBJECT_TH, OBJECT_TH)\n",
        "            # gold triples\n",
        "            B,L = input_ids.shape\n",
        "            for i in range(B):\n",
        "                gold_sample = []\n",
        "                shg = batch[\"sub_head\"][i].cpu().numpy().astype(int)\n",
        "                stg = batch[\"sub_tail\"][i].cpu().numpy().astype(int)\n",
        "                gold_subs = []\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        q=None\n",
        "                        for k in range(p,L):\n",
        "                            if stg[k]==1:\n",
        "                                q=k; break\n",
        "                        if q is not None: gold_subs.append((p,q))\n",
        "                for (s_start,s_end) in gold_subs:\n",
        "                    for rid in range(num_rels):\n",
        "                        ohg = batch[\"obj_head\"][i,rid].cpu().numpy().astype(int)\n",
        "                        otg = batch[\"obj_tail\"][i,rid].cpu().numpy().astype(int)\n",
        "                        for a in range(L):\n",
        "                            if ohg[a]==1:\n",
        "                                b=None\n",
        "                                for k in range(a,L):\n",
        "                                    if otg[k]==1:\n",
        "                                        b=k; break\n",
        "                                if b is not None:\n",
        "                                    gold_sample.append(((s_start,s_end),(a,b), id2rel[rid]))\n",
        "                gold_all.extend(gold_sample)\n",
        "            for sp in preds_batch:\n",
        "                preds_all.extend(sp)\n",
        "    # compute metrics\n",
        "    def to_set(triples_list):\n",
        "        s=set()\n",
        "        for t in triples_list:\n",
        "            (ss,se),(os,oe),pred = t\n",
        "            s.add((ss,se,os,oe,pred))\n",
        "        return s\n",
        "    gset = to_set(gold_all)\n",
        "    pset = to_set(preds_all)\n",
        "    tp = len(pset & gset)\n",
        "    fp = len(pset - gset)\n",
        "    fn = len(gset - pset)\n",
        "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "    rec = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "    f1 = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn}\n",
        "\n",
        "dev_metrics = final_eval(model, dev_loader)\n",
        "test_metrics = final_eval(model, test_loader)\n",
        "print(\"\\n=== FINAL METRICS ===\")\n",
        "print(\"DEV triple P/R/F: {:.4f} / {:.4f} / {:.4f}\".format(dev_metrics[\"precision\"], dev_metrics[\"recall\"], dev_metrics[\"f1\"]))\n",
        "print(\"TEST triple P/R/F: {:.4f} / {:.4f} / {:.4f}\".format(test_metrics[\"precision\"], test_metrics[\"recall\"], test_metrics[\"f1\"]))\n",
        "\n",
        "# -------------------- Save model, tokenizer and metrics summary --------------------\n",
        "torch.save(model.state_dict(), OUTPUT_DIR / \"casrel_finbert_state_dict.pt\")\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "\n",
        "metrics_summary = {\n",
        "    \"relation_list\": relation_list,\n",
        "    \"num_rels\": num_rels,\n",
        "    \"train_records\": len(train_records),\n",
        "    \"dev_records\": len(dev_records),\n",
        "    \"test_records\": len(test_records),\n",
        "    \"weights\": weights,\n",
        "    \"history\": history,\n",
        "    \"dev_metrics\": dev_metrics,\n",
        "    \"test_metrics\": test_metrics\n",
        "}\n",
        "with open(OUTPUT_DIR / \"casrel_metrics_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics_summary, f, indent=2)\n",
        "\n",
        "print(\"Saved model & metrics to\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGD3wgYDw_-_",
        "outputId": "517476d9-0404-497b-a373-bffe25f28e2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "[convert] Wrote 5700 records to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_train.jsonl\n",
            "[convert] Wrote 1007 records to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_dev.jsonl\n",
            "[convert] Wrote 1068 records to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_test.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicates found: 29 ['brand', 'business_division', 'chairperson', 'chief_executive_officer', 'creator', 'currency', 'developer', 'director_/_manager', 'distributed_by', 'distribution_format', 'employer', 'founded_by', 'headquarters_location', 'industry', 'legal_form', 'location_of_formation', 'manufacturer', 'member_of', 'operator', 'original_broadcaster', 'owned_by', 'owner_of', 'parent_organization', 'platform', 'position_held', 'product_or_material_produced', 'publisher', 'stock_exchange', 'subsidiary']\n",
            "Records: train 5582 dev 971 test 1034\n",
            "Pos-weights sample: {'s_head': 37.49537774944214, 's_tail': 37.05262328659209, 'obj_head': '[29]', 'obj_tail': '[29]'}\n",
            "DataLoaders ready. Examples: 5582 971 1034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 1/3: 100%|██████████| 698/698 [07:10<00:00,  1.62it/s, loss=1.0623]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:54<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 summary: train_loss=1.061420 dev_loss=0.583323 triples P/R/F=0.0439/0.4809/0.0805 time=430.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 2/3: 100%|██████████| 698/698 [07:04<00:00,  1.65it/s, loss=0.2302]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:47<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 summary: train_loss=0.411317 dev_loss=0.588394 triples P/R/F=0.0558/0.5599/0.1016 time=424.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 3/3: 100%|██████████| 698/698 [07:01<00:00,  1.66it/s, loss=0.2045]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:45<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 summary: train_loss=0.256114 dev_loss=0.718115 triples P/R/F=0.0719/0.5554/0.1273 time=421.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Final Eval: 100%|██████████| 122/122 [00:19<00:00,  6.18it/s]\n",
            "Final Eval: 100%|██████████| 130/130 [00:22<00:00,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL METRICS ===\n",
            "DEV triple P/R/F: 0.0719 / 0.5554 / 0.1273\n",
            "TEST triple P/R/F: 0.0630 / 0.5108 / 0.1122\n",
            "Saved model & metrics to /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_finbert_model_v3\n"
          ]
        }
      ]
    }
  ]
}