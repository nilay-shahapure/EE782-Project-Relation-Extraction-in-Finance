{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DbudNDhE8UR",
        "outputId": "a1a2fa16-a121-4843-cbd6-039c3a4f1a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "[convert] wrote 5700 records → /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_train.jsonl\n",
            "[convert] wrote 1007 records → /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_dev.jsonl\n",
            "[convert] wrote 1068 records → /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_test.jsonl\n",
            "Relations: 29 ['brand', 'business_division', 'chairperson', 'chief_executive_officer', 'creator', 'currency', 'developer', 'director_/_manager', 'distributed_by', 'distribution_format', 'employer', 'founded_by', 'headquarters_location', 'industry', 'legal_form', 'location_of_formation', 'manufacturer', 'member_of', 'operator', 'original_broadcaster', 'owned_by', 'owner_of', 'parent_organization', 'platform', 'position_held', 'product_or_material_produced', 'publisher', 'stock_exchange', 'subsidiary']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records: 5585 972 1035\n",
            "Weights computed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4: 100%|██████████| 699/699 [06:48<00:00,  1.71it/s, loss=2.9054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 → Train 9.1928 | Dev 3.5151\n",
            "Triple P/R/F = 0.1046/0.2850/0.1530\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4: 100%|██████████| 699/699 [06:46<00:00,  1.72it/s, loss=1.7205]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 → Train 2.4975 | Dev 2.4536\n",
            "Triple P/R/F = 0.1045/0.5421/0.1752\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4: 100%|██████████| 699/699 [06:42<00:00,  1.74it/s, loss=2.0391]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 → Train 1.4846 | Dev 2.5801\n",
            "Triple P/R/F = 0.1202/0.5662/0.1983\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4: 100%|██████████| 699/699 [06:42<00:00,  1.74it/s, loss=10.5255]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 → Train 1.0744 | Dev 2.9456\n",
            "Triple P/R/F = 0.1388/0.5560/0.2222\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 122/122 [00:21<00:00,  5.60it/s]\n",
            "100%|██████████| 130/130 [00:23<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL METRICS (BERT-BASE + CASREL) ===\n",
            "DEV →  P/R/F = {'precision': 0.13883617963314357, 'recall': 0.5560481317289424, 'f1': 0.22219410350499808, 'tp': 878, 'fp': 5446, 'fn': 701}\n",
            "TEST → P/R/F = {'precision': 0.13633139452404597, 'recall': 0.5983810709838107, 'f1': 0.22206816868861928, 'tp': 961, 'fp': 6088, 'fn': 645}\n",
            "Saved model & metrics → /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_bertbase_model\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "#       CASREL + BERT-BASE COMPLETE TRAINING PIPELINE\n",
        "#               (Fully Compatible with FinRED)\n",
        "# ================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# -------------------- Imports --------------------\n",
        "import json, os, tqdm, math, time, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# -------------------- USER PATHS --------------------\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset\")\n",
        "\n",
        "SRC_TRAIN_TXT = BASE / \"finred_train.txt\"\n",
        "SRC_DEV_TXT   = BASE / \"finred_dev.txt\"\n",
        "SRC_TEST_TXT  = BASE / \"finred_test.txt\"\n",
        "\n",
        "CASREL_TRAIN  = BASE / \"casrel_train.jsonl\"\n",
        "CASREL_DEV    = BASE / \"casrel_dev.jsonl\"\n",
        "CASREL_TEST   = BASE / \"casrel_test.jsonl\"\n",
        "\n",
        "OUTPUT_DIR = BASE / \"casrel_bertbase_model\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# -------------------- HYPERPARAMETERS --------------------\n",
        "MODEL_NAME = \"bert-base-uncased\"      # <<<<<<< CHANGED HERE\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4                             # BERT needs slightly more training\n",
        "LR = 3e-5                               # Slightly larger LR helps BERT base\n",
        "SEED = 42\n",
        "\n",
        "SUBJECT_TH = 0.9\n",
        "OBJECT_TH  = 0.9\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ===========================================================\n",
        "# 1) Convert FinRED text → CASREL JSONL\n",
        "# ===========================================================\n",
        "\n",
        "def parse_finred_line(line):\n",
        "    parts = [p.strip() for p in line.strip().split(\"|\")]\n",
        "    if len(parts) == 0:\n",
        "        return None\n",
        "    text = parts[0]\n",
        "    triples = []\n",
        "    for p in parts[1:]:\n",
        "        if not p:\n",
        "            continue\n",
        "        fields = [x.strip() for x in p.split(\";\") if x.strip() != \"\"]\n",
        "        if len(fields) != 3:\n",
        "            continue\n",
        "        h,t,r = fields\n",
        "        triples.append((h,t,r))\n",
        "    return {\"text\": text, \"triples\": triples}\n",
        "\n",
        "def convert_txt_to_casrel_jsonl(src_path, out_path):\n",
        "    if not src_path.exists():\n",
        "        print(f\"[convert] Missing source file {src_path}\")\n",
        "        return 0\n",
        "    n=0\n",
        "    with open(src_path, \"r\", encoding=\"utf-8\") as fr, open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for ln in fr:\n",
        "            if not ln.strip():\n",
        "                continue\n",
        "            parsed = parse_finred_line(ln)\n",
        "            if parsed is None:\n",
        "                continue\n",
        "            text = parsed[\"text\"]\n",
        "            spo_list = []\n",
        "            for (h,t,r) in parsed[\"triples\"]:\n",
        "                spo_list.append({\"subject\": h, \"predicate\": r, \"object\": t})\n",
        "            rec = {\"text\": text, \"spo_list\": spo_list}\n",
        "            fw.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            n+=1\n",
        "    print(f\"[convert] wrote {n} records → {out_path}\")\n",
        "    return n\n",
        "\n",
        "convert_txt_to_casrel_jsonl(SRC_TRAIN_TXT, CASREL_TRAIN)\n",
        "convert_txt_to_casrel_jsonl(SRC_DEV_TXT,   CASREL_DEV)\n",
        "convert_txt_to_casrel_jsonl(SRC_TEST_TXT,  CASREL_TEST)\n",
        "\n",
        "# ===========================================================\n",
        "# 2) Collect relations\n",
        "# ===========================================================\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def collect_relations(jsonl_paths):\n",
        "    rels = set()\n",
        "    for p in jsonl_paths:\n",
        "        if not p.exists():\n",
        "            continue\n",
        "        for ln in open(p, \"r\", encoding=\"utf-8\"):\n",
        "            if not ln.strip(): continue\n",
        "            rec = json.loads(ln)\n",
        "            for spo in rec.get(\"spo_list\", []):\n",
        "                rels.add(spo[\"predicate\"])\n",
        "    return sorted(list(rels))\n",
        "\n",
        "relation_list = collect_relations([CASREL_TRAIN, CASREL_DEV, CASREL_TEST])\n",
        "rel2id = {r:i for i,r in enumerate(relation_list)}\n",
        "id2rel = {i:r for r,i in rel2id.items()}\n",
        "num_rels = len(relation_list)\n",
        "\n",
        "print(\"Relations:\", num_rels, relation_list)\n",
        "\n",
        "# ===========================================================\n",
        "# 3) Build token-level CASREL labels\n",
        "# ===========================================================\n",
        "\n",
        "def build_casrel_records(path, tokenizer, max_len=128):\n",
        "    records=[]\n",
        "    if not path.exists():\n",
        "        return records\n",
        "    for ln in open(path, \"r\", encoding=\"utf-8\"):\n",
        "        if not ln.strip(): continue\n",
        "        rec = json.loads(ln)\n",
        "        text = rec[\"text\"]\n",
        "        spo_list = rec[\"spo_list\"]\n",
        "\n",
        "        enc = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "        offsets = enc[\"offset_mapping\"]\n",
        "        input_ids = enc[\"input_ids\"]\n",
        "        L = len(input_ids)\n",
        "        if L==0 or L>max_len:\n",
        "            continue\n",
        "\n",
        "        sub_heads = [0]*L\n",
        "        sub_tails = [0]*L\n",
        "        obj_heads = [[0]*L for _ in range(num_rels)]\n",
        "        obj_tails = [[0]*L for _ in range(num_rels)]\n",
        "\n",
        "        for spo in spo_list:\n",
        "            subj = spo[\"subject\"]\n",
        "            obj  = spo[\"object\"]\n",
        "            pred = spo[\"predicate\"]\n",
        "            if pred not in rel2id:\n",
        "                continue\n",
        "            rid = rel2id[pred]\n",
        "\n",
        "            s_pos = text.lower().find(subj.lower())\n",
        "            o_pos = text.lower().find(obj.lower())\n",
        "            if s_pos==-1 or o_pos==-1:\n",
        "                continue\n",
        "\n",
        "            s_end = s_pos + len(subj)\n",
        "            o_end = o_pos + len(obj)\n",
        "\n",
        "            s_tok = s_tok_end = None\n",
        "            o_tok = o_tok_end = None\n",
        "\n",
        "            for i,(a,b) in enumerate(offsets):\n",
        "                if a<=s_pos<b: s_tok=i\n",
        "                if a<s_end<=b: s_tok_end=i\n",
        "                if a<=o_pos<b: o_tok=i\n",
        "                if a<o_end<=b: o_tok_end=i\n",
        "\n",
        "            if None in [s_tok,s_tok_end,o_tok,o_tok_end]:\n",
        "                continue\n",
        "\n",
        "            sub_heads[s_tok]=1\n",
        "            sub_tails[s_tok_end]=1\n",
        "            obj_heads[rid][o_tok]=1\n",
        "            obj_tails[rid][o_tok_end]=1\n",
        "\n",
        "        records.append({\n",
        "            \"text\":text,\n",
        "            \"input_ids\":input_ids,\n",
        "            \"offsets\":offsets,\n",
        "            \"sub_heads\":sub_heads,\n",
        "            \"sub_tails\":sub_tails,\n",
        "            \"obj_heads\":obj_heads,\n",
        "            \"obj_tails\":obj_tails\n",
        "        })\n",
        "    return records\n",
        "\n",
        "train_records = build_casrel_records(CASREL_TRAIN, tokenizer, MAX_LEN)\n",
        "dev_records   = build_casrel_records(CASREL_DEV, tokenizer, MAX_LEN)\n",
        "test_records  = build_casrel_records(CASREL_TEST, tokenizer, MAX_LEN)\n",
        "\n",
        "print(\"Records:\", len(train_records), len(dev_records), len(test_records))\n",
        "\n",
        "# ===========================================================\n",
        "# 4) Pos-weights for BCE\n",
        "# ===========================================================\n",
        "\n",
        "def compute_pos_weights(records):\n",
        "    total=0\n",
        "    sh=st=0\n",
        "    oh=[0]*num_rels\n",
        "    ot=[0]*num_rels\n",
        "\n",
        "    for r in records:\n",
        "        L=len(r[\"sub_heads\"])\n",
        "        total+=L\n",
        "        sh+=sum(r[\"sub_heads\"])\n",
        "        st+=sum(r[\"sub_tails\"])\n",
        "        for rid in range(num_rels):\n",
        "            oh[rid]+=sum(r[\"obj_heads\"][rid])\n",
        "            ot[rid]+=sum(r[\"obj_tails\"][rid])\n",
        "\n",
        "    def safe(pos):\n",
        "        neg=total-pos\n",
        "        pos=max(pos,1e-6)\n",
        "        w=neg/pos\n",
        "        return float(min(max(w,1.0),200.0))\n",
        "\n",
        "    return {\n",
        "        \"s_head\":safe(sh),\n",
        "        \"s_tail\":safe(st),\n",
        "        \"obj_head\":[safe(oh[i]) for i in range(num_rels)],\n",
        "        \"obj_tail\":[safe(ot[i]) for i in range(num_rels)],\n",
        "    }\n",
        "\n",
        "weights = compute_pos_weights(train_records)\n",
        "print(\"Weights computed.\")\n",
        "\n",
        "# ===========================================================\n",
        "# 5) Dataset + Collate\n",
        "# ===========================================================\n",
        "\n",
        "class CASRELDataset(Dataset):\n",
        "    def __init__(self, records):\n",
        "        self.records=records\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        r=self.records[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(r[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.ones(len(r[\"input_ids\"]), dtype=torch.long),\n",
        "            \"sub_head\": torch.tensor(r[\"sub_heads\"], dtype=torch.float),\n",
        "            \"sub_tail\": torch.tensor(r[\"sub_tails\"], dtype=torch.float),\n",
        "            \"obj_head\": torch.tensor(r[\"obj_heads\"], dtype=torch.float),\n",
        "            \"obj_tail\": torch.tensor(r[\"obj_tails\"], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "def casrel_collate(batch):\n",
        "    max_len=max(len(b[\"input_ids\"]) for b in batch)\n",
        "    R=num_rels\n",
        "\n",
        "    ids,att,sh,st,oh,ot=[],[],[],[],[],[]\n",
        "    for b in batch:\n",
        "        L=len(b[\"input_ids\"])\n",
        "        pad=max_len-L\n",
        "\n",
        "        ids.append(torch.cat([b[\"input_ids\"], torch.full((pad,), tokenizer.pad_token_id)]))\n",
        "        att.append(torch.cat([b[\"attention_mask\"], torch.zeros(pad)]))\n",
        "        sh.append(torch.cat([b[\"sub_head\"], torch.zeros(pad)]))\n",
        "        st.append(torch.cat([b[\"sub_tail\"], torch.zeros(pad)]))\n",
        "\n",
        "        oh_pad=torch.cat([b[\"obj_head\"], torch.zeros((R,pad))],dim=1)\n",
        "        ot_pad=torch.cat([b[\"obj_tail\"], torch.zeros((R,pad))],dim=1)\n",
        "        oh.append(oh_pad)\n",
        "        ot.append(ot_pad)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.stack(ids),\n",
        "        \"attention_mask\": torch.stack(att),\n",
        "        \"sub_head\": torch.stack(sh),\n",
        "        \"sub_tail\": torch.stack(st),\n",
        "        \"obj_head\": torch.stack(oh),\n",
        "        \"obj_tail\": torch.stack(ot)\n",
        "    }\n",
        "\n",
        "train_ds=CASRELDataset(train_records)\n",
        "dev_ds  =CASRELDataset(dev_records)\n",
        "test_ds =CASRELDataset(test_records)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=casrel_collate)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "\n",
        "# ===========================================================\n",
        "# 6) CASREL Model\n",
        "# ===========================================================\n",
        "\n",
        "class CASRELModel(nn.Module):\n",
        "    def __init__(self, bert_name, num_rels):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        H=self.bert.config.hidden_size\n",
        "\n",
        "        self.sub_head_proj = nn.Linear(H,1)\n",
        "        self.sub_tail_proj = nn.Linear(H,1)\n",
        "\n",
        "        self.obj_fc = nn.Linear(H*2, H)\n",
        "        self.obj_head_proj = nn.Linear(H, num_rels)\n",
        "        self.obj_tail_proj = nn.Linear(H, num_rels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, subject_span=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        seq = out.last_hidden_state\n",
        "\n",
        "        sh = self.sub_head_proj(seq).squeeze(-1)\n",
        "        st = self.sub_tail_proj(seq).squeeze(-1)\n",
        "\n",
        "        if subject_span is None:\n",
        "            return sh, st, None, None\n",
        "\n",
        "        # subject-conditioned object prediction\n",
        "        B,L,H = seq.size()\n",
        "        spans=subject_span.clamp(0,L-1)\n",
        "\n",
        "        subj_repr=[]\n",
        "        for i in range(B):\n",
        "            s,e = spans[i]\n",
        "            if e<s: e=s\n",
        "            subj_repr.append(seq[i, s:e+1, :].mean(0))\n",
        "        subj_repr = torch.stack(subj_repr)\n",
        "\n",
        "        subj_exp = subj_repr.unsqueeze(1).expand(-1,L,-1)\n",
        "        concat = torch.cat([seq, subj_exp], dim=-1)\n",
        "        h = self.relu(self.obj_fc(concat))\n",
        "\n",
        "        oh = self.obj_head_proj(h).permute(0,2,1)\n",
        "        ot = self.obj_tail_proj(h).permute(0,2,1)\n",
        "        return sh, st, oh, ot\n",
        "\n",
        "model = CASRELModel(MODEL_NAME, num_rels).to(DEVICE)\n",
        "\n",
        "# ===========================================================\n",
        "# 7) Loss + Optimizer\n",
        "# ===========================================================\n",
        "\n",
        "s_head_loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights[\"s_head\"]).to(DEVICE))\n",
        "s_tail_loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weights[\"s_tail\"]).to(DEVICE))\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "total_steps = len(train_loader)*EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=int(0.1*total_steps),\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "# ===========================================================\n",
        "# 8) Triple Decoder\n",
        "# ===========================================================\n",
        "\n",
        "def decode_triples_from_batch(input_ids, sh_log, st_log, model, attn, s_th, o_th):\n",
        "    B,L = sh_log.size()\n",
        "    preds=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(B):\n",
        "            sh = (torch.sigmoid(sh_log[i]) > s_th).cpu().numpy()\n",
        "            st = (torch.sigmoid(st_log[i]) > s_th).cpu().numpy()\n",
        "\n",
        "            subj_spans=[]\n",
        "            for p in range(L):\n",
        "                if sh[p]==1:\n",
        "                    for q in range(p,L):\n",
        "                        if st[q]==1:\n",
        "                            subj_spans.append((p,q))\n",
        "                            break\n",
        "\n",
        "            sample_preds=[]\n",
        "            for (s,e) in subj_spans:\n",
        "                span = torch.tensor([[s,e]], dtype=torch.long, device=DEVICE)\n",
        "                _,_,oh,ot = model(input_ids[i:i+1], attn[i:i+1], span)\n",
        "\n",
        "                oh = torch.sigmoid(oh[0]).cpu().numpy()\n",
        "                ot = torch.sigmoid(ot[0]).cpu().numpy()\n",
        "\n",
        "                for rid in range(num_rels):\n",
        "                    for a in range(L):\n",
        "                        if oh[rid,a] > o_th:\n",
        "                            for b in range(a,L):\n",
        "                                if ot[rid,b] > o_th:\n",
        "                                    sample_preds.append(((s,e),(a,b), id2rel[rid]))\n",
        "                                    break\n",
        "\n",
        "            preds.append(sample_preds)\n",
        "    return preds\n",
        "\n",
        "# ===========================================================\n",
        "# 9) Train + Evaluate\n",
        "# ===========================================================\n",
        "\n",
        "def train_and_evaluate():\n",
        "    history={\"train_loss\":[],\"dev_loss\":[]}\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        total=0\n",
        "        pbar=tqdm.tqdm(train_loader, desc=f\"Epoch {ep}/{EPOCHS}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            sub_h = batch[\"sub_head\"].to(DEVICE)\n",
        "            sub_t = batch[\"sub_tail\"].to(DEVICE)\n",
        "            obj_h = batch[\"obj_head\"].to(DEVICE)\n",
        "            obj_t = batch[\"obj_tail\"].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            sh_log, st_log, _, _ = model(input_ids, attn, None)\n",
        "            loss_sh = s_head_loss_fn(sh_log, sub_h)\n",
        "            loss_st = s_tail_loss_fn(st_log, sub_t)\n",
        "\n",
        "            # object loss\n",
        "            loss_obj=0.0\n",
        "            count=0\n",
        "            B,L = sub_h.size()\n",
        "\n",
        "            for i in range(B):\n",
        "                gold_spans=[]\n",
        "                shg=sub_h[i].cpu().numpy()\n",
        "                stg=sub_t[i].cpu().numpy()\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        for q in range(p,L):\n",
        "                            if stg[q]==1:\n",
        "                                gold_spans.append((p,q))\n",
        "                                break\n",
        "\n",
        "                for (s,e) in gold_spans:\n",
        "                    span = torch.tensor([[s,e]], dtype=torch.long, device=DEVICE)\n",
        "                    _,_,oh_log,ot_log = model(input_ids[i:i+1], attn[i:i+1], span)\n",
        "                    oh_log = oh_log[0]\n",
        "                    ot_log = ot_log[0]\n",
        "\n",
        "                    for rid in range(num_rels):\n",
        "                        pw_h = torch.tensor(weights[\"obj_head\"][rid]).to(DEVICE)\n",
        "                        pw_t = torch.tensor(weights[\"obj_tail\"][rid]).to(DEVICE)\n",
        "\n",
        "                        loss_h = nn.BCEWithLogitsLoss(pos_weight=pw_h)(oh_log[rid], obj_h[i,rid])\n",
        "                        loss_t = nn.BCEWithLogitsLoss(pos_weight=pw_t)(ot_log[rid], obj_t[i,rid])\n",
        "                        loss_obj += (loss_h + loss_t)/2\n",
        "\n",
        "                    count+=1\n",
        "\n",
        "            if count>0: loss_obj/=count\n",
        "            loss = loss_sh + loss_st + loss_obj\n",
        "\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total+=loss.item()\n",
        "            pbar.set_postfix({\"loss\":f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_train=total/len(train_loader)\n",
        "        history[\"train_loss\"].append(avg_train)\n",
        "\n",
        "        # ------------------- DEV EVAL -------------------\n",
        "        model.eval()\n",
        "        dev_loss=0\n",
        "        preds_all=[]\n",
        "        gold_all=[]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dev_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "                attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "                sub_h = batch[\"sub_head\"].to(DEVICE)\n",
        "                sub_t = batch[\"sub_tail\"].to(DEVICE)\n",
        "                obj_h = batch[\"obj_head\"].to(DEVICE)\n",
        "                obj_t = batch[\"obj_tail\"].to(DEVICE)\n",
        "\n",
        "                sh_log, st_log, _, _ = model(input_ids, attn, None)\n",
        "\n",
        "                # dev loss same as train\n",
        "                loss_sh = s_head_loss_fn(sh_log, sub_h)\n",
        "                loss_st = s_tail_loss_fn(st_log, sub_t)\n",
        "\n",
        "                loss_obj=0\n",
        "                count=0\n",
        "                B,L = sub_h.size()\n",
        "\n",
        "                for i in range(B):\n",
        "                    gold_spans=[]\n",
        "                    shg=sub_h[i].cpu().numpy()\n",
        "                    stg=sub_t[i].cpu().numpy()\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            for q in range(p,L):\n",
        "                                if stg[q]==1:\n",
        "                                    gold_spans.append((p,q))\n",
        "                                    break\n",
        "\n",
        "                    for (s,e) in gold_spans:\n",
        "                        span = torch.tensor([[s,e]], dtype=torch.long, device=DEVICE)\n",
        "                        _,_,oh_log,ot_log = model(input_ids[i:i+1], attn[i:i+1], span)\n",
        "                        oh_log = oh_log[0]\n",
        "                        ot_log = ot_log[0]\n",
        "\n",
        "                        for rid in range(num_rels):\n",
        "                            pw_h = torch.tensor(weights[\"obj_head\"][rid]).to(DEVICE)\n",
        "                            pw_t = torch.tensor(weights[\"obj_tail\"][rid]).to(DEVICE)\n",
        "                            loss_h = nn.BCEWithLogitsLoss(pos_weight=pw_h)(oh_log[rid], obj_h[i,rid])\n",
        "                            loss_t = nn.BCEWithLogitsLoss(pos_weight=pw_t)(ot_log[rid], obj_t[i,rid])\n",
        "                            loss_obj += (loss_h+loss_t)/2\n",
        "                        count+=1\n",
        "\n",
        "                if count>0: loss_obj/=count\n",
        "                dev_loss += (loss_sh + loss_st + loss_obj).item()\n",
        "\n",
        "                preds_batch = decode_triples_from_batch(\n",
        "                    input_ids, sh_log, st_log, model, attn, SUBJECT_TH, OBJECT_TH\n",
        "                )\n",
        "\n",
        "                # gold triples\n",
        "                B,L = input_ids.size()\n",
        "                for i in range(B):\n",
        "                    gold=[]\n",
        "                    shg=sub_h[i].cpu().numpy()\n",
        "                    stg=sub_t[i].cpu().numpy()\n",
        "\n",
        "                    subs=[]\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            for q in range(p,L):\n",
        "                                if stg[q]==1:\n",
        "                                    subs.append((p,q))\n",
        "                                    break\n",
        "\n",
        "                    for (s,e) in subs:\n",
        "                        for rid in range(num_rels):\n",
        "                            ohg=obj_h[i,rid].cpu().numpy()\n",
        "                            otg=obj_t[i,rid].cpu().numpy()\n",
        "                            for a in range(L):\n",
        "                                if ohg[a]==1:\n",
        "                                    for b in range(a,L):\n",
        "                                        if otg[b]==1:\n",
        "                                            gold.append(((s,e),(a,b),id2rel[rid]))\n",
        "                                            break\n",
        "                    gold_all.extend(gold)\n",
        "\n",
        "                for s in preds_batch:\n",
        "                    preds_all.extend(s)\n",
        "\n",
        "        avg_dev = dev_loss/len(dev_loader)\n",
        "        history[\"dev_loss\"].append(avg_dev)\n",
        "\n",
        "        # metrics\n",
        "        def to_set(lst):\n",
        "            return set([(ss,se,os,oe,rel) for ((ss,se),(os,oe),rel) in lst])\n",
        "\n",
        "        gold_set = to_set(gold_all)\n",
        "        pred_set = to_set(preds_all)\n",
        "\n",
        "        tp=len(pred_set & gold_set)\n",
        "        fp=len(pred_set - gold_set)\n",
        "        fn=len(gold_set - pred_set)\n",
        "\n",
        "        prec=tp/(tp+fp) if tp+fp>0 else 0\n",
        "        rec =tp/(tp+fn) if tp+fn>0 else 0\n",
        "        f1  =2*prec*rec/(prec+rec) if prec+rec>0 else 0\n",
        "\n",
        "        print(f\"\\nEpoch {ep} → Train {avg_train:.4f} | Dev {avg_dev:.4f}\")\n",
        "        print(f\"Triple P/R/F = {prec:.4f}/{rec:.4f}/{f1:.4f}\\n\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Train\n",
        "model, history = train_and_evaluate()\n",
        "\n",
        "# ===========================================================\n",
        "# 10) Final Evaluation on DEV + TEST\n",
        "# ===========================================================\n",
        "\n",
        "def final_eval(loader):\n",
        "    model.eval()\n",
        "    preds_all=[]\n",
        "    gold_all=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(loader):\n",
        "            input_ids=batch[\"input_ids\"].to(DEVICE)\n",
        "            attn=batch[\"attention_mask\"].to(DEVICE)\n",
        "            sh_log, st_log, _, _ = model(input_ids, attn, None)\n",
        "\n",
        "            preds_batch = decode_triples_from_batch(input_ids, sh_log, st_log, model, attn,\n",
        "                                                    SUBJECT_TH, OBJECT_TH)\n",
        "            B,L = input_ids.size()\n",
        "            for i in range(B):\n",
        "                gold=[]\n",
        "                shg=batch[\"sub_head\"][i].cpu().numpy()\n",
        "                stg=batch[\"sub_tail\"][i].cpu().numpy()\n",
        "\n",
        "                subs=[]\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        for q in range(p,L):\n",
        "                            if stg[q]==1:\n",
        "                                subs.append((p,q))\n",
        "                                break\n",
        "\n",
        "                for (s,e) in subs:\n",
        "                    for rid in range(num_rels):\n",
        "                        ohg=batch[\"obj_head\"][i,rid].cpu().numpy()\n",
        "                        otg=batch[\"obj_tail\"][i,rid].cpu().numpy()\n",
        "                        for a in range(L):\n",
        "                            if ohg[a]==1:\n",
        "                                for b in range(a,L):\n",
        "                                    if otg[b]==1:\n",
        "                                        gold.append(((s,e),(a,b),id2rel[rid]))\n",
        "                                        break\n",
        "                gold_all.extend(gold)\n",
        "            for sp in preds_batch:\n",
        "                preds_all.extend(sp)\n",
        "\n",
        "    def to_set(lst):\n",
        "        return set([(ss,se,os,oe,rel) for ((ss,se),(os,oe),rel) in lst])\n",
        "\n",
        "    gset=to_set(gold_all)\n",
        "    pset=to_set(preds_all)\n",
        "\n",
        "    tp=len(pset & gset)\n",
        "    fp=len(pset - gset)\n",
        "    fn=len(gset - pset)\n",
        "\n",
        "    prec=tp/(tp+fp) if tp+fp>0 else 0\n",
        "    rec =tp/(tp+fn) if tp+fn>0 else 0\n",
        "    f1  =2*prec*rec/(prec+rec) if prec+rec>0 else 0\n",
        "\n",
        "    return {\"precision\":prec,\"recall\":rec,\"f1\":f1,\"tp\":tp,\"fp\":fp,\"fn\":fn}\n",
        "\n",
        "dev_metrics = final_eval(dev_loader)\n",
        "test_metrics = final_eval(test_loader)\n",
        "\n",
        "print(\"\\n=== FINAL METRICS (BERT-BASE + CASREL) ===\")\n",
        "print(\"DEV →  P/R/F =\", dev_metrics)\n",
        "print(\"TEST → P/R/F =\", test_metrics)\n",
        "\n",
        "# ===========================================================\n",
        "# 11) Save model + metrics\n",
        "# ===========================================================\n",
        "\n",
        "torch.save(model.state_dict(), OUTPUT_DIR / \"casrel_bertbase_state_dict.pt\")\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "\n",
        "with open(OUTPUT_DIR / \"casrel_metrics_summary.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"relation_list\": relation_list,\n",
        "        \"num_rels\": num_rels,\n",
        "        \"train_records\": len(train_records),\n",
        "        \"dev_records\": len(dev_records),\n",
        "        \"test_records\": len(test_records),\n",
        "        \"weights\": weights,\n",
        "        \"history\": history,\n",
        "        \"dev_metrics\": dev_metrics,\n",
        "        \"test_metrics\": test_metrics\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"Saved model & metrics →\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "#       IMPROVED CASREL + BERT-BASE (Single Colab Cell)\n",
        "#       - Improved decoding (span scoring + top-k)\n",
        "#       - Dev threshold sweep to choose best decoding thresholds\n",
        "#       - Pos-weight clipping safeguard\n",
        "# ================================================================\n",
        "\n",
        "# Mount drive (Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# -------------------- Imports --------------------\n",
        "import json, os, tqdm, math, time, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertModel, get_linear_schedule_with_warmup\n",
        "\n",
        "# -------------------- User Paths (edit if needed) --------------------\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset\")\n",
        "SRC_TRAIN_TXT = BASE / \"finred_train.txt\"\n",
        "SRC_DEV_TXT   = BASE / \"finred_dev.txt\"\n",
        "SRC_TEST_TXT  = BASE / \"finred_test.txt\"\n",
        "CASREL_TRAIN  = BASE / \"casrel_train.jsonl\"\n",
        "CASREL_DEV    = BASE / \"casrel_dev.jsonl\"\n",
        "CASREL_TEST   = BASE / \"casrel_test.jsonl\"\n",
        "OUTPUT_DIR = BASE / \"casrel_bertbase_improved\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# -------------------- Hyperparameters --------------------\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LR = 3e-5\n",
        "SEED = 42\n",
        "\n",
        "# decoding defaults (will be tuned by sweep)\n",
        "DEFAULT_SUBJ_SCORE_TH = 0.18\n",
        "DEFAULT_OBJ_SCORE_TH  = 0.08\n",
        "DEFAULT_TOP_K_SUBJ = 5\n",
        "DEFAULT_MAX_OBJ_SPAN = 25\n",
        "\n",
        "# reproducibility\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# -------------------- Convert FinRED text -> CASREL jsonl --------------------\n",
        "def parse_finred_line(line):\n",
        "    parts = [p.strip() for p in line.strip().split(\"|\")]\n",
        "    if len(parts) == 0:\n",
        "        return None\n",
        "    text = parts[0]\n",
        "    triples = []\n",
        "    for p in parts[1:]:\n",
        "        if not p: continue\n",
        "        fields = [x.strip() for x in p.split(\";\") if x.strip() != \"\"]\n",
        "        if len(fields) != 3: continue\n",
        "        h,t,r = fields\n",
        "        triples.append((h,t,r))\n",
        "    return {\"text\": text, \"triples\": triples}\n",
        "\n",
        "def convert_txt_to_casrel_jsonl(src_path, out_path):\n",
        "    if not src_path.exists():\n",
        "        print(f\"[convert] Missing {src_path}; skipping.\")\n",
        "        return 0\n",
        "    n=0\n",
        "    with open(src_path, \"r\", encoding=\"utf-8\") as fr, open(out_path, \"w\", encoding=\"utf-8\") as fw:\n",
        "        for ln in fr:\n",
        "            if not ln.strip(): continue\n",
        "            parsed = parse_finred_line(ln)\n",
        "            if parsed is None: continue\n",
        "            text = parsed[\"text\"]\n",
        "            spo_list = []\n",
        "            for (h,t,r) in parsed[\"triples\"]:\n",
        "                spo_list.append({\"subject\": h, \"predicate\": r, \"object\": t})\n",
        "            rec = {\"text\": text, \"spo_list\": spo_list}\n",
        "            fw.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            n += 1\n",
        "    print(f\"[convert] wrote {n} records -> {out_path}\")\n",
        "    return n\n",
        "\n",
        "convert_txt_to_casrel_jsonl(SRC_TRAIN_TXT, CASREL_TRAIN)\n",
        "convert_txt_to_casrel_jsonl(SRC_DEV_TXT,   CASREL_DEV)\n",
        "convert_txt_to_casrel_jsonl(SRC_TEST_TXT,  CASREL_TEST)\n",
        "\n",
        "# -------------------- Tokenizer + relation collection --------------------\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def collect_relations(jsonl_paths):\n",
        "    rels = set()\n",
        "    for p in jsonl_paths:\n",
        "        if not p.exists(): continue\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            for ln in f:\n",
        "                if not ln.strip(): continue\n",
        "                rec = json.loads(ln)\n",
        "                for spo in rec.get(\"spo_list\", []):\n",
        "                    pred = spo.get(\"predicate\")\n",
        "                    if pred: rels.add(pred)\n",
        "    return sorted(list(rels))\n",
        "\n",
        "relation_list = collect_relations([CASREL_TRAIN, CASREL_DEV, CASREL_TEST])\n",
        "rel2id = {r:i for i,r in enumerate(relation_list)}\n",
        "id2rel = {i:r for r,i in rel2id.items()}\n",
        "num_rels = len(relation_list)\n",
        "print(\"Predicates found:\", num_rels)\n",
        "\n",
        "# -------------------- Build token-level CASREL records --------------------\n",
        "def build_casrel_records(jsonl_path, tokenizer, max_len=MAX_LEN):\n",
        "    records = []\n",
        "    if not jsonl_path.exists(): return records\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for ln in f:\n",
        "            if not ln.strip(): continue\n",
        "            rec = json.loads(ln)\n",
        "            text = rec.get(\"text\",\"\")\n",
        "            spo_list = rec.get(\"spo_list\", [])\n",
        "            enc = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "            offsets = enc[\"offset_mapping\"]\n",
        "            input_ids = enc[\"input_ids\"]\n",
        "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "            L = len(input_ids)\n",
        "            if L==0 or L>max_len: continue\n",
        "            sub_heads = [0]*L\n",
        "            sub_tails = [0]*L\n",
        "            obj_heads = [[0]*L for _ in range(num_rels)]\n",
        "            obj_tails = [[0]*L for _ in range(num_rels)]\n",
        "            for spo in spo_list:\n",
        "                subj = spo.get(\"subject\",\"\")\n",
        "                obj  = spo.get(\"object\",\"\")\n",
        "                pred = spo.get(\"predicate\",\"\")\n",
        "                if pred not in rel2id: continue\n",
        "                rid = rel2id[pred]\n",
        "                s_pos = text.lower().find(subj.lower())\n",
        "                o_pos = text.lower().find(obj.lower())\n",
        "                if s_pos == -1 or o_pos == -1: continue\n",
        "                s_end = s_pos + len(subj)\n",
        "                o_end = o_pos + len(obj)\n",
        "                s_tok = s_tok_end = None\n",
        "                o_tok = o_tok_end = None\n",
        "                for i,(a,b) in enumerate(offsets):\n",
        "                    if a <= s_pos < b: s_tok = i\n",
        "                    if a < s_end <= b: s_tok_end = i\n",
        "                    if a <= o_pos < b: o_tok = i\n",
        "                    if a < o_end <= b: o_tok_end = i\n",
        "                if None in [s_tok, s_tok_end, o_tok, o_tok_end]: continue\n",
        "                sub_heads[s_tok] = 1\n",
        "                sub_tails[s_tok_end] = 1\n",
        "                obj_heads[rid][o_tok] = 1\n",
        "                obj_tails[rid][o_tok_end] = 1\n",
        "            records.append({\n",
        "                \"text\": text,\n",
        "                \"tokens\": tokens,\n",
        "                \"input_ids\": input_ids,\n",
        "                \"offsets\": offsets,\n",
        "                \"sub_heads\": sub_heads,\n",
        "                \"sub_tails\": sub_tails,\n",
        "                \"obj_heads\": obj_heads,\n",
        "                \"obj_tails\": obj_tails\n",
        "            })\n",
        "    return records\n",
        "\n",
        "train_records = build_casrel_records(CASREL_TRAIN, tokenizer, MAX_LEN)\n",
        "dev_records   = build_casrel_records(CASREL_DEV, tokenizer, MAX_LEN)\n",
        "test_records  = build_casrel_records(CASREL_TEST, tokenizer, MAX_LEN)\n",
        "print(f\"Records: train {len(train_records)} dev {len(dev_records)} test {len(test_records)}\")\n",
        "\n",
        "# -------------------- Compute pos-weights and clip extremes --------------------\n",
        "def compute_pos_weights(records, clip_max=100.0):\n",
        "    total_tokens = 0\n",
        "    s_heads = s_tails = 0\n",
        "    obj_head_counts = [0]*num_rels\n",
        "    obj_tail_counts = [0]*num_rels\n",
        "    for r in records:\n",
        "        L = len(r[\"sub_heads\"])\n",
        "        total_tokens += L\n",
        "        s_heads += sum(r[\"sub_heads\"])\n",
        "        s_tails += sum(r[\"sub_tails\"])\n",
        "        for rid in range(num_rels):\n",
        "            obj_head_counts[rid] += sum(r[\"obj_heads\"][rid])\n",
        "            obj_tail_counts[rid] += sum(r[\"obj_tails\"][rid])\n",
        "    def safe_weight(pos, total):\n",
        "        neg = max(total - pos, 0)\n",
        "        pos = max(pos, 1e-6)\n",
        "        w = neg / pos\n",
        "        return float(min(max(w, 1.0), clip_max))\n",
        "    s_head_w = safe_weight(s_heads, total_tokens)\n",
        "    s_tail_w = safe_weight(s_tails, total_tokens)\n",
        "    obj_head_w = [safe_weight(obj_head_counts[i], total_tokens) for i in range(num_rels)]\n",
        "    obj_tail_w = [safe_weight(obj_tail_counts[i], total_tokens) for i in range(num_rels)]\n",
        "    return {\"s_head\": s_head_w, \"s_tail\": s_tail_w, \"obj_head\": obj_head_w, \"obj_tail\": obj_tail_w}\n",
        "\n",
        "weights = compute_pos_weights(train_records, clip_max=80.0)\n",
        "print(\"Pos-weights sample:\", {k:(weights[k] if not isinstance(weights[k], list) else f\"[{len(weights[k])}]\") for k in weights})\n",
        "\n",
        "# -------------------- Dataset and collate_fn --------------------\n",
        "class CASRELDataset(Dataset):\n",
        "    def __init__(self, records):\n",
        "        self.records = records\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        input_ids = torch.tensor(r[\"input_ids\"], dtype=torch.long)\n",
        "        attn = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        sub_head = torch.tensor(r[\"sub_heads\"], dtype=torch.float)\n",
        "        sub_tail = torch.tensor(r[\"sub_tails\"], dtype=torch.float)\n",
        "        obj_head = torch.tensor(r[\"obj_heads\"], dtype=torch.float)\n",
        "        obj_tail = torch.tensor(r[\"obj_tails\"], dtype=torch.float)\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"sub_head\": sub_head, \"sub_tail\": sub_tail, \"obj_head\": obj_head, \"obj_tail\": obj_tail}\n",
        "\n",
        "def casrel_collate(batch):\n",
        "    max_len = max([b[\"input_ids\"].size(0) for b in batch])\n",
        "    R = num_rels\n",
        "    input_ids_p, attn_p = [], []\n",
        "    sh_p, st_p = [], []\n",
        "    oh_p, ot_p = [], []\n",
        "    for b in batch:\n",
        "        L = b[\"input_ids\"].size(0)\n",
        "        pad_len = max_len - L\n",
        "        input_ids_p.append(torch.cat([b[\"input_ids\"], torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)]))\n",
        "        attn_p.append(torch.cat([b[\"attention_mask\"], torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        sh_p.append(torch.cat([b[\"sub_head\"], torch.zeros(pad_len)]))\n",
        "        st_p.append(torch.cat([b[\"sub_tail\"], torch.zeros(pad_len)]))\n",
        "        oh = b[\"obj_head\"]\n",
        "        ot = b[\"obj_tail\"]\n",
        "        # pad relation rows if needed\n",
        "        if oh.dim()==1:\n",
        "            oh = oh.unsqueeze(0)\n",
        "            ot = ot.unsqueeze(0)\n",
        "        if oh.size(0) != R:\n",
        "            # fallback: create zeros of shape R x L\n",
        "            oh = torch.zeros((R, oh.size(1)))\n",
        "            ot = torch.zeros((R, ot.size(1)))\n",
        "        oh_pad = torch.cat([oh, torch.zeros((R, pad_len))], dim=1)\n",
        "        ot_pad = torch.cat([ot, torch.zeros((R, pad_len))], dim=1)\n",
        "        oh_p.append(oh_pad)\n",
        "        ot_p.append(ot_pad)\n",
        "    batch_out = {\n",
        "        \"input_ids\": torch.stack(input_ids_p),\n",
        "        \"attention_mask\": torch.stack(attn_p),\n",
        "        \"sub_head\": torch.stack(sh_p),\n",
        "        \"sub_tail\": torch.stack(st_p),\n",
        "        \"obj_head\": torch.stack(oh_p),\n",
        "        \"obj_tail\": torch.stack(ot_p)\n",
        "    }\n",
        "    return batch_out\n",
        "\n",
        "train_ds = CASRELDataset(train_records)\n",
        "dev_ds   = CASRELDataset(dev_records)\n",
        "test_ds  = CASRELDataset(test_records)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=casrel_collate)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "print(\"DataLoaders ready. Examples:\", len(train_ds), len(dev_ds), len(test_ds))\n",
        "\n",
        "# -------------------- CASREL Model --------------------\n",
        "class CASRELModel(nn.Module):\n",
        "    def __init__(self, bert_name, num_rels):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        H = self.bert.config.hidden_size\n",
        "        self.sub_head_proj = nn.Linear(H, 1)\n",
        "        self.sub_tail_proj = nn.Linear(H, 1)\n",
        "        self.obj_fc = nn.Linear(H*2, H)\n",
        "        self.obj_head_proj = nn.Linear(H, num_rels)\n",
        "        self.obj_tail_proj = nn.Linear(H, num_rels)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_mask, subject_span=None):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        seq_out = bert_out.last_hidden_state   # B x L x H\n",
        "        sub_head_logits = self.sub_head_proj(seq_out).squeeze(-1)  # B x L\n",
        "        sub_tail_logits = self.sub_tail_proj(seq_out).squeeze(-1)\n",
        "        subj_cond_obj_head = None\n",
        "        subj_cond_obj_tail = None\n",
        "        if subject_span is not None:\n",
        "            if isinstance(subject_span, torch.Tensor):\n",
        "                spans = subject_span\n",
        "            else:\n",
        "                spans = torch.tensor(subject_span, dtype=torch.long, device=seq_out.device)\n",
        "            B, L, H = seq_out.size()\n",
        "            spans = spans.clamp(0, L-1)\n",
        "            subj_repr = []\n",
        "            for i in range(B):\n",
        "                s = spans[i,0].item()\n",
        "                e = spans[i,1].item()\n",
        "                if e < s: e = s\n",
        "                vec = seq_out[i, s:e+1, :].mean(dim=0)\n",
        "                subj_repr.append(vec)\n",
        "            subj_repr = torch.stack(subj_repr, dim=0)  # B x H\n",
        "            subj_exp = subj_repr.unsqueeze(1).expand(-1, seq_out.size(1), -1)  # B x L x H\n",
        "            concat = torch.cat([seq_out, subj_exp], dim=-1)  # B x L x 2H\n",
        "            h = self.relu(self.obj_fc(concat))  # B x L x H\n",
        "            oh = self.obj_head_proj(h)  # B x L x R\n",
        "            ot = self.obj_tail_proj(h)  # B x L x R\n",
        "            subj_cond_obj_head = oh.permute(0,2,1)  # B x R x L\n",
        "            subj_cond_obj_tail = ot.permute(0,2,1)\n",
        "        return sub_head_logits, sub_tail_logits, subj_cond_obj_head, subj_cond_obj_tail\n",
        "\n",
        "model = CASRELModel(MODEL_NAME, num_rels).to(DEVICE)\n",
        "\n",
        "# -------------------- Losses + Optimizer --------------------\n",
        "s_head_pw = torch.tensor(weights[\"s_head\"], dtype=torch.float, device=DEVICE)\n",
        "s_tail_pw = torch.tensor(weights[\"s_tail\"], dtype=torch.float, device=DEVICE)\n",
        "sub_head_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_head_pw)\n",
        "sub_tail_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_tail_pw)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "total_steps = max(1, len(train_loader) * EPOCHS)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1,int(0.1*total_steps)), num_training_steps=total_steps)\n",
        "\n",
        "# -------------------- Improved decoder --------------------\n",
        "def decode_triples_improved(input_ids, sub_h_logits, sub_t_logits, model, attention_mask,\n",
        "                             subject_score_th=DEFAULT_SUBJ_SCORE_TH, object_score_th=DEFAULT_OBJ_SCORE_TH,\n",
        "                             top_k_subj=DEFAULT_TOP_K_SUBJ, max_obj_span=DEFAULT_MAX_OBJ_SPAN, use_topk_subjects=True):\n",
        "    \"\"\"\n",
        "    - Span score for subjects: head_prob * tail_prob\n",
        "    - Choose top_k subject spans (if use_topk_subjects True) else threshold by subject_score_th\n",
        "    - For each chosen subject span, compute conditioned object head/tail probs and accept object spans\n",
        "      where object_head_prob * object_tail_prob >= object_score_th\n",
        "    - Limits object span length by max_obj_span\n",
        "    \"\"\"\n",
        "    B, L = sub_h_logits.size()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        sh_probs = torch.sigmoid(sub_h_logits).cpu().numpy()  # B x L\n",
        "        st_probs = torch.sigmoid(sub_t_logits).cpu().numpy()\n",
        "        for i in range(B):\n",
        "            # collect candidate subject spans with product score\n",
        "            subj_cands = []\n",
        "            for p in range(L):\n",
        "                if sh_probs[i,p] < 1e-3:  # skip tiny\n",
        "                    continue\n",
        "                for q in range(p, L):\n",
        "                    if st_probs[i,q] < 1e-3:\n",
        "                        continue\n",
        "                    score = float(sh_probs[i,p] * st_probs[i,q])\n",
        "                    subj_cands.append((p,q,score))\n",
        "            if len(subj_cands)==0:\n",
        "                all_preds.append([])\n",
        "                continue\n",
        "            subj_cands.sort(key=lambda x: x[2], reverse=True)\n",
        "            if use_topk_subjects:\n",
        "                chosen = subj_cands[:max(1, top_k_subj)]\n",
        "            else:\n",
        "                chosen = [s for s in subj_cands if s[2] >= subject_score_th]\n",
        "            sample_preds = []\n",
        "            # evaluate objects for each chosen subject\n",
        "            for (s_start, s_end, s_score) in chosen:\n",
        "                subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long, device=DEVICE)\n",
        "                input_i = input_ids[i:i+1,:].to(DEVICE)\n",
        "                attn_i = attention_mask[i:i+1,:].to(DEVICE)\n",
        "                _, _, obj_head_logits, obj_tail_logits = model(input_i, attn_i, subj_span_tensor)\n",
        "                if obj_head_logits is None or obj_tail_logits is None:\n",
        "                    continue\n",
        "                oh = torch.sigmoid(obj_head_logits.squeeze(0)).cpu().numpy()  # R x L\n",
        "                ot = torch.sigmoid(obj_tail_logits.squeeze(0)).cpu().numpy()  # R x L\n",
        "                for rid in range(num_rels):\n",
        "                    # find heads above very small threshold to speed up\n",
        "                    head_idxs = [a for a in range(L) if oh[rid,a] > 1e-4]\n",
        "                    if not head_idxs:\n",
        "                        continue\n",
        "                    for a in head_idxs:\n",
        "                        # greedy: first tail after head within max_obj_span that meets score\n",
        "                        for b in range(a, min(L, a + max_obj_span)):\n",
        "                            prod = float(oh[rid,a] * ot[rid,b])\n",
        "                            if prod >= object_score_th:\n",
        "                                sample_preds.append(((s_start, s_end),(a,b), id2rel[rid]))\n",
        "                                break\n",
        "            all_preds.append(sample_preds)\n",
        "    return all_preds\n",
        "\n",
        "# -------------------- Dev threshold sweep for auto tuning --------------------\n",
        "def dev_threshold_sweep(model, dev_loader, subj_ths=None, obj_ths=None, top_k_list=None, max_obj_span=DEFAULT_MAX_OBJ_SPAN):\n",
        "    if subj_ths is None:\n",
        "        subj_ths = [0.05, 0.1, 0.15, 0.18, 0.22, 0.3]\n",
        "    if obj_ths is None:\n",
        "        obj_ths = [0.01, 0.03, 0.05, 0.08, 0.12, 0.18]\n",
        "    if top_k_list is None:\n",
        "        top_k_list = [3,5,7]\n",
        "    best = {\"f1\": -1.0}\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for topk in top_k_list:\n",
        "            for st in subj_ths:\n",
        "                for ot in obj_ths:\n",
        "                    preds_all=[]\n",
        "                    gold_all=[]\n",
        "                    for batch in dev_loader:\n",
        "                        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "                        attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "                        sh_log, st_log, _, _ = model(input_ids, attn, None)\n",
        "                        preds_batch = decode_triples_improved(input_ids, sh_log, st_log, model, attn,\n",
        "                                                              subject_score_th=st, object_score_th=ot,\n",
        "                                                              top_k_subj=topk, max_obj_span=max_obj_span, use_topk_subjects=True)\n",
        "                        for sp in preds_batch:\n",
        "                            preds_all.extend(sp)\n",
        "                        # gold triples\n",
        "                        B,L = input_ids.size()\n",
        "                        for i in range(B):\n",
        "                            gold_sample=[]\n",
        "                            shg = batch[\"sub_head\"][i].cpu().numpy().astype(int)\n",
        "                            stg = batch[\"sub_tail\"][i].cpu().numpy().astype(int)\n",
        "                            gold_subs=[]\n",
        "                            for p in range(L):\n",
        "                                if shg[p]==1:\n",
        "                                    for q in range(p,L):\n",
        "                                        if stg[q]==1:\n",
        "                                            gold_subs.append((p,q)); break\n",
        "                            for (s,e) in gold_subs:\n",
        "                                for rid in range(num_rels):\n",
        "                                    ohg = batch[\"obj_head\"][i,rid].cpu().numpy().astype(int)\n",
        "                                    otg = batch[\"obj_tail\"][i,rid].cpu().numpy().astype(int)\n",
        "                                    for a in range(L):\n",
        "                                        if ohg[a]==1:\n",
        "                                            for b in range(a,L):\n",
        "                                                if otg[b]==1:\n",
        "                                                    gold_sample.append(((s,e),(a,b), id2rel[rid])); break\n",
        "                            gold_all.extend(gold_sample)\n",
        "                    # compute metrics\n",
        "                    def to_set(lst):\n",
        "                        return set([(ss,se,os,oe,rel) for ((ss,se),(os,oe),rel) in lst])\n",
        "                    gset = to_set(gold_all)\n",
        "                    pset = to_set(preds_all)\n",
        "                    tp = len(pset & gset)\n",
        "                    fp = len(pset - gset)\n",
        "                    fn = len(gset - pset)\n",
        "                    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "                    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "                    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "                    if f1 > best[\"f1\"]:\n",
        "                        best.update({\"f1\": f1, \"prec\": prec, \"rec\": rec, \"subj_th\": st, \"obj_th\": ot, \"topk\": topk})\n",
        "    return best\n",
        "\n",
        "# -------------------- Diagnostic helper to inspect raw probs --------------------\n",
        "def inspect_probs(model, dev_loader, n=1):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            sh_log, st_log, _, _ = model(input_ids, attn, None)\n",
        "            sh_p = torch.sigmoid(sh_log).cpu().numpy()\n",
        "            st_p = torch.sigmoid(st_log).cpu().numpy()\n",
        "            # print first n examples (token probs truncated)\n",
        "            for i in range(min(n, sh_p.shape[0])):\n",
        "                print(\"Sample\", i)\n",
        "                print(\"sh probs (first 60 tokens):\", np.round(sh_p[i,:60],3).tolist())\n",
        "                print(\"st probs (first 60 tokens):\", np.round(st_p[i,:60],3).tolist())\n",
        "            return\n",
        "\n",
        "# -------------------- Training loop (same structure as original, plus optional sweep after training) --------------------\n",
        "def train_and_evaluate(model, train_loader, dev_loader, epochs=EPOCHS):\n",
        "    history = {\"train_loss\": [], \"dev_loss\": []}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        t0 = time.time()\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f\"Train epoch {epoch}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            B, L = input_ids.shape\n",
        "            sub_head_gold = batch[\"sub_head\"].to(DEVICE)\n",
        "            sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "            obj_head_gold = batch[\"obj_head\"].to(DEVICE)\n",
        "            obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            sub_head_logits, sub_tail_logits, _, _ = model(input_ids, attn, subject_span=None)\n",
        "            loss_sh = sub_head_loss_fn(sub_head_logits, sub_head_gold)\n",
        "            loss_st = sub_tail_loss_fn(sub_tail_logits, sub_tail_gold)\n",
        "            # object loss computed using gold subject spans\n",
        "            loss_obj_total = 0.0\n",
        "            obj_count = 0\n",
        "            for i in range(B):\n",
        "                shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                subj_spans = []\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        q=None\n",
        "                        for k in range(p,L):\n",
        "                            if stg[k]==1:\n",
        "                                q=k; break\n",
        "                        if q is not None:\n",
        "                            subj_spans.append((p,q))\n",
        "                if len(subj_spans)==0: continue\n",
        "                for (s_start, s_end) in subj_spans:\n",
        "                    subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                    _, _, obj_head_logits, obj_tail_logits = model(input_ids[i:i+1,:], attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                    logits_oh = obj_head_logits.squeeze(0)  # R x L\n",
        "                    logits_ot = obj_tail_logits.squeeze(0)\n",
        "                    loss_rel = 0.0\n",
        "                    for rid in range(num_rels):\n",
        "                        pos_w_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                        pos_w_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                        loss_h = nn.BCEWithLogitsLoss(pos_weight=pos_w_h)(logits_oh[rid], obj_head_gold[i,rid])\n",
        "                        loss_t = nn.BCEWithLogitsLoss(pos_weight=pos_w_t)(logits_ot[rid], obj_tail_gold[i,rid])\n",
        "                        loss_rel += (loss_h + loss_t) / 2.0\n",
        "                    loss_obj_total += loss_rel / max(1, num_rels)\n",
        "                    obj_count += 1\n",
        "            if obj_count>0:\n",
        "                loss_obj_total = loss_obj_total / obj_count\n",
        "            else:\n",
        "                loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "            loss = loss_sh + loss_st + loss_obj_total\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        t1 = time.time()\n",
        "        # Validation (compute dev loss only)\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm.tqdm(dev_loader, desc=\"Dev evaluation\"):\n",
        "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "                attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "                sub_head_gold = batch[\"sub_head\"].to(DEVICE)\n",
        "                sub_tail_gold = batch[\"sub_tail\"].to(DEVICE)\n",
        "                obj_head_gold = batch[\"obj_head\"].to(DEVICE)\n",
        "                obj_tail_gold = batch[\"obj_tail\"].to(DEVICE)\n",
        "                sub_head_logits, sub_tail_logits, _, _ = model(input_ids, attn, subject_span=None)\n",
        "                loss_sh = sub_head_loss_fn(sub_head_logits, sub_head_gold)\n",
        "                loss_st = sub_tail_loss_fn(sub_tail_logits, sub_tail_gold)\n",
        "                loss_obj_total = 0.0\n",
        "                obj_count = 0\n",
        "                B,L = input_ids.shape\n",
        "                for i in range(B):\n",
        "                    shg = sub_head_gold[i].cpu().numpy().astype(int)\n",
        "                    stg = sub_tail_gold[i].cpu().numpy().astype(int)\n",
        "                    subj_spans=[]\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            q=None\n",
        "                            for k in range(p,L):\n",
        "                                if stg[k]==1:\n",
        "                                    q=k; break\n",
        "                            if q is not None: subj_spans.append((p,q))\n",
        "                    if len(subj_spans)==0: continue\n",
        "                    for (s_start, s_end) in subj_spans:\n",
        "                        subj_span_tensor = torch.tensor([[s_start, s_end]], dtype=torch.long).to(DEVICE)\n",
        "                        _, _, obj_head_logits, obj_tail_logits = model(input_ids[i:i+1,:], attn[i:i+1,:], subject_span=subj_span_tensor)\n",
        "                        logits_oh = obj_head_logits.squeeze(0)\n",
        "                        logits_ot = obj_tail_logits.squeeze(0)\n",
        "                        loss_rel = 0.0\n",
        "                        for rid in range(num_rels):\n",
        "                            pos_w_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                            pos_w_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                            loss_h = nn.BCEWithLogitsLoss(pos_weight=pos_w_h)(logits_oh[rid], obj_head_gold[i,rid])\n",
        "                            loss_t = nn.BCEWithLogitsLoss(pos_weight=pos_w_t)(logits_ot[rid], obj_tail_gold[i,rid])\n",
        "                            loss_rel += (loss_h + loss_t) / 2.0\n",
        "                        loss_obj_total += loss_rel / max(1, num_rels)\n",
        "                        obj_count += 1\n",
        "                if obj_count>0:\n",
        "                    loss_obj_total = loss_obj_total / obj_count\n",
        "                else:\n",
        "                    loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "                batch_loss = loss_sh + loss_st + loss_obj_total\n",
        "                val_loss += batch_loss.item()\n",
        "        avg_dev_loss = val_loss / max(1, len(dev_loader))\n",
        "        history[\"dev_loss\"].append(avg_dev_loss)\n",
        "        print(f\"\\nEpoch {epoch} summary: train_loss={avg_train_loss:.6f} dev_loss={avg_dev_loss:.6f} time={t1-t0:.1f}s\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# -------------------- Run training --------------------\n",
        "model, history = train_and_evaluate(model, train_loader, dev_loader, epochs=EPOCHS)\n",
        "\n",
        "# -------------------- Automatic threshold sweep on dev to pick best decoding params --------------------\n",
        "print(\"\\nRunning dev threshold sweep to pick best decoding thresholds (this may be slow)...\")\n",
        "best_cfg = dev_threshold_sweep(model, dev_loader, subj_ths=[0.06,0.1,0.14,0.18,0.22], obj_ths=[0.01,0.03,0.05,0.08,0.12], top_k_list=[3,5])\n",
        "print(\"Best dev cfg found:\", best_cfg)\n",
        "\n",
        "# You can override best_cfg if you prefer manual values:\n",
        "SUBJ_SCORE_TH = best_cfg.get(\"subj_th\", DEFAULT_SUBJ_SCORE_TH)\n",
        "OBJ_SCORE_TH  = best_cfg.get(\"obj_th\", DEFAULT_OBJ_SCORE_TH)\n",
        "TOP_K_SUBJ    = best_cfg.get(\"topk\", DEFAULT_TOP_K_SUBJ)\n",
        "MAX_OBJ_SPAN  = DEFAULT_MAX_OBJ_SPAN\n",
        "\n",
        "print(f\"Using decode params -> subj_score_th={SUBJ_SCORE_TH}, obj_score_th={OBJ_SCORE_TH}, top_k_subj={TOP_K_SUBJ}\")\n",
        "\n",
        "# -------------------- Final evaluation (using chosen decoding params) --------------------\n",
        "def final_eval(loader, decode_params):\n",
        "    model.eval()\n",
        "    preds_all=[]\n",
        "    gold_all=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(loader, desc=\"Final Eval\"):\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            sub_head_logits, sub_tail_logits, _, _ = model(input_ids, attn, None)\n",
        "            preds_batch = decode_triples_improved(input_ids, sub_head_logits, sub_tail_logits, model, attn,\n",
        "                                                  subject_score_th=decode_params[\"subj_th\"],\n",
        "                                                  object_score_th=decode_params[\"obj_th\"],\n",
        "                                                  top_k_subj=decode_params[\"topk\"],\n",
        "                                                  max_obj_span=decode_params.get(\"max_obj_span\", DEFAULT_MAX_OBJ_SPAN),\n",
        "                                                  use_topk_subjects=True)\n",
        "            # collect preds\n",
        "            for sp in preds_batch:\n",
        "                preds_all.extend(sp)\n",
        "            # collect gold\n",
        "            B,L = input_ids.size()\n",
        "            for i in range(B):\n",
        "                gold_sample=[]\n",
        "                shg = batch[\"sub_head\"][i].cpu().numpy().astype(int)\n",
        "                stg = batch[\"sub_tail\"][i].cpu().numpy().astype(int)\n",
        "                subs=[]\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        for q in range(p,L):\n",
        "                            if stg[q]==1:\n",
        "                                subs.append((p,q)); break\n",
        "                for (s,e) in subs:\n",
        "                    for rid in range(num_rels):\n",
        "                        ohg = batch[\"obj_head\"][i,rid].cpu().numpy().astype(int)\n",
        "                        otg = batch[\"obj_tail\"][i,rid].cpu().numpy().astype(int)\n",
        "                        for a in range(L):\n",
        "                            if ohg[a]==1:\n",
        "                                for b in range(a,L):\n",
        "                                    if otg[b]==1:\n",
        "                                        gold_sample.append(((s,e),(a,b), id2rel[rid])); break\n",
        "                gold_all.extend(gold_sample)\n",
        "    # metrics\n",
        "    def to_set(lst):\n",
        "        return set([(ss,se,os,oe,rel) for ((ss,se),(os,oe),rel) in lst])\n",
        "    gset = to_set(gold_all)\n",
        "    pset = to_set(preds_all)\n",
        "    tp = len(pset & gset)\n",
        "    fp = len(pset - gset)\n",
        "    fn = len(gset - pset)\n",
        "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"tp\": tp, \"fp\": fp, \"fn\": fn}\n",
        "\n",
        "decode_params = {\"subj_th\": SUBJ_SCORE_TH, \"obj_th\": OBJ_SCORE_TH, \"topk\": TOP_K_SUBJ, \"max_obj_span\": MAX_OBJ_SPAN}\n",
        "dev_metrics = final_eval(dev_loader, decode_params)\n",
        "test_metrics = final_eval(test_loader, decode_params)\n",
        "\n",
        "print(\"\\n=== FINAL METRICS (BERT-BASE + CASREL, improved decoding) ===\")\n",
        "print(\"DEV triple P/R/F: {:.4f} / {:.4f} / {:.4f}\".format(dev_metrics[\"precision\"], dev_metrics[\"recall\"], dev_metrics[\"f1\"]))\n",
        "print(\"TEST triple P/R/F: {:.4f} / {:.4f} / {:.4f}\".format(test_metrics[\"precision\"], test_metrics[\"recall\"], test_metrics[\"f1\"]))\n",
        "\n",
        "# -------------------- Save model, tokenizer and metrics summary --------------------\n",
        "torch.save(model.state_dict(), OUTPUT_DIR / \"casrel_bertbase_improved_state_dict.pt\")\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "\n",
        "metrics_summary = {\n",
        "    \"relation_list\": relation_list,\n",
        "    \"num_rels\": num_rels,\n",
        "    \"train_records\": len(train_records),\n",
        "    \"dev_records\": len(dev_records),\n",
        "    \"test_records\": len(test_records),\n",
        "    \"weights\": weights,\n",
        "    \"history\": history,\n",
        "    \"best_dev_cfg\": best_cfg,\n",
        "    \"dev_metrics\": dev_metrics,\n",
        "    \"test_metrics\": test_metrics\n",
        "}\n",
        "with open(OUTPUT_DIR / \"casrel_metrics_summary_improved.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics_summary, f, indent=2)\n",
        "\n",
        "print(\"Saved model & metrics to\", OUTPUT_DIR)\n",
        "\n",
        "# -------------------- Optional: inspect raw probs for a few dev samples --------------------\n",
        "print(\"\\nSample probability inspection (first dev batch):\")\n",
        "inspect_probs(model, dev_loader, n=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgmwkgkdYfTX",
        "outputId": "1293c8d9-fbef-4f83-81b7-dd7b7e3fe7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "[convert] wrote 5700 records -> /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_train.jsonl\n",
            "[convert] wrote 1007 records -> /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_dev.jsonl\n",
            "[convert] wrote 1068 records -> /content/drive/MyDrive/Datasets_EE782_course_project/FinRED_dataset/casrel_test.jsonl\n",
            "Predicates found: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records: train 5585 dev 972 test 1035\n",
            "Pos-weights sample: {'s_head': 37.212396430847676, 's_tail': 36.77303512364152, 'obj_head': '[29]', 'obj_tail': '[29]'}\n",
            "DataLoaders ready. Examples: 5585 972 1035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 1/4: 100%|██████████| 699/699 [06:46<00:00,  1.72it/s, loss=0.4050]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:26<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 summary: train_loss=1.043166 dev_loss=0.547334 time=406.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 2/4: 100%|██████████| 699/699 [06:45<00:00,  1.73it/s, loss=0.1749]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:26<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 summary: train_loss=0.405961 dev_loss=0.509880 time=405.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 3/4: 100%|██████████| 699/699 [06:44<00:00,  1.73it/s, loss=0.1363]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:26<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 summary: train_loss=0.231152 dev_loss=0.692913 time=404.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 4/4: 100%|██████████| 699/699 [06:45<00:00,  1.72it/s, loss=0.2676]\n",
            "Dev evaluation: 100%|██████████| 122/122 [00:26<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 summary: train_loss=0.161012 dev_loss=0.782831 time=405.8s\n",
            "\n",
            "Running dev threshold sweep to pick best decoding thresholds (this may be slow)...\n"
          ]
        }
      ]
    }
  ]
}