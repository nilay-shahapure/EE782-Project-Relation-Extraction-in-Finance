{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohuYOFji1qv_",
        "outputId": "474e8887-1d99-46c4-9894-b46e76d18ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cuda\n",
            "Loaded records: 2117 454 454\n",
            "Relations found: 19\n",
            "CASREL records -> train/dev/test: 2116 454 454\n",
            "Pos weights samples: {'s_head': 13.835115229270611, 's_tail': 13.835115229270611, 'obj_head': '[19]', 'obj_tail': '[19]'}\n",
            "DataLoaders prepared: 2116 454 454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 1/3: 100%|██████████| 265/265 [03:28<00:00,  1.27it/s, loss=1.3965]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 summary: train_loss=1.444146 dev_loss=0.803490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 2/3: 100%|██████████| 265/265 [03:27<00:00,  1.28it/s, loss=0.7042]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 summary: train_loss=0.662454 dev_loss=0.658286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 3/3: 100%|██████████| 265/265 [03:27<00:00,  1.28it/s, loss=0.8261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 summary: train_loss=0.489188 dev_loss=0.672049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Final Eval: 100%|██████████| 57/57 [00:42<00:00,  1.34it/s]\n",
            "Final Eval: 100%|██████████| 57/57 [00:44<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL METRICS ===\n",
            "DEV triple P/R/F: 0.007165040471474335 0.34079674323931375 0.014035003682392178\n",
            "TEST triple P/R/F: 0.007052239511448072 0.39332161687170475 0.013856040812788668\n",
            "Saved model + metrics to /content/drive/MyDrive/Datasets_EE782_course_project/FIRE_models/casrel_bertbase\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "#   CASREL + BERT-BASE on FIRE (Colab-ready single cell)\n",
        "# ================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import json, random, time, tqdm\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertModel, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FIRE_dataset\")\n",
        "# source finred-style text (your file)\n",
        "\n",
        "TRAIN_JSON = BASE /\"fire_train.json\"\n",
        "DEV_JSON   = BASE / \"fire_dev.json\"   # may or may not exist\n",
        "TEST_JSON = BASE / \"fire_test.json\"  # may or may not exist\n",
        "TYPES_JSON = BASE / \"fire_types.json\" # relations / entity types\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/Datasets_EE782_course_project/FIRE_models/casrel_bertbase\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ---------- Hyperparams ----------\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3        # set to 1..3 to fit GPU budget\n",
        "LR = 3e-5\n",
        "SEED = 42\n",
        "TOP_K_SUBJ = 5\n",
        "SUBJ_SCORE_TH = 0.18\n",
        "OBJ_SCORE_TH  = 0.08\n",
        "MAX_OBJ_SPAN = 30\n",
        "POS_WEIGHT_CLIP = 80.0\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# ---------- Load JSON helper ----------\n",
        "def load_json(path):\n",
        "    if not path.exists():\n",
        "        print(f\"[warn] {path} not found -> returning []\")\n",
        "        return []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "train_rec = load_json(TRAIN_JSON)\n",
        "dev_rec   = load_json(DEV_JSON)\n",
        "test_rec  = load_json(TEST_JSON)\n",
        "types_rec = load_json(TYPES_JSON)\n",
        "\n",
        "print(\"Loaded records:\", len(train_rec), len(dev_rec), len(test_rec))\n",
        "\n",
        "# ---------- Infer relation list ----------\n",
        "def infer_relations(recs, types):\n",
        "    rels = set()\n",
        "    for r in recs:\n",
        "        for rel in r.get(\"relations\", []):\n",
        "            if rel.get(\"type\") is not None:\n",
        "                rels.add(rel[\"type\"])\n",
        "    if isinstance(types, dict):\n",
        "        for k in types.get(\"relations\", {}).keys():\n",
        "            rels.add(k)\n",
        "    rels = sorted(list(rels))\n",
        "    if \"no_relation\" not in rels:\n",
        "        rels = [\"no_relation\"] + rels\n",
        "    return rels\n",
        "\n",
        "relation_list = infer_relations(train_rec + dev_rec + test_rec, types_rec)\n",
        "rel2id = {r:i for i,r in enumerate(relation_list)}\n",
        "id2rel = {i:r for r,i in rel2id.items()}\n",
        "num_rels = len(relation_list)\n",
        "print(\"Relations found:\", num_rels)\n",
        "\n",
        "# ---------- Tokenizer ----------\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ---------- Build CASREL records from FIRE ----------\n",
        "# FIRE records typically contain 'tokens' (list) and 'entities' each with start/end and possibly id.\n",
        "# We will treat entity 'start' and 'end' as token indices. The code is robust to 'end' being exclusive or inclusive.\n",
        "def build_casrel_records_from_fire(records, tokenizer, max_len=MAX_LEN):\n",
        "    recs = []\n",
        "    for r in records:\n",
        "        tokens = r.get(\"tokens\", [])\n",
        "        if not tokens:\n",
        "            continue\n",
        "        # We'll assume tokens are already tokenized (wordpieces) — convert tokens -> input_ids via tokenizer.convert_tokens_to_ids\n",
        "        # If tokens include special markers not in BERT vocab, they will map to [UNK] — acceptable.\n",
        "        try:\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        except Exception:\n",
        "            # fallback: encode joined text\n",
        "            text = \" \".join(tokens)\n",
        "            enc = tokenizer(text, add_special_tokens=False)\n",
        "            input_ids = enc[\"input_ids\"]\n",
        "        L = len(input_ids)\n",
        "        if L==0 or L>max_len:\n",
        "            continue\n",
        "\n",
        "        # init labels\n",
        "        sub_heads = [0]*L\n",
        "        sub_tails = [0]*L\n",
        "        obj_heads = [[0]*L for _ in range(num_rels)]\n",
        "        obj_tails = [[0]*L for _ in range(num_rels)]\n",
        "\n",
        "        # Build mapping from entity index -> token span (inclusive ends)\n",
        "        ent_list = r.get(\"entities\", [])\n",
        "        ent_spans = []\n",
        "        for ent in ent_list:\n",
        "            s = ent.get(\"start\")\n",
        "            e = ent.get(\"end\")\n",
        "            # safety and normalization: many datasets use exclusive 'end'; if end equals len(tokens) treat as exclusive\n",
        "            if s is None or e is None:\n",
        "                ent_spans.append(None); continue\n",
        "            if not isinstance(s, int) or not isinstance(e, int):\n",
        "                ent_spans.append(None); continue\n",
        "            # Normalize: if end > s and end <= L then exclusive -> convert to inclusive end-1\n",
        "            if e > s and e <= L:\n",
        "                e_idx = e-1\n",
        "            else:\n",
        "                # if end within 0..L-1 assume inclusive\n",
        "                if 0 <= e < L:\n",
        "                    e_idx = e\n",
        "                else:\n",
        "                    ent_spans.append(None); continue\n",
        "            if not (0 <= s < L and 0 <= e_idx < L and s <= e_idx):\n",
        "                ent_spans.append(None); continue\n",
        "            ent_spans.append((s, e_idx))\n",
        "\n",
        "        # Fill labels using relations on record\n",
        "        for rel in r.get(\"relations\", []):\n",
        "            rtype = rel.get(\"type\")\n",
        "            if rtype is None or rtype not in rel2id:\n",
        "                continue\n",
        "            rid = rel2id[rtype]\n",
        "            # head / tail refer to entity indices (usually)\n",
        "            h_idx = rel.get(\"head\")\n",
        "            t_idx = rel.get(\"tail\")\n",
        "            # safety\n",
        "            if h_idx is None or t_idx is None:\n",
        "                continue\n",
        "            if not (0 <= h_idx < len(ent_spans)) or not (0 <= t_idx < len(ent_spans)):\n",
        "                continue\n",
        "            h_span = ent_spans[h_idx]\n",
        "            t_span = ent_spans[t_idx]\n",
        "            if h_span is None or t_span is None:\n",
        "                continue\n",
        "            s_tok, s_tok_end = h_span\n",
        "            o_tok, o_tok_end = t_span\n",
        "            # set head/tail labels\n",
        "            sub_heads[s_tok] = 1\n",
        "            sub_tails[s_tok_end] = 1\n",
        "            obj_heads[rid][o_tok] = 1\n",
        "            obj_tails[rid][o_tok_end] = 1\n",
        "\n",
        "        recs.append({\n",
        "            \"tokens\": tokens,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"sub_heads\": sub_heads,\n",
        "            \"sub_tails\": sub_tails,\n",
        "            \"obj_heads\": obj_heads,\n",
        "            \"obj_tails\": obj_tails\n",
        "        })\n",
        "    return recs\n",
        "\n",
        "train_records = build_casrel_records_from_fire(train_rec, tokenizer, MAX_LEN)\n",
        "dev_records = build_casrel_records_from_fire(dev_rec, tokenizer, MAX_LEN)\n",
        "test_records = build_casrel_records_from_fire(test_rec, tokenizer, MAX_LEN)\n",
        "print(\"CASREL records -> train/dev/test:\", len(train_records), len(dev_records), len(test_records))\n",
        "\n",
        "# ---------- Compute pos-weights (with clipping) ----------\n",
        "def compute_pos_weights(records, clip_max=POS_WEIGHT_CLIP):\n",
        "    total_tokens = 0\n",
        "    s_heads = s_tails = 0\n",
        "    obj_head_counts = [0]*num_rels\n",
        "    obj_tail_counts = [0]*num_rels\n",
        "    for r in records:\n",
        "        L = len(r[\"sub_heads\"])\n",
        "        total_tokens += L\n",
        "        s_heads += sum(r[\"sub_heads\"])\n",
        "        s_tails += sum(r[\"sub_tails\"])\n",
        "        for rid in range(num_rels):\n",
        "            obj_head_counts[rid] += sum(r[\"obj_heads\"][rid])\n",
        "            obj_tail_counts[rid] += sum(r[\"obj_tails\"][rid])\n",
        "    def safe(pos, total):\n",
        "        neg = max(total - pos, 0)\n",
        "        pos = max(pos, 1e-6)\n",
        "        w = neg / pos\n",
        "        return float(min(max(w, 1.0), clip_max))\n",
        "    return {\"s_head\": safe(s_heads, total_tokens), \"s_tail\": safe(s_tails, total_tokens),\n",
        "            \"obj_head\": [safe(obj_head_counts[i], total_tokens) for i in range(num_rels)],\n",
        "            \"obj_tail\": [safe(obj_tail_counts[i], total_tokens) for i in range(num_rels)]}\n",
        "\n",
        "weights = compute_pos_weights(train_records)\n",
        "print(\"Pos weights samples:\", {k:(weights[k] if not isinstance(weights[k], list) else f\"[{len(weights[k])}]\") for k in weights})\n",
        "\n",
        "# ---------- Dataset + collate ----------\n",
        "class CASRELDataset(Dataset):\n",
        "    def __init__(self, records):\n",
        "        self.records = records\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        input_ids = torch.tensor(r[\"input_ids\"], dtype=torch.long)\n",
        "        attn = torch.ones_like(input_ids, dtype=torch.long)\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attn,\n",
        "            \"sub_head\": torch.tensor(r[\"sub_heads\"], dtype=torch.float),\n",
        "            \"sub_tail\": torch.tensor(r[\"sub_tails\"], dtype=torch.float),\n",
        "            \"obj_head\": torch.tensor(r[\"obj_heads\"], dtype=torch.float),\n",
        "            \"obj_tail\": torch.tensor(r[\"obj_tails\"], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "def casrel_collate(batch):\n",
        "    max_len = max([b[\"input_ids\"].size(0) for b in batch])\n",
        "    R = num_rels\n",
        "    ids, atts, sh, st, oh, ot = [], [], [], [], [], []\n",
        "    for b in batch:\n",
        "        L = b[\"input_ids\"].size(0)\n",
        "        pad = max_len - L\n",
        "        ids.append(torch.cat([b[\"input_ids\"], torch.full((pad,), tokenizer.pad_token_id, dtype=torch.long)]))\n",
        "        atts.append(torch.cat([b[\"attention_mask\"], torch.zeros(pad, dtype=torch.long)]))\n",
        "        sh.append(torch.cat([b[\"sub_head\"], torch.zeros(pad)]))\n",
        "        st.append(torch.cat([b[\"sub_tail\"], torch.zeros(pad)]))\n",
        "        oh_b = b[\"obj_head\"]\n",
        "        ot_b = b[\"obj_tail\"]\n",
        "        if oh_b.dim() == 1:\n",
        "            oh_b = oh_b.unsqueeze(0)\n",
        "            ot_b = ot_b.unsqueeze(0)\n",
        "        if oh_b.size(0) != R:\n",
        "            oh_b = torch.zeros((R, oh_b.size(1)))\n",
        "            ot_b = torch.zeros((R, ot_b.size(1)))\n",
        "        oh.append(torch.cat([oh_b, torch.zeros((R, pad))], dim=1))\n",
        "        ot.append(torch.cat([ot_b, torch.zeros((R, pad))], dim=1))\n",
        "    return {\n",
        "        \"input_ids\": torch.stack(ids),\n",
        "        \"attention_mask\": torch.stack(atts),\n",
        "        \"sub_head\": torch.stack(sh),\n",
        "        \"sub_tail\": torch.stack(st),\n",
        "        \"obj_head\": torch.stack(oh),\n",
        "        \"obj_tail\": torch.stack(ot)\n",
        "    }\n",
        "\n",
        "train_ds = CASRELDataset(train_records)\n",
        "dev_ds   = CASRELDataset(dev_records)\n",
        "test_ds  = CASRELDataset(test_records)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=casrel_collate)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=casrel_collate)\n",
        "\n",
        "print(\"DataLoaders prepared:\", len(train_ds), len(dev_ds), len(test_ds))\n",
        "\n",
        "# ---------- Model ----------\n",
        "class CASRELModel(nn.Module):\n",
        "    def __init__(self, bert_name, num_rels):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_name)\n",
        "        H = self.bert.config.hidden_size\n",
        "        self.sub_head_proj = nn.Linear(H,1)\n",
        "        self.sub_tail_proj = nn.Linear(H,1)\n",
        "        self.obj_fc = nn.Linear(H*2, H)\n",
        "        self.obj_head_proj = nn.Linear(H, num_rels)\n",
        "        self.obj_tail_proj = nn.Linear(H, num_rels)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, input_ids, attention_mask, subject_span=None):\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        seq = out.last_hidden_state  # B x L x H\n",
        "        sh = self.sub_head_proj(seq).squeeze(-1)\n",
        "        st = self.sub_tail_proj(seq).squeeze(-1)\n",
        "        if subject_span is None:\n",
        "            return sh, st, None, None\n",
        "        # subject_span: tensor B x 2\n",
        "        if not isinstance(subject_span, torch.Tensor):\n",
        "            spans = torch.tensor(subject_span, dtype=torch.long, device=seq.device)\n",
        "        else:\n",
        "            spans = subject_span\n",
        "        spans = spans.clamp(0, seq.size(1)-1)\n",
        "        subj_repr = []\n",
        "        for i in range(seq.size(0)):\n",
        "            s = spans[i,0].item(); e = spans[i,1].item()\n",
        "            if e < s: e = s\n",
        "            subj_repr.append(seq[i, s:e+1, :].mean(dim=0))\n",
        "        subj_repr = torch.stack(subj_repr, dim=0)  # B x H\n",
        "        subj_exp = subj_repr.unsqueeze(1).expand(-1, seq.size(1), -1)\n",
        "        concat = torch.cat([seq, subj_exp], dim=-1)\n",
        "        h = self.relu(self.obj_fc(concat))\n",
        "        oh = self.obj_head_proj(h).permute(0,2,1)  # B x R x L\n",
        "        ot = self.obj_tail_proj(h).permute(0,2,1)\n",
        "        return sh, st, oh, ot\n",
        "\n",
        "model = CASRELModel(MODEL_NAME, num_rels).to(DEVICE)\n",
        "\n",
        "# ---------- Losses + optimizer ----------\n",
        "s_head_pw = torch.tensor(weights[\"s_head\"], dtype=torch.float, device=DEVICE)\n",
        "s_tail_pw = torch.tensor(weights[\"s_tail\"], dtype=torch.float, device=DEVICE)\n",
        "sub_head_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_head_pw)\n",
        "sub_tail_loss_fn = nn.BCEWithLogitsLoss(pos_weight=s_tail_pw)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "total_steps = max(1, len(train_loader) * EPOCHS)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max(1,int(0.1*total_steps)), num_training_steps=total_steps)\n",
        "\n",
        "# ---------- Decoder: top-k subjects + product scoring ----------\n",
        "def decode_triples(input_ids, sh_log, st_log, model, attn, subject_score_th=SUBJ_SCORE_TH, object_score_th=OBJ_SCORE_TH, top_k_subj=TOP_K_SUBJ, max_obj_span=MAX_OBJ_SPAN, use_topk=True):\n",
        "    B,L = sh_log.size()\n",
        "    all_preds=[]\n",
        "    with torch.no_grad():\n",
        "        sh_p = torch.sigmoid(sh_log).cpu().numpy()\n",
        "        st_p = torch.sigmoid(st_log).cpu().numpy()\n",
        "        for i in range(B):\n",
        "            cand = []\n",
        "            for p in range(L):\n",
        "                if sh_p[i,p] < 1e-4: continue\n",
        "                for q in range(p, L):\n",
        "                    if st_p[i,q] < 1e-4: continue\n",
        "                    score = float(sh_p[i,p] * st_p[i,q])\n",
        "                    cand.append((p,q,score))\n",
        "            if not cand:\n",
        "                all_preds.append([]); continue\n",
        "            cand.sort(key=lambda x: x[2], reverse=True)\n",
        "            if use_topk:\n",
        "                chosen = cand[:max(1, top_k_subj)]\n",
        "            else:\n",
        "                chosen = [c for c in cand if c[2] >= subject_score_th]\n",
        "            preds = []\n",
        "            for (s,e,sc) in chosen:\n",
        "                span_tensor = torch.tensor([[s,e]], dtype=torch.long, device=DEVICE)\n",
        "                inp = input_ids[i:i+1,:].to(DEVICE); att = attn[i:i+1,:].to(DEVICE)\n",
        "                _,_, oh_log, ot_log = model(inp, att, subject_span=span_tensor)\n",
        "                if oh_log is None: continue\n",
        "                oh = torch.sigmoid(oh_log.squeeze(0)).cpu().numpy()\n",
        "                ot = torch.sigmoid(ot_log.squeeze(0)).cpu().numpy()\n",
        "                for rid in range(num_rels):\n",
        "                    head_idxs = [a for a in range(L) if oh[rid,a] > 1e-4]\n",
        "                    for a in head_idxs:\n",
        "                        for b in range(a, min(L, a+max_obj_span)):\n",
        "                            prod = float(oh[rid,a] * ot[rid,b])\n",
        "                            if prod >= object_score_th:\n",
        "                                preds.append(((s,e),(a,b), id2rel[rid]))\n",
        "                                break\n",
        "            all_preds.append(preds)\n",
        "    return all_preds\n",
        "\n",
        "# ---------- Training loop ----------\n",
        "def train_and_eval(model, train_loader, dev_loader, epochs=EPOCHS):\n",
        "    history = {\"train_loss\": [], \"dev_loss\": []}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss=0.0\n",
        "        pbar = tqdm.tqdm(train_loader, desc=f\"Train epoch {epoch}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            sub_h = batch[\"sub_head\"].to(DEVICE)\n",
        "            sub_t = batch[\"sub_tail\"].to(DEVICE)\n",
        "            obj_h = batch[\"obj_head\"].to(DEVICE)\n",
        "            obj_t = batch[\"obj_tail\"].to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            sh_log, st_log, _, _ = model(input_ids, attn, subject_span=None)\n",
        "            loss_sh = sub_head_loss_fn(sh_log, sub_h)\n",
        "            loss_st = sub_tail_loss_fn(st_log, sub_t)\n",
        "            # object loss using gold subjects\n",
        "            loss_obj_total = 0.0\n",
        "            obj_count = 0\n",
        "            B,L = input_ids.size()\n",
        "            for i in range(B):\n",
        "                shg = sub_h[i].cpu().numpy().astype(int)\n",
        "                stg = sub_t[i].cpu().numpy().astype(int)\n",
        "                gold_subs=[]\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        for q in range(p,L):\n",
        "                            if stg[q]==1:\n",
        "                                gold_subs.append((p,q)); break\n",
        "                if not gold_subs: continue\n",
        "                for (s,e) in gold_subs:\n",
        "                    span_tensor = torch.tensor([[s,e]], dtype=torch.long).to(DEVICE)\n",
        "                    _,_, oh_log, ot_log = model(input_ids[i:i+1,:], attn[i:i+1,:], subject_span=span_tensor)\n",
        "                    logits_oh = oh_log.squeeze(0)\n",
        "                    logits_ot = ot_log.squeeze(0)\n",
        "                    loss_rel = 0.0\n",
        "                    for rid in range(num_rels):\n",
        "                        pw_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                        pw_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                        loss_h = nn.BCEWithLogitsLoss(pos_weight=pw_h)(logits_oh[rid], obj_h[i,rid])\n",
        "                        loss_t = nn.BCEWithLogitsLoss(pos_weight=pw_t)(logits_ot[rid], obj_t[i,rid])\n",
        "                        loss_rel += (loss_h + loss_t)/2.0\n",
        "                    loss_obj_total += loss_rel / max(1, num_rels)\n",
        "                    obj_count += 1\n",
        "            if obj_count>0:\n",
        "                loss_obj_total = loss_obj_total / obj_count\n",
        "            else:\n",
        "                loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "            loss = loss_sh + loss_st + loss_obj_total\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "        avg_train = total_loss / max(1, len(train_loader))\n",
        "        history[\"train_loss\"].append(avg_train)\n",
        "        # quick dev loss eval (no heavy decoding)\n",
        "        model.eval()\n",
        "        dev_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in dev_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "                attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "                sub_h = batch[\"sub_head\"].to(DEVICE)\n",
        "                sub_t = batch[\"sub_tail\"].to(DEVICE)\n",
        "                obj_h = batch[\"obj_head\"].to(DEVICE)\n",
        "                obj_t = batch[\"obj_tail\"].to(DEVICE)\n",
        "                sh_log, st_log, _, _ = model(input_ids, attn, subject_span=None)\n",
        "                loss_sh = sub_head_loss_fn(sh_log, sub_h)\n",
        "                loss_st = sub_tail_loss_fn(st_log, sub_t)\n",
        "                # object loss same as train\n",
        "                loss_obj_total = 0.0\n",
        "                obj_count = 0\n",
        "                B,L = input_ids.size()\n",
        "                for i in range(B):\n",
        "                    shg = sub_h[i].cpu().numpy().astype(int)\n",
        "                    stg = sub_t[i].cpu().numpy().astype(int)\n",
        "                    gold_subs=[]\n",
        "                    for p in range(L):\n",
        "                        if shg[p]==1:\n",
        "                            for q in range(p,L):\n",
        "                                if stg[q]==1:\n",
        "                                    gold_subs.append((p,q)); break\n",
        "                    if not gold_subs: continue\n",
        "                    for (s,e) in gold_subs:\n",
        "                        span_tensor = torch.tensor([[s,e]], dtype=torch.long).to(DEVICE)\n",
        "                        _,_, oh_log, ot_log = model(input_ids[i:i+1,:], attn[i:i+1,:], subject_span=span_tensor)\n",
        "                        logits_oh = oh_log.squeeze(0)\n",
        "                        logits_ot = ot_log.squeeze(0)\n",
        "                        loss_rel = 0.0\n",
        "                        for rid in range(num_rels):\n",
        "                            pw_h = torch.tensor(weights[\"obj_head\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                            pw_t = torch.tensor(weights[\"obj_tail\"][rid], dtype=torch.float, device=DEVICE)\n",
        "                            loss_h = nn.BCEWithLogitsLoss(pos_weight=pw_h)(logits_oh[rid], obj_h[i,rid])\n",
        "                            loss_t = nn.BCEWithLogitsLoss(pos_weight=pw_t)(logits_ot[rid], obj_t[i,rid])\n",
        "                            loss_rel += (loss_h + loss_t)/2.0\n",
        "                        loss_obj_total += loss_rel / max(1, num_rels)\n",
        "                        obj_count += 1\n",
        "                if obj_count>0:\n",
        "                    loss_obj_total = loss_obj_total / obj_count\n",
        "                else:\n",
        "                    loss_obj_total = torch.tensor(0.0, device=DEVICE)\n",
        "                dev_loss += (loss_sh + loss_st + loss_obj_total).item()\n",
        "        avg_dev = dev_loss / max(1, len(dev_loader))\n",
        "        history[\"dev_loss\"].append(avg_dev)\n",
        "        print(f\"Epoch {epoch} summary: train_loss={avg_train:.6f} dev_loss={avg_dev:.6f}\")\n",
        "    return model, history\n",
        "\n",
        "# ---------- Run training ----------\n",
        "model, history = train_and_eval(model, train_loader, dev_loader, epochs=EPOCHS)\n",
        "\n",
        "# ---------- Final eval (decode triples on dev/test) ----------\n",
        "def final_eval(model, loader, decode_params):\n",
        "    model.eval()\n",
        "    preds_all = []\n",
        "    gold_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(loader, desc=\"Final Eval\"):\n",
        "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "            sh_log, st_log, _, _ = model(input_ids, attn, None)\n",
        "            preds_batch = decode_triples(input_ids, sh_log, st_log, model, attn,\n",
        "                                         subject_score_th=decode_params[\"subj_th\"],\n",
        "                                         object_score_th=decode_params[\"obj_th\"],\n",
        "                                         top_k_subj=decode_params[\"topk\"],\n",
        "                                         max_obj_span=decode_params.get(\"max_obj_span\", MAX_OBJ_SPAN),\n",
        "                                         use_topk=True)\n",
        "            # gold triples from labels\n",
        "            B,L = input_ids.size()\n",
        "            for i in range(B):\n",
        "                # gold subs\n",
        "                shg = batch[\"sub_head\"][i].cpu().numpy().astype(int)\n",
        "                stg = batch[\"sub_tail\"][i].cpu().numpy().astype(int)\n",
        "                gold_subs=[]\n",
        "                for p in range(L):\n",
        "                    if shg[p]==1:\n",
        "                        for q in range(p,L):\n",
        "                            if stg[q]==1:\n",
        "                                gold_subs.append((p,q)); break\n",
        "                for (s,e) in gold_subs:\n",
        "                    for rid in range(num_rels):\n",
        "                        ohg = batch[\"obj_head\"][i,rid].cpu().numpy().astype(int)\n",
        "                        otg = batch[\"obj_tail\"][i,rid].cpu().numpy().astype(int)\n",
        "                        for a in range(L):\n",
        "                            if ohg[a]==1:\n",
        "                                for b in range(a,L):\n",
        "                                    if otg[b]==1:\n",
        "                                        gold_all.append(((s,e),(a,b), id2rel[rid])); break\n",
        "            for pb in preds_batch:\n",
        "                preds_all.extend(pb)\n",
        "    def to_set(lst):\n",
        "        return set([(ss,se,os,oe,rel) for ((ss,se),(os,oe),rel) in lst])\n",
        "    gset = to_set(gold_all)\n",
        "    pset = to_set(preds_all)\n",
        "    tp = len(pset & gset)\n",
        "    fp = len(pset - gset)\n",
        "    fn = len(gset - pset)\n",
        "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "    return {\"precision\":prec,\"recall\":rec,\"f1\":f1,\"tp\":tp,\"fp\":fp,\"fn\":fn}\n",
        "\n",
        "decode_params = {\"subj_th\": SUBJ_SCORE_TH, \"obj_th\": OBJ_SCORE_TH, \"topk\": TOP_K_SUBJ}\n",
        "dev_metrics = final_eval(model, dev_loader, decode_params)\n",
        "test_metrics = final_eval(model, test_loader, decode_params)\n",
        "\n",
        "print(\"\\n=== FINAL METRICS ===\")\n",
        "print(\"DEV triple P/R/F:\", dev_metrics[\"precision\"], dev_metrics[\"recall\"], dev_metrics[\"f1\"])\n",
        "print(\"TEST triple P/R/F:\", test_metrics[\"precision\"], test_metrics[\"recall\"], test_metrics[\"f1\"])\n",
        "\n",
        "# ---------- Save ----------\n",
        "torch.save(model.state_dict(), OUTPUT_DIR / \"casrel_bert_fire_state_dict.pt\")\n",
        "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
        "with open(OUTPUT_DIR / \"casrel_metrics_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"relation_list\": relation_list,\n",
        "        \"num_rels\": num_rels,\n",
        "        \"train_records\": len(train_records),\n",
        "        \"dev_records\": len(dev_records),\n",
        "        \"test_records\": len(test_records),\n",
        "        \"weights\": weights,\n",
        "        \"history\": history,\n",
        "        \"dev_metrics\": dev_metrics,\n",
        "        \"test_metrics\": test_metrics\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"Saved model + metrics to\", OUTPUT_DIR)\n"
      ]
    }
  ]
}